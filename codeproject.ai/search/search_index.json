{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"CodeProject.AI Server","text":"<p>Docker Images Windows Installer API Docs FAQs GitHub Discussions </p> <p></p> <p>CodeProject.AI Server is a self-hosted, free and Open Source Artificial Intelligence Server for any  platform, any language. Just like you would install a database server to provide data storage, you install CodeProject.AI Server to provide AI services.</p> <p>It can be installed locally, requires no off-device or out of network data transfer, and is easy to use.</p> <p></p> <p>AI programming is something every single developer should be aware of. We wanted a fun project we could use to help teach developers and get them involved in AI. We'll be using CodeProject.AI as a focus for articles and exploration to make it fun and painless to learn AI programming.</p> <p>We got sick of fighting versions and libraries and models and being blocked by tiny annoying things every step of the way. So we put put this together so we could save you the frustation. We'll take care of the housekeeping, you focus on the code.</p> <p>We don't always want our personal data in the cloud. It should always be your choice as to who sees your data. </p> <p>Help us make this something amazing!</p>"},{"location":"index.html#cut-to-the-chase-how-do-i-play-with-it","title":"Cut to the chase: how do I play with it?","text":""},{"location":"index.html#1-running-and-playing-with-the-features","title":"1: Running and playing with the features","text":"<ol> <li>Download the latest version, install, and launch the shortcut to the server's dashboard on your desktop.</li> <li>On the dashboard, top and center, is a link to the CodeProject.AI Explorer. Open that and play!</li> </ol>"},{"location":"index.html#2-running-and-debugging-the-code","title":"2: Running and debugging the code","text":"<p>Read our quick guide to setting up and running CodeProject.AI in Visual Studio Code or Visual Studio.</p>"},{"location":"index.html#how-do-i-use-it-in-my-application","title":"How do I use it in my application?","text":"<p>Here's an example of using the API for scene detection using a simple JavaScript call:</p> HTML<pre><code>&lt;html&gt;\n&lt;body&gt;\nDetect the scene in this file: &lt;input id=\"image\" type=\"file\" /&gt;\n&lt;input type=\"button\" value=\"Detect Scene\" onclick=\"detectScene(image)\" /&gt;\n\n&lt;script&gt;\nfunction detectScene(fileChooser) {\n    var formData = new FormData();\n    formData.append('image', fileChooser.files[0]);\n\n    fetch('http://localhost:32168/v1/vision/detect/scene', {\n        method: \"POST\",\n        body: formData\n    })\n    .then(response =&gt; {\n        if (response.ok) response.json().then(data =&gt; {\n            console.log(`Scene is ${data.label}, ${data.confidence} confidence`)\n        });\n    });\n}\n&lt;/script&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre> <p>You can include the CodeProject.AI installer (or just a link to the latest version of the installer) in your own apps and installers and voila, you have an AI enabled app.</p>"},{"location":"index.html#what-does-it-include","title":"What does it include?","text":"<p>CodeProject.AI includes</p> <ol> <li>A HTTP REST API Server. The server listens for requests from other apps, passes them to the backend analysis services for processing, and then passes the results back to the caller. It runs as a simple self contained web service on your device.</li> <li>Backend Analysis services.  The brains of the operation is in the analysis services sitting behind the front end API. All processing of data is done on the current machine. No calls to the cloud and no data leaving the device.</li> <li>The Source Code, naturally.</li> </ol>"},{"location":"index.html#what-can-it-do","title":"What can it do?","text":"<p>CodeProject.AI can currently</p> <ul> <li>Detect objects in images</li> <li>Detect faces in images</li> <li>Transform a face into an anime-style cartoon</li> <li>Read text and license plates</li> <li>Detect the type of scene represented in an image</li> <li>Recognise faces that have been registered with the service</li> <li>Pull out the most important sentences in text to generate a text summary</li> <li>Remove the background automatically from images</li> <li>Blur the background of images to produce a portrait effect</li> <li>Generate a sentiment score for text</li> </ul> <p>We will be constantly expanding the feature list.</p> <p>Please note that the Windows Install and Docker versions only contain object detection modules. Other modules can be installed via the dashboard at runtime.</p>"},{"location":"index.html#our-goals","title":"Our Goals","text":"<ol> <li>To promote AI development and inspire the AI developer community to dive in and have a go. AI is here, it's in demand, and it's a huge paradigm change in the industry. Whether you like AI or not, developers owe it to themselves to experiment in and familiarise themselves with the  technology. This is CodeProject.AI: a demonstration, an explorer, a learning tool, and a library and service that can be used out of the box.</li> <li>To make AI development easy. It's not that AI development is that hard. It's that there are so, so many options. Our architecture is designed to allow any AI implementation to find a home in our system, and for our service to be callable from any language.</li> <li>To focus on core use-cases. We're deliberately not a solution for everyone. Instead we're a solution for common day-to-day needs. We will be adding dozens of modules and scores of AI capabilities to our system, but our goal is always clarity and simplicity over a 100% solution.</li> <li>To tap the expertise of the Developer Community. We're not experts but we know a developer or two out there who are. The true power of CodeProject.AI comes from the contributions and improvements from our AI community.</li> </ol>"},{"location":"index.html#roadmap","title":"Roadmap","text":"<p>The following features will be added over the coming weeks and months</p> <ol> <li>More modules and a streamlined plugin architecture</li> <li>A GUI management system</li> <li>GPU support across a range of graphics cards and accelerators</li> <li>More analysis services</li> </ol>"},{"location":"about/about.html","title":"About CodeProject.AI","text":"<p>CodeProject was formed to allow developers to freely share their knowledge, code and ideas. We believe that by opening your code to others, by teaching those who are learning, and by sharing our daily experiences we all become better programmers.</p> <p>We understand that no matter how advanced you are in your field, we were all beginners once. There's always something more to learn.</p>"},{"location":"about/about.html#cofounders","title":"CoFounders","text":""},{"location":"about/about.html#chris-maunder","title":"Chris Maunder","text":"<p>Chris is the Co-founder of the popular code-sharing site CodeProject.com, the digital advertising agency DeveloperMedia.com and the content marketing agency ContentLab.IO.</p> <p>He's been programming way too long and has been, in various guides, an astrophysicist, mechanic, mathematician, physicist, breeder of carnivorous plants, hydrologist, geomorphologist, defence intelligence researcher and then, when all that got a bit rough on the nerves, a serial entrepreneur.</p> <p>Chris has programmed everything from FORTRAN on a CRAY to Python on a Pi, but generally leans on TypeScript, C#, Python and SQL for the front, middle and back bits of his applications. His current focus is on ensuring developers know enough about Artificial Intelligence to be dangerous.</p> <p>Chris is currently neck deep building systems and integrations for CodeProject.AI Server.</p>"},{"location":"about/about.html#david-cunningham","title":"David Cunningham","text":"<p>I started photo-etching circuit boards when I was 8, and at 11 was haunting the halls and computer science labs at the local university so much that I was invited by Professor Wayne Ayott to audit his software and hardware design courses.</p> <p>Over my career I have used C# , C++, Win32, MFC, Assembler, Basic, and Clipper, on applications for the military, commercial ventures, medical research and the labour movement. Through my medical informatics work I came to know what real-time and mission-critical really mean...just try being part of the critical path when a woman goes into labor.</p> <p>I have been honoured to receive many business, industry and leadership awards including being named an Exceptional Young Entrepreneur (Profit Magazine) and to the list of Who's Who in Canadian Business. The companies I started have been recognized as the Fastest Growing companies in Canada (Profit Magazine), as the Fastest Growing North American Technology Companies (Deloitte &amp; Touche) and named as a Top 100 Innovator and Leader by SDTimes 6 years in a row.</p> <p>Here's my professional profile on LinkedIn: http://www.linkedin.com/in/davidcunningham</p> <p>In 2007 Microsoft acquired technology from one of my companies, Dundas Data Visualization, for inclusion in SQL Server.</p> <p>I live in Toronto and enjoy photography, scuba, food, and motorcycle riding.</p>"},{"location":"about/about.html#dev-and-architect","title":"Dev and Architect","text":""},{"location":"about/about.html#matthew-dennis","title":"Matthew Dennis","text":"<p>As Senior Architect, Matthew is responsible for the Architecture, Design, and Coding of the CodeProject software as well as Manager of the Infrastructure that runs the web site.</p> <p>Matthew works on improving the performance and experience of the Code Project site for users, clients, and administrators.</p> <p>Matthew has more years of software development, QA and architecture experience under his belt than he likes to admit. He graduated from the University of Waterloo with a B.Sc. in Electrical Engineering. He started out developing micro-processor based hardware and software including compilers and operating systems.</p> <p>His current focus is on .NET web development including jQuery, Webforms, MVC, AJAX, and patterns and practices for creating better websites. He is the author of the Munq IOC, the fastest ASP.NET focused IOC Container. His non-programming passions include golf, pool, curling, reading and building stuff for the house.</p>"},{"location":"about/about.html#editorial-and-community","title":"Editorial and Community","text":""},{"location":"about/about.html#sean-ewington","title":"Sean Ewington","text":"<p>His background in programming is primarily C++ and HTML, but has experience in other, \"unsavoury\" languages.</p> <p>He loves movies, and likes to say inconceivable often, even if it does not mean what he thinks it means.</p>"},{"location":"about/license.html","title":"License","text":""},{"location":"about/license.html#server-side-public-license","title":"Server Side Public License","text":"<p>VERSION 1, OCTOBER 16, 2018</p> <p>Copyright \u00a9 2018 MongoDB, Inc.</p> <p>Everyone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed.</p>"},{"location":"about/license.html#terms-and-conditions","title":"TERMS AND CONDITIONS","text":"<ol> <li>Definitions.  This License\" refers to Server Side Public License.  \"Copyright\" also means copyright-like laws that apply to other kinds of works, such as semiconductor masks.  \"The Program\" refers to any copyrightable work licensed under this License. Each licensee is addressed as \"you\". \"Licensees\" and \"recipients\" may be individuals or organizations.  To \"modify\" a work means to copy from or adapt all or part of the work in a fashion requiring copyright permission, other than the making of an exact copy. The resulting work is called a \"modified version\" of the earlier work or a work \"based on\" the earlier work.  A \"covered work\" means either the unmodified Program or a work based on the Program.  To \"propagate\" a work means to do anything with it that, without permission, would make you directly or secondarily liable for infringement under applicable copyright law, except executing it on a computer or modifying a private copy. Propagation includes copying, distribution (with or without modification), making available to the public, and in some countries other activities as well.  To \"convey\" a work means any kind of propagation that enables other parties to make or receive copies. Mere interaction with a user through a computer network, with no transfer of a copy, is not conveying.  An interactive user interface displays \"Appropriate Legal Notices\" to the extent that it includes a convenient and prominently visible feature that (1) displays an appropriate copyright notice, and (2) tells the user that there is no warranty for the work (except to the extent that warranties are provided), that licensees may convey the work under this License, and how to view a copy of this License. If the interface presents a list of user commands or options, such as a menu, a prominent item in the list meets this criterion. </li> <li>Source Code.  The \"source code\" for a work means the preferred form of the work for making modifications to it. \"Object code\" means any non-source form of a work.  A \"Standard Interface\" means an interface that either is an official standard defined by a recognized standards body, or, in the case of interfaces specified for a particular programming language, one that is widely used among developers working in that language.  The \"System Libraries\" of an executable work include anything, other than the work as a whole, that (a) is included in the normal form of packaging a Major Component, but which is not part of that Major Component, and (b) serves only to enable use of the work with that Major Component, or to implement a Standard Interface for which an implementation is available to the public in source code form. A \"Major Component\", in this context, means a major essential component (kernel, window system, and so on) of the specific operating system (if any) on which the executable work runs, or a compiler used to produce the work, or an object code interpreter used to run it.  The \"Corresponding Source\" for a work in object code form means all the source code needed to generate, install, and (for an executable work) run the object code and to modify the work, including scripts to control those activities. However, it does not include the work's System Libraries, or general-purpose tools or generally available free programs which are used unmodified in performing those activities but which are not part of the work. For example, Corresponding Source includes interface definition files associated with source files for the work, and the source code for shared libraries and dynamically linked subprograms that the work is specifically designed to require, such as by intimate data communication or control flow between those subprograms and other parts of the work.  The Corresponding Source need not include anything that users can regenerate automatically from other parts of the Corresponding Source.  The Corresponding Source for a work in source code form is that same work. </li> <li>Basic Permissions.  All rights granted under this License are granted for the term of copyright on the Program, and are irrevocable provided the stated conditions are met. This License explicitly affirms your unlimited permission to run the unmodified Program, subject to section 13. The output from running a covered work is covered by this License only if the output, given its content, constitutes a covered work. This License acknowledges your rights of fair use or other equivalent, as provided by copyright law.  Subject to section 13, you may make, run and propagate covered works that you do not convey, without conditions so long as your license otherwise remains in force. You may convey covered works to others for the sole purpose of having them make modifications exclusively for you, or provide you with facilities for running those works, provided that you comply with the terms of this License in conveying all material for which you do not control copyright. Those thus making or running the covered works for you must do so exclusively on your behalf, under your direction and control, on terms that prohibit them from making any copies of your copyrighted material outside their relationship with you.  Conveying under any other circumstances is permitted solely under the conditions stated below. Sublicensing is not allowed; section 10 makes it unnecessary. </li> <li>Protecting Users' Legal Rights From Anti-Circumvention Law.  No covered work shall be deemed part of an effective technological measure under any applicable law fulfilling obligations under article 11 of the WIPO copyright treaty adopted on 20 December 1996, or similar laws prohibiting or restricting circumvention of such measures.  When you convey a covered work, you waive any legal power to forbid circumvention of technological measures to the extent such circumvention is effected by exercising rights under this License with respect to the covered work, and you disclaim any intention to limit operation or modification of the work as a means of enforcing, against the work's users, your or third parties' legal rights to forbid circumvention of technological measures. </li> <li>Conveying Verbatim Copies.  You may convey verbatim copies of the Program's source code as you receive it, in any medium, provided that you conspicuously and appropriately publish on each copy an appropriate copyright notice; keep intact all notices stating that this License and any non-permissive terms added in accord with section 7 apply to the code; keep intact all notices of the absence of any warranty; and give all recipients a copy of this License along with the Program.  You may charge any price or no price for each copy that you convey, and you may offer support or warranty protection for a fee. </li> <li>Conveying Modified Source Versions.  You may convey a work based on the Program, or the modifications to produce it from the Program, in the form of source code under the terms of section 4, provided that you also meet all of these conditions:  a. The work must carry prominent notices stating that you modified it, and giving a relevant date.  b) The work must carry prominent notices stating that it is released under this License and any conditions added under section 7. This requirement modifies the requirement in section 4 to \"keep intact all notices\".  c) You must license the entire work, as a whole, under this License to anyone who comes into possession of a copy. This License will therefore apply, along with any applicable section 7 additional terms, to the whole of the work, and all its parts, regardless of how they are packaged. This License gives no permission to license the work in any other way, but it does not invalidate such permission if you have separately received it.  d) If the work has interactive user interfaces, each must display Appropriate Legal Notices; however, if the Program has interactive interfaces that do not display Appropriate Legal Notices, your work need not make them do so.  A compilation of a covered work with other separate and independent works, which are not by their nature extensions of the covered work, and which are not combined with it such as to form a larger program, in or on a volume of a storage or distribution medium, is called an \"aggregate\" if the compilation and its resulting copyright are not used to limit the access or legal rights of the compilation's users beyond what the individual works permit. Inclusion of a covered work in an aggregate does not cause this License to apply to the other parts of the aggregate. </li> <li>Conveying Non-Source Forms.  You may convey a covered work in object code form under the terms of sections 4 and 5, provided that you also convey the machine-readable Corresponding Source under the terms of this License, in one of these ways:  a) Convey the object code in, or embodied in, a physical product (including a physical distribution medium), accompanied by the Corresponding Source fixed on a durable physical medium customarily used for software interchange.  b) Convey the object code in, or embodied in, a physical product (including a physical distribution medium), accompanied by a written offer, valid for at least three years and valid for as long as you offer spare parts or customer support for that product model, to give anyone who possesses the object code either (1) a copy of the Corresponding Source for all the software in the product that is covered by this License, on a durable physical medium customarily used for software interchange, for a price no more than your reasonable cost of physically performing this conveying of source, or (2) access to copy the Corresponding Source from a network server at no charge.  c) Convey individual copies of the object code with a copy of the written offer to provide the Corresponding Source. This alternative is allowed only occasionally and noncommercially, and only if you received the object code with such an offer, in accord with subsection 6b.  d) Convey the object code by offering access from a designated place (gratis or for a charge), and offer equivalent access to the Corresponding Source in the same way through the same place at no further charge. You need not require recipients to copy the Corresponding Source along with the object code. If the place to copy the object code is a network server, the Corresponding Source may be on a different server (operated by you or a third party) that supports equivalent copying facilities, provided you maintain clear directions next to the object code saying where to find the Corresponding Source. Regardless of what server hosts the Corresponding Source, you remain obligated to ensure that it is available for as long as needed to satisfy these requirements.  e) Convey the object code using peer-to-peer transmission, provided you inform other peers where the object code and Corresponding Source of the work are being offered to the general public at no charge under subsection 6d.  A separable portion of the object code, whose source code is excluded from the Corresponding Source as a System Library, need not be included in conveying the object code work.  A \"User Product\" is either (1) a \"consumer product\", which means any tangible personal property which is normally used for personal, family, or household purposes, or (2) anything designed or sold for incorporation into a dwelling. In determining whether a product is a consumer product, doubtful cases shall be resolved in favor of coverage. For a particular product received by a particular user, \"normally used\" refers to a typical or common use of that class of product, regardless of the status of the particular user or of the way in which the particular user actually uses, or expects or is expected to use, the product. A product is a consumer product regardless of whether the product has substantial commercial, industrial or non-consumer uses, unless such uses represent the only significant mode of use of the product.  \"Installation Information\" for a User Product means any methods, procedures, authorization keys, or other information required to install and execute modified versions of a covered work in that User Product from a modified version of its Corresponding Source. The information must suffice to ensure that the continued functioning of the modified object code is in no case prevented or interfered with solely because modification has been made.  If you convey an object code work under this section in, or with, or specifically for use in, a User Product, and the conveying occurs as part of a transaction in which the right of possession and use of the User Product is transferred to the recipient in perpetuity or for a fixed term (regardless of how the transaction is characterized), the Corresponding Source conveyed under this section must be accompanied by the Installation Information. But this requirement does not apply if neither you nor any third party retains the ability to install modified object code on the User Product (for example, the work has been installed in ROM).  The requirement to provide Installation Information does not include a requirement to continue to provide support service, warranty, or updates for a work that has been modified or installed by the recipient, or for the User Product in which it has been modified or installed. Access to a network may be denied when the modification itself materially and adversely affects the operation of the network or violates the rules and protocols for communication across the network.  Corresponding Source conveyed, and Installation Information provided, in accord with this section must be in a format that is publicly documented (and with an implementation available to the public in source code form), and must require no special password or key for unpacking, reading or copying. </li> <li>Additional Terms.  \"Additional permissions\" are terms that supplement the terms of this License by making exceptions from one or more of its conditions. Additional permissions that are applicable to the entire Program shall be treated as though they were included in this License, to the extent that they are valid under applicable law. If additional permissions apply only to part of the Program, that part may be used separately under those permissions, but the entire Program remains governed by this License without regard to the additional permissions.  When you convey a copy of a covered work, you may at your option remove any additional permissions from that copy, or from any part of it. (Additional permissions may be written to require their own removal in certain cases when you modify the work.) You may place additional permissions on material, added by you to a covered work, for which you have or can give appropriate copyright permission.  Notwithstanding any other provision of this License, for material you add to a covered work, you may (if authorized by the copyright holders of that material) supplement the terms of this License with terms:  a) Disclaiming warranty or limiting liability differently from the terms of sections 15 and 16 of this License; or  b) Requiring preservation of specified reasonable legal notices or author attributions in that material or in the Appropriate Legal Notices displayed by works containing it; or  c) Prohibiting misrepresentation of the origin of that material, or requiring that modified versions of such material be marked in reasonable ways as different from the original version; or  d) Limiting the use for publicity purposes of names of licensors or authors of the material; or  e) Declining to grant rights under trademark law for use of some trade names, trademarks, or service marks; or  f) Requiring indemnification of licensors and authors of that material by anyone who conveys the material (or modified versions of it) with contractual assumptions of liability to the recipient, for any liability that these contractual assumptions directly impose on those licensors and authors.  All other non-permissive additional terms are considered \"further restrictions\" within the meaning of section 10. If the Program as you received it, or any part of it, contains a notice stating that it is governed by this License along with a term that is a further restriction, you may remove that term. If a license document contains a further restriction but permits relicensing or conveying under this License, you may add to a covered work material governed by the terms of that license document, provided that the further restriction does not survive such relicensing or conveying.  If you add terms to a covered work in accord with this section, you must place, in the relevant source files, a statement of the additional terms that apply to those files, or a notice indicating where to find the applicable terms.  Additional terms, permissive or non-permissive, may be stated in the form of a separately written license, or stated as exceptions; the above requirements apply either way. </li> <li>Termination.  You may not propagate or modify a covered work except as expressly provided under this License. Any attempt otherwise to propagate or modify it is void, and will automatically terminate your rights under this License (including any patent licenses granted under the third paragraph of section 11).  However, if you cease all violation of this License, then your license from a particular copyright holder is reinstated (a) provisionally, unless and until the copyright holder explicitly and finally terminates your license, and (b) permanently, if the copyright holder fails to notify you of the violation by some reasonable means prior to 60 days after the cessation.  Moreover, your license from a particular copyright holder is reinstated permanently if the copyright holder notifies you of the violation by some reasonable means, this is the first time you have received notice of violation of this License (for any work) from that copyright holder, and you cure the violation prior to 30 days after your receipt of the notice.  Termination of your rights under this section does not terminate the licenses of parties who have received copies or rights from you under this License. If your rights have been terminated and not permanently reinstated, you do not qualify to receive new licenses for the same material under section 10. </li> <li>Acceptance Not Required for Having Copies.  You are not required to accept this License in order to receive or run a copy of the Program. Ancillary propagation of a covered work occurring solely as a consequence of using peer-to-peer transmission to receive a copy likewise does not require acceptance. However, nothing other than this License grants you permission to propagate or modify any covered work. These actions infringe copyright if you do not accept this License. Therefore, by modifying or propagating a covered work, you indicate your acceptance of this License to do so. </li> <li>Automatic Licensing of Downstream Recipients.  Each time you convey a covered work, the recipient automatically receives a license from the original licensors, to run, modify and propagate that work, subject to this License. You are not responsible for enforcing compliance by third parties with this License.  An \"entity transaction\" is a transaction transferring control of an organization, or substantially all assets of one, or subdividing an organization, or merging organizations. If propagation of a covered work results from an entity transaction, each party to that transaction who receives a copy of the work also receives whatever licenses to the work the party's predecessor in interest had or could give under the previous paragraph, plus a right to possession of the Corresponding Source of the work from the predecessor in interest, if the predecessor has it or can get it with reasonable efforts.  You may not impose any further restrictions on the exercise of the rights granted or affirmed under this License. For example, you may not impose a license fee, royalty, or other charge for exercise of rights granted under this License, and you may not initiate litigation (including a cross-claim or counterclaim in a lawsuit) alleging that any patent claim is infringed by making, using, selling, offering for sale, or importing the Program or any portion of it. </li> <li>Patents.  A \"contributor\" is a copyright holder who authorizes use under this License of the Program or a work on which the Program is based. The work thus licensed is called the contributor's \"contributor version\".  A contributor's \"essential patent claims\" are all patent claims owned or controlled by the contributor, whether already acquired or hereafter acquired, that would be infringed by some manner, permitted by this License, of making, using, or selling its contributor version, but do not include claims that would be infringed only as a consequence of further modification of the contributor version. For purposes of this definition, \"control\" includes the right to grant patent sublicenses in a manner consistent with the requirements of this License.  Each contributor grants you a non-exclusive, worldwide, royalty-free patent license under the contributor's essential patent claims, to make, use, sell, offer for sale, import and otherwise run, modify and propagate the contents of its contributor version.  In the following three paragraphs, a \"patent license\" is any express agreement or commitment, however denominated, not to enforce a patent (such as an express permission to practice a patent or covenant not to sue for patent infringement). To \"grant\" such a patent license to a party means to make such an agreement or commitment not to enforce a patent against the party.  If you convey a covered work, knowingly relying on a patent license, and the Corresponding Source of the work is not available for anyone to copy, free of charge and under the terms of this License, through a publicly available network server or other readily accessible means, then you must either (1) cause the Corresponding Source to be so available, or (2) arrange to deprive yourself of the benefit of the patent license for this particular work, or (3) arrange, in a manner consistent with the requirements of this License, to extend the patent license to downstream recipients. \"Knowingly relying\" means you have actual knowledge that, but for the patent license, your conveying the covered work in a country, or your recipient's use of the covered work in a country, would infringe one or more identifiable patents in that country that you have reason to believe are valid.  If, pursuant to or in connection with a single transaction or arrangement, you convey, or propagate by procuring conveyance of, a covered work, and grant a patent license to some of the parties receiving the covered work authorizing them to use, propagate, modify or convey a specific copy of the covered work, then the patent license you grant is automatically extended to all recipients of the covered work and works based on it.  A patent license is \"discriminatory\" if it does not include within the scope of its coverage, prohibits the exercise of, or is conditioned on the non-exercise of one or more of the rights that are specifically granted under this License. You may not convey a covered work if you are a party to an arrangement with a third party that is in the business of distributing software, under which you make payment to the third party based on the extent of your activity of conveying the work, and under which the third party grants, to any of the parties who would receive the covered work from you, a discriminatory patent license (a) in connection with copies of the covered work conveyed by you (or copies made from those copies), or (b) primarily for and in connection with specific products or compilations that contain the covered work, unless you entered into that arrangement, or that patent license was granted, prior to 28 March 2007.  Nothing in this License shall be construed as excluding or limiting any implied license or other defenses to infringement that may otherwise be available to you under applicable patent law. </li> <li>No Surrender of Others' Freedom.  If conditions are imposed on you (whether by court order, agreement or otherwise) that contradict the conditions of this License, they do not excuse you from the conditions of this License. If you cannot use, propagate or convey a covered work so as to satisfy simultaneously your obligations under this License and any other pertinent obligations, then as a consequence you may not use, propagate or convey it at all. For example, if you agree to terms that obligate you to collect a royalty for further conveying from those to whom you convey the Program, the only way you could satisfy both those terms and this License would be to refrain entirely from conveying the Program. </li> <li>Offering the Program as a Service.  If you make the functionality of the Program or a modified version available to third parties as a service, you must make the Service Source Code available via network download to everyone at no charge, under the terms of this License. Making the functionality of the Program or modified version available to third parties as a service includes, without limitation, enabling third parties to interact with the functionality of the Program or modified version remotely through a computer network, offering a service the value of which entirely or primarily derives from the value of the Program or modified version, or offering a service that accomplishes for users the primary purpose of the Program or modified version.  \"Service Source Code\" means the Corresponding Source for the Program or the modified version, and the Corresponding Source for all programs that you use to make the Program or modified version available as a service, including, without limitation, management software, user interfaces, application program interfaces, automation software, monitoring software, backup software, storage software and hosting software, all such that a user could run an instance of the service using the Service Source Code you make available. </li> <li>Revised Versions of this License.  MongoDB, Inc. may publish revised and/or new versions of the Server Side Public License from time to time. Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns.  Each version is given a distinguishing version number. If the Program specifies that a certain numbered version of the Server Side Public License \"or any later version\" applies to it, you have the option of following the terms and conditions either of that numbered version or of any later version published by MongoDB, Inc. If the Program does not specify a version number of the Server Side Public License, you may choose any version ever published by MongoDB, Inc.  If the Program specifies that a proxy can decide which future versions of the Server Side Public License can be used, that proxy's public statement of acceptance of a version permanently authorizes you to choose that version for the Program.  Later license versions may give you additional or different permissions. However, no additional obligations are imposed on any author or copyright holder as a result of your choosing to follow a later version. </li> <li>Disclaimer of Warranty.  THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY APPLICABLE LAW. EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM \"AS IS\" WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM IS WITH YOU. SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL NECESSARY SERVICING, REPAIR OR CORRECTION. </li> <li>Limitation of Liability.  IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS), EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. </li> <li>Interpretation of Sections 15 and 16.  If the disclaimer of warranty and limitation of liability provided above cannot be given local legal effect according to their terms, reviewing courts shall apply local law that most closely approximates an absolute waiver of all civil liability in connection with the Program, unless a warranty or assumption of liability accompanies a copy of the Program in return for a fee."},{"location":"about/license.html#end-of-terms-and-conditions","title":"END OF TERMS AND CONDITIONS","text":""},{"location":"about/release-notes.html","title":"Release Notes","text":""},{"location":"about/release-notes.html#release-29-beta","title":"Release 2.9 Beta","text":"<ul> <li>Updated to .NET 9</li> <li>Support for Ubuntu 24.10</li> <li>Improved CUDA 12 support</li> <li>Improvements to CUDA support in Windows and Linux</li> <li>Further Windows arm64 fixes</li> <li>Further macOS arm64 fixes</li> <li>General dev environment setup fixes</li> <li>Fixes for Windows installer when wget is missing</li> </ul>"},{"location":"about/release-notes.html#release-28-beta","title":"Release 2.8 Beta","text":"<ul> <li>General fixes and reorg of project</li> </ul>"},{"location":"about/release-notes.html#release-26-beta","title":"Release 2.6 Beta","text":"<ul> <li>You can now select, at install time, which modules you wish to have initially installed</li> <li>Some modules (Coral, Yolov8) now allow you to download individual models at runtime via the dashboard.</li> <li>A new generative AI module (Llama LLM Chatbot)</li> <li>A standardised way to handle (in code) modules that run long processes such as generative AI</li> <li>Debian support has been improved</li> <li>Small UI improvements to the dashboard</li> <li>Some simplification of the modulesettings files</li> <li>The inclusion, in the source code, of template .NET and Python modules (both simple and long process demos)</li> <li>Improvements to the Coral and ALPR modules (thanks to Seth and Mike)</li> <li>Docker CUDA 12.2 image now includes cuDNN</li> <li>Install script fixes</li> <li>Added Object Segmentation to the YOLOv8 module</li> </ul>"},{"location":"about/release-notes.html#release-25-beta","title":"Release 2.5 Beta","text":"<ul> <li>Dynamic Explorer UI: Each module now supplies its own UI for the explorer</li> <li>More information returned by each module's response as standard</li> <li>Support for sound audition modules in the explorer</li> <li>Improvements to, and a more responsive module status on the dashboard</li> <li>Updated module settings schema that includes module author and original project acknowledgement</li> <li>A separate status update from each module that decouples the stats for a module. This just cleans things up a little on the backend</li> <li>Installer fixes</li> <li>Minor modulesettings.json schema update, which introduces the concept of model requirements.</li> <li>Updated ALPR, OCR (thanks to Mike Lud) and Coral Object Detection (Thanks to Seth Price) modules</li> <li>Improved Jetson support</li> <li>Pre-installed modules in Docker can now be uninstalled / reinstalled </li> </ul>"},{"location":"about/release-notes.html#release-24-beta","title":"Release 2.4 Beta","text":"<ul> <li>Mesh support Automatically offload inference work to other servers on your network based on inference speed. Zero config, and dashboard support to enable/disable.</li> <li>CUDA detection fixed</li> <li>Support for CUDA 10.2</li> <li>Module self-test performed on installation</li> <li>YOLOv8 module added</li> <li>YOLOv5 .NET module fixes for GPU, and YOLOv5 3.1 GPU support fixed</li> <li>Python package and .NET installation issues fixed</li> <li>Better prompts for admin-only installs</li> <li>Fixes for Python package installs</li> <li>Issues installing .NET</li> <li>More logging output to help diagnose issues</li> <li>VC Redist hash error fixed</li> <li>General bug fixes.</li> <li>Breaking: modulesettings.json schema changed</li> </ul>"},{"location":"about/release-notes.html#release-23-beta","title":"Release 2.3 Beta","text":"<ul> <li>A focus on improving the installation of modules at runtime. More error checks, faster re-install, better reporting, and manual fallbacks in situations where admin rights are needed</li> <li>A revamped SDK that removes much (or all, in some cases) of the boilerplate code needed in install scripts</li> <li>Fine grained support for different CUDA versions as well as systems such as Raspberry Pi, Orange Pi and Jetson</li> <li>Support for CUDA 12.2</li> <li>GPU support for PaddlePaddle (OCR and license plate readers benefit)</li> <li>CUDA 12.2 Docker image</li> <li>Lots of bug fixes in install scripts</li> <li>UI tweaks</li> <li>ALPR now using GPU in Windows</li> <li>Corrections to Linux/macOS installers</li> </ul>"},{"location":"about/release-notes.html#release-22-beta","title":"Release 2.2 Beta","text":"<ul> <li>An entirely new Windows installer offering more installation options and a smoother upgrade experience from here on.</li> <li>New macOS and Ubuntu native installers, for x64 and arm64 (including Raspberry Pi)</li> <li>A new installation SDK for making module installers far easier</li> <li>Improved installation feedback and self-checks</li> <li>Coral.AI support for Linux, macOS (version 11 and 12 only) and Windows</li> </ul>"},{"location":"about/release-notes.html#release-21-beta","title":"Release 2.1 Beta","text":"<ul> <li>Improved Raspberry Pi support. A new, fast object detection module with   support for the Coral.AI TPU, all within an Arm64 Docker image</li> <li>All modules can now be installed / uninstalled (rather than having some modules fixed and uninstallble).</li> <li>Installer is streamlined: Only the server is installed at installation time, and on first run we install Object Detection (Python and .NET) and Face Processing (which can be uninstalled).</li> <li>Reworking of the Python module SDK. Modules are new child classes, not aggregators of our module runner.</li> <li>Reworking of the modulesettings file to make it simpler and have less replication</li> <li>Improved logging: quantity, quality, filtering and better information</li> <li>Addition of 2 modules: ObjectDetectionTFLite for Object Detection on a on Raspberry Pi using Coral,    and Cartoonise for some fun</li> <li>Improvements to half-precision support checks on CUDA cards</li> <li>Modules are now versioned and our module registry will now only show modules that fit your current server version.</li> <li>Various bug fixes</li> <li>Shared Python runtimes now in <code>runtimes</code>. </li> <li>All modules moved from the <code>AnalysisLayer</code> folder to the <code>modules</code> folder</li> <li>Tested on CUDA 12 (Note: ALPR and OCR do not run on CUDA 12)</li> </ul>"},{"location":"about/release-notes.html#release-20-beta","title":"Release 2.0 Beta","text":"<ul> <li>New Downloadable module system</li> <li>Re-introduction of PyTorch 1.7 YOLO module for older GPUs</li> <li>.NET 7</li> </ul>"},{"location":"about/release-notes.html#release-1600-beta","title":"Release 1.6.0.0 Beta","text":"<ul> <li>Optimised RAM use</li> <li>Ability to enable / disable modules and GPU support via the dashboard</li> <li>REST settings API for updating settings on the fly</li> <li>Apple M1/M2 GPU support</li> <li>Async processes and logging for a performance boost</li> <li>Breaking: the CustomObjectDetection is now part of ObjectDetectionYolo</li> </ul>"},{"location":"about/release-notes.html#release-1562-beta","title":"Release 1.5.6.2 Beta","text":"<ul> <li>Docker NVIDIA GPU support</li> <li>Further performance improvements</li> <li>cuDNN install script to help with NVIDIA driver and toolkit installation</li> <li>Bug fixes</li> </ul>"},{"location":"about/release-notes.html#release-156-beta","title":"Release 1.5.6 Beta","text":"<ul> <li>NVIDIA GPU support for Windows</li> <li>Perf improvements to Python modules</li> <li>Work on the Python SDK to make creating modules easier</li> <li>Dev installers now drastically simplified for those creating new modules</li> <li>Added SuperResolution as a demo module</li> </ul>"},{"location":"about/release-notes.html#release-15-beta","title":"Release 1.5 Beta","text":"<ul> <li>Support for custom models</li> </ul>"},{"location":"about/release-notes.html#release-13x-beta","title":"Release 1.3.x Beta","text":"<ul> <li>Refactored and improved setup and module addition system</li> <li>Introduction of modulesettings.json files</li> <li>New analysis modules</li> </ul>"},{"location":"about/release-notes.html#release-12x-beta","title":"Release 1.2.x Beta","text":"<ul> <li>Support for Apple Silicon for development mode</li> <li>Native Windows installer</li> <li>Runs as Windows Service</li> <li>Run in a Docker Container</li> <li>Installs and Builds using VSCode in Linux (Ubuntu), macOS and Windows, as well as Visual Studio on Windows</li> <li>General optimisation of the download payload sizes</li> </ul>"},{"location":"about/release-notes.html#previous","title":"Previous","text":"<ul> <li>We started with a proof of concept on Windows 10+ only. Installs we via a simple BAT script, and the code has is full of exciting sharp edges. A simple dashboard and playground are included. Analysis is currently Python code only</li> <li>Version checks are enabled to alert users to new versions</li> <li>A new .NET implementation scene detection using the YOLO model to ensure the codebase is platform and tech stack agnostic</li> <li>Blue Iris integration completed</li> </ul>"},{"location":"api/api_reference.html","title":"API reference","text":"<p>The API for CodeProject.AI is divided into categories Image, Vision, Text, and Status with each category further broken into sub-topics.</p> <p>This document will continually change and be updated to reflect the latest server version and installed analysis modules</p>","tags":["api"]},{"location":"api/api_reference.html#computer-audition","title":"Computer Audition","text":"","tags":["api"]},{"location":"api/api_reference.html#sound-classifier","title":"Sound Classifier","text":"<p>Classify sound files based on the UrbanSound8K dataset.</p> <pre><code>POST: http://localhost:32168/v1/sound/classify\n</code></pre> <p>Platforms</p> <p>All</p> <p>Parameters</p> <ul> <li> <p>sound (File): The HTTP file object (WAV sound file) to be analyzed.</p> </li> <li> <p>min_confidence (Float): The minimum confidence level for successful classification. In the range 0.0 to 1.0. Default 0.4.    Optional. Defaults to 0.4</p> </li> </ul> <p>Response</p> JSON<pre><code>{\n  \"success\": (Boolean) // True if successful.\n  \"label\": (Text) // The classification label of the sound.\n  \"confidence\": (Float) // The confidence in the classification in the range of 0.0 to 1.0.\n  \"inferenceMs\": (Integer) // The time (ms) to perform the AI inference.\n  \"processMs\": (Integer) // The time (ms) to process the image (includes inference and image manipulation operations).\n  \"moduleId\": (String) // The Id of the module that processed this request.\n  \"moduleName\": (String) // The name of the module that processed this request.\n  \"command\": (String) // The command that was sent as part of this request. Can be detect, list, status.\n  \"executionProvider\": (String) // The name of the device or package handling the inference. eg CPU, GPU, TPU, DirectML.\n  \"canUseGPU\": (Boolean) // True if this module can use the current GPU if one is present.\n  \"analysisRoundTripMs\": (Integer) // The time (ms) for the round trip to the analysis module and back.\n}\n</code></pre>","tags":["api"]},{"location":"api/api_reference.html#example","title":"Example","text":"JavaScript<pre><code>// Assume we have a HTML INPUT type=file control with ID=fileChooser\nvar formData = new FormData();\nformData.append('sound', fileChooser.files[0]);\nformData.append(\"min_confidence\", 0.4);\n\nvar url = 'http://localhost:32168/v1/sound/classify';\n\nfetch(url, { method: \"POST\", body: formData})\n      .then(response =&gt; {\n           if (response.ok) {\n               response.json().then(data =&gt; {\n                   console.log(\"success: \" + data.success)\n                   console.log(\"label: \" + data.label)\n                   console.log(\"confidence: \" + data.confidence.toFixed(2))\n                   console.log(\"inferenceMs: \" + data.inferenceMs)\n                   console.log(\"processMs: \" + data.processMs)\n                   console.log(\"moduleId: \" + data.moduleId)\n                   console.log(\"moduleName: \" + data.moduleName)\n                   console.log(\"command: \" + data.command)\n                   console.log(\"executionProvider: \" + data.executionProvider)\n                   console.log(\"canUseGPU: \" + data.canUseGPU)\n                   console.log(\"analysisRoundTripMs: \" + data.analysisRoundTripMs)\n               })\n           }\n       });\n        .catch (error =&gt; {\n            console.log('Unable to complete API call: ' + error);\n       });\n</code></pre>","tags":["api"]},{"location":"api/api_reference.html#computer-vision","title":"Computer Vision","text":"","tags":["api"]},{"location":"api/api_reference.html#license-plate-reader","title":"License Plate Reader","text":"<p>Detects and reads the characters in license plates detected within an image</p> <pre><code>POST: http://localhost:32168/v1/vision/alpr\n</code></pre> <p>Platforms</p> <p>All</p> <p>Parameters</p> <ul> <li>upload (File): The image to ALPR.</li> </ul> <p>Response</p> JSON<pre><code>{\n  \"success\": (Boolean) // True if successful.\n  \"message\": (String) // A summary of the inference operation.\n  \"error\": (String) // (Optional) An description of the error if success was false.\n  \"predictions\": (Object[]) // An array of objects with the x_max, x_min, max, y_min bounds of the plate, label, the plate chars and confidence.\n  \"count\": (Integer) // The number of objects found.\n  \"inferenceMs\": (Integer) // The time (ms) to perform the AI inference.\n  \"processMs\": (Integer) // The time (ms) to process the image (includes inference and image manipulation operations).\n  \"moduleId\": (String) // The Id of the module that processed this request.\n  \"moduleName\": (String) // The name of the module that processed this request.\n  \"command\": (String) // The command that was sent as part of this request. Can be detect, list, status.\n  \"executionProvider\": (String) // The name of the device or package handling the inference. eg CPU, GPU, TPU, DirectML.\n  \"canUseGPU\": (Boolean) // True if this module can use the current GPU if one is present.\n  \"analysisRoundTripMs\": (Integer) // The time (ms) for the round trip to the analysis module and back.\n}\n</code></pre>","tags":["api"]},{"location":"api/api_reference.html#example_1","title":"Example","text":"JavaScript<pre><code>// Assume we have a HTML INPUT type=file control with ID=fileChooser\nvar formData = new FormData();\nformData.append('upload', fileChooser.files[0]);\n\nvar url = 'http://localhost:32168/v1/vision/alpr';\n\nfetch(url, { method: \"POST\", body: formData})\n      .then(response =&gt; {\n           if (response.ok) {\n               response.json().then(data =&gt; {\n                   console.log(\"success: \" + data.success)\n                   console.log(\"message: \" + data.message)\n                   console.log(\"error: \" + data.error)\n                   console.log(\"predictions: \" + (nothing returned))\n                   console.log(\"count: \" + data.count)\n                   console.log(\"inferenceMs: \" + data.inferenceMs)\n                   console.log(\"processMs: \" + data.processMs)\n                   console.log(\"moduleId: \" + data.moduleId)\n                   console.log(\"moduleName: \" + data.moduleName)\n                   console.log(\"command: \" + data.command)\n                   console.log(\"executionProvider: \" + data.executionProvider)\n                   console.log(\"canUseGPU: \" + data.canUseGPU)\n                   console.log(\"analysisRoundTripMs: \" + data.analysisRoundTripMs)\n               })\n           }\n       });\n        .catch (error =&gt; {\n            console.log('Unable to complete API call: ' + error);\n       });\n</code></pre>","tags":["api"]},{"location":"api/api_reference.html#license-plate-reader-legacy-route","title":"License Plate Reader, Legacy route","text":"<p>Detects the characters in license plates detected within an image</p> <pre><code>POST: http://localhost:32168/v1/image/alpr\n</code></pre> <p>Platforms</p> <p>All</p> <p>Parameters</p> <ul> <li>upload (File): The image to ALPR.</li> </ul> <p>Response</p> JSON<pre><code>{\n  \"success\": (Boolean) // True if successful.\n  \"message\": (String) // A summary of the inference operation.\n  \"error\": (String) // (Optional) An description of the error if success was false.\n  \"predictions\": (Object[]) // An array of objects with the x_max, x_min, max, y_min bounds of the plate, label, the plate chars and confidence.\n  \"count\": (Integer) // The number of objects found.\n  \"inferenceMs\": (Integer) // The time (ms) to perform the AI inference.\n  \"processMs\": (Integer) // The time (ms) to process the image (includes inference and image manipulation operations).\n  \"moduleId\": (String) // The Id of the module that processed this request.\n  \"moduleName\": (String) // The name of the module that processed this request.\n  \"command\": (String) // The command that was sent as part of this request. Can be detect, list, status.\n  \"executionProvider\": (String) // The name of the device or package handling the inference. eg CPU, GPU, TPU, DirectML.\n  \"canUseGPU\": (Boolean) // True if this module can use the current GPU if one is present.\n  \"analysisRoundTripMs\": (Integer) // The time (ms) for the round trip to the analysis module and back.\n}\n</code></pre>","tags":["api"]},{"location":"api/api_reference.html#example_2","title":"Example","text":"JavaScript<pre><code>// Assume we have a HTML INPUT type=file control with ID=fileChooser\nvar formData = new FormData();\nformData.append('upload', fileChooser.files[0]);\n\nvar url = 'http://localhost:32168/v1/image/alpr';\n\nfetch(url, { method: \"POST\", body: formData})\n      .then(response =&gt; {\n           if (response.ok) {\n               response.json().then(data =&gt; {\n                   console.log(\"success: \" + data.success)\n                   console.log(\"message: \" + data.message)\n                   console.log(\"error: \" + data.error)\n                   console.log(\"predictions: \" + (nothing returned))\n                   console.log(\"count: \" + data.count)\n                   console.log(\"inferenceMs: \" + data.inferenceMs)\n                   console.log(\"processMs: \" + data.processMs)\n                   console.log(\"moduleId: \" + data.moduleId)\n                   console.log(\"moduleName: \" + data.moduleName)\n                   console.log(\"command: \" + data.command)\n                   console.log(\"executionProvider: \" + data.executionProvider)\n                   console.log(\"canUseGPU: \" + data.canUseGPU)\n                   console.log(\"analysisRoundTripMs: \" + data.analysisRoundTripMs)\n               })\n           }\n       });\n        .catch (error =&gt; {\n            console.log('Unable to complete API call: ' + error);\n       });\n</code></pre>","tags":["api"]},{"location":"api/api_reference.html#license-plate-reader-rknn","title":"License Plate Reader RKNN","text":"<p>Detects the characters in license plates detected within an image</p> <pre><code>POST: http://localhost:32168/v1/vision/alpr\n</code></pre> <p>Platforms</p> <p>Orangepi</p> <p>Parameters</p> <ul> <li>upload (File): The image to ALPR.</li> </ul> <p>Response</p> JSON<pre><code>{\n  \"success\": (Boolean) // True if successful.\n  \"message\": (String) // A summary of the inference operation.\n  \"error\": (String) // (Optional) An description of the error if success was false.\n  \"predictions\": (Object[]) // An array of objects with the x_max, x_min, max, y_min bounds of the plate, label, the plate chars and confidence.\n  \"count\": (Integer) // The number of objects found.\n  \"inferenceMs\": (Integer) // The time (ms) to perform the AI inference.\n  \"processMs\": (Integer) // The time (ms) to process the image (includes inference and image manipulation operations).\n  \"moduleId\": (String) // The Id of the module that processed this request.\n  \"moduleName\": (String) // The name of the module that processed this request.\n  \"command\": (String) // The command that was sent as part of this request. Can be detect, list, status.\n  \"executionProvider\": (String) // The name of the device or package handling the inference. eg CPU, GPU, TPU, DirectML.\n  \"canUseGPU\": (Boolean) // True if this module can use the current GPU if one is present.\n  \"analysisRoundTripMs\": (Integer) // The time (ms) for the round trip to the analysis module and back.\n}\n</code></pre>","tags":["api"]},{"location":"api/api_reference.html#example_3","title":"Example","text":"JavaScript<pre><code>// Assume we have a HTML INPUT type=file control with ID=fileChooser\nvar formData = new FormData();\nformData.append('upload', fileChooser.files[0]);\n\nvar url = 'http://localhost:32168/v1/vision/alpr';\n\nfetch(url, { method: \"POST\", body: formData})\n      .then(response =&gt; {\n           if (response.ok) {\n               response.json().then(data =&gt; {\n                   console.log(\"success: \" + data.success)\n                   console.log(\"message: \" + data.message)\n                   console.log(\"error: \" + data.error)\n                   console.log(\"predictions: \" + (nothing returned))\n                   console.log(\"count: \" + data.count)\n                   console.log(\"inferenceMs: \" + data.inferenceMs)\n                   console.log(\"processMs: \" + data.processMs)\n                   console.log(\"moduleId: \" + data.moduleId)\n                   console.log(\"moduleName: \" + data.moduleName)\n                   console.log(\"command: \" + data.command)\n                   console.log(\"executionProvider: \" + data.executionProvider)\n                   console.log(\"canUseGPU: \" + data.canUseGPU)\n                   console.log(\"analysisRoundTripMs: \" + data.analysisRoundTripMs)\n               })\n           }\n       });\n        .catch (error =&gt; {\n            console.log('Unable to complete API call: ' + error);\n       });\n</code></pre>","tags":["api"]},{"location":"api/api_reference.html#license-plate-reader-rknn-legacy-route","title":"License Plate Reader RKNN, legacy route","text":"<p>Detects the characters in license plates detected within an image</p> <pre><code>POST: http://localhost:32168/v1/image/alpr\n</code></pre> <p>Platforms</p> <p>Orangepi</p> <p>Parameters</p> <ul> <li>upload (File): The image to ALPR.</li> </ul> <p>Response</p> JSON<pre><code>{\n  \"success\": (Boolean) // True if successful.\n  \"message\": (String) // A summary of the inference operation.\n  \"error\": (String) // (Optional) An description of the error if success was false.\n  \"predictions\": (Object[]) // An array of objects with the x_max, x_min, max, y_min bounds of the plate, label, the plate chars and confidence.\n  \"count\": (Integer) // The number of objects found.\n  \"inferenceMs\": (Integer) // The time (ms) to perform the AI inference.\n  \"processMs\": (Integer) // The time (ms) to process the image (includes inference and image manipulation operations).\n  \"moduleId\": (String) // The Id of the module that processed this request.\n  \"moduleName\": (String) // The name of the module that processed this request.\n  \"command\": (String) // The command that was sent as part of this request. Can be detect, list, status.\n  \"executionProvider\": (String) // The name of the device or package handling the inference. eg CPU, GPU, TPU, DirectML.\n  \"canUseGPU\": (Boolean) // True if this module can use the current GPU if one is present.\n  \"analysisRoundTripMs\": (Integer) // The time (ms) for the round trip to the analysis module and back.\n}\n</code></pre>","tags":["api"]},{"location":"api/api_reference.html#example_4","title":"Example","text":"JavaScript<pre><code>// Assume we have a HTML INPUT type=file control with ID=fileChooser\nvar formData = new FormData();\nformData.append('upload', fileChooser.files[0]);\n\nvar url = 'http://localhost:32168/v1/image/alpr';\n\nfetch(url, { method: \"POST\", body: formData})\n      .then(response =&gt; {\n           if (response.ok) {\n               response.json().then(data =&gt; {\n                   console.log(\"success: \" + data.success)\n                   console.log(\"message: \" + data.message)\n                   console.log(\"error: \" + data.error)\n                   console.log(\"predictions: \" + (nothing returned))\n                   console.log(\"count: \" + data.count)\n                   console.log(\"inferenceMs: \" + data.inferenceMs)\n                   console.log(\"processMs: \" + data.processMs)\n                   console.log(\"moduleId: \" + data.moduleId)\n                   console.log(\"moduleName: \" + data.moduleName)\n                   console.log(\"command: \" + data.command)\n                   console.log(\"executionProvider: \" + data.executionProvider)\n                   console.log(\"canUseGPU: \" + data.canUseGPU)\n                   console.log(\"analysisRoundTripMs: \" + data.analysisRoundTripMs)\n               })\n           }\n       });\n        .catch (error =&gt; {\n            console.log('Unable to complete API call: ' + error);\n       });\n</code></pre>","tags":["api"]},{"location":"api/api_reference.html#detection","title":"Detection","text":"","tags":["api"]},{"location":"api/api_reference.html#object-detection","title":"Object Detection","text":"<p>Detects multiple objects in an image using the standard YOLOv5 models trained on the COCO image sets.</p> <pre><code>POST: http://localhost:32168/v1/vision/detection\n</code></pre> <p>The object detection module uses YOLO (You Only Look Once) to locate and  classify the objects the models have been trained on. At this point there are  80 different types of objects that can be detected:</p> <ul> <li>person</li> <li>bicycle, car, motorbike, aeroplane, bus, train, truck, boat</li> <li>traffic light, fire hydrant, stop sign, parking meter, bench</li> <li>cat, dog, horse, sheep, cow, elephant, bear, zebra, giraffe</li> <li>backpack, umbrella, handbag, tie, suitcase, frisbee, skis, snowboard, sports ball, kite, baseball bat, baseball glove, skateboard, surfboard, tennis racket</li> <li>bottle, wine glass, cup, fork, knife, spoon, bowl</li> <li>banana, apple, sandwich, orange, broccoli, carrot, hot dog, pizza, donut, cake</li> <li>chair, sofa, pottedplant, bed, diningtable, toilet, tvmonitor, laptop, mouse, remote, keyboard, cell phone, microwave, oven, toaster, sink, refrigerator, book, clock, vase, scissors, teddy bear, hair drier, toothbrush</li> </ul> <p>Platforms</p> <p>Windows, Linux, macOS, macOS-Arm, Docker</p> <p>Parameters</p> <ul> <li> <p>image (File): The HTTP File Object (image) to be analyzed.</p> </li> <li> <p>min_confidence (Float): The minimum confidence level for an object will be detected. In the range 0.0 to 1.0. Default 0.4.</p> </li> </ul> <p>Response</p> JSON<pre><code>{\n  \"success\": (Boolean) // True if successful.\n  \"message\": (String) // A summary of the inference operation.\n  \"error\": (String) // (Optional) An description of the error if success was false.\n  \"predictions\": (Object) // An array of objects with the x_max, x_min, max, y_min, label and confidence.\n  \"count\": (Integer) // The number of objects found.\n  \"inferenceMs\": (Integer) // The time (ms) to perform the AI inference.\n  \"processMs\": (Integer) // The time (ms) to process the image (includes inference and image manipulation operations).\n  \"moduleId\": (String) // The Id of the module that processed this request.\n  \"moduleName\": (String) // The name of the module that processed this request.\n  \"command\": (String) // The command that was sent as part of this request. Can be detect, list, status.\n  \"executionProvider\": (String) // The name of the device or package handling the inference. eg CPU, GPU, TPU, DirectML.\n  \"canUseGPU\": (Boolean) // True if this module can use the current GPU if one is present.\n  \"analysisRoundTripMs\": (Integer) // The time (ms) for the round trip to the analysis module and back.\n}\n</code></pre>","tags":["api"]},{"location":"api/api_reference.html#example_5","title":"Example","text":"PythonJavscript <pre><code>import requests\n\nimage_data = open(\"my_image.jpg\",\"rb\").read()\n\nresponse = requests.post(\"http://localhost:32168/v1/vision/detection\",\n                         files={\"image\":image_data}).json()\n\nfor object in response[\"predictions\"]:\n    print(object[\"label\"])\n\nprint(response)\n</code></pre> JavaScript<pre><code>// Assume we have a HTML INPUT type=file control with ID=fileChooser\nvar formData = new FormData();\nformData.append('image', fileChooser.files[0]);\nformData.append(\"min_confidence\", 0.4);\n\nvar url = 'http://localhost:32168/v1/vision/detection';\n\nfetch(url, { method: \"POST\", body: formData})\n  .then(response =&gt; {\n       if (response.ok) {\n           response.json().then(data =&gt; {\n               console.log(\"success: \" + data.success)\n               console.log(\"message: \" + data.message)\n               console.log(\"error: \" + data.error)\n               console.log(\"predictions: \" + JSON.stringify(data.predictions))\n               console.log(\"count: \" + data.count)\n               console.log(\"command: \" + data.command)\n               console.log(\"moduleId: \" + data.moduleId)\n               console.log(\"executionProvider: \" + data.executionProvider)\n               console.log(\"canUseGPU: \" + data.canUseGPU)\n               console.log(\"inferenceMs: \" + data.inferenceMs)\n               console.log(\"processMs: \" + data.processMs)\n               console.log(\"analysisRoundTripMs: \" + data.analysisRoundTripMs)\n           })\n       }\n   });\n    .catch (error =&gt; {\n        console.log('Unable to complete API call: ' + error);\n   });\n</code></pre> <p>Response</p> <pre><code>dog\nperson\nperson\n{'predictions': [ {'x_max': 600, 'x_min': 400, 'y_min': 200, 'y_max': 400,\n   'confidence': 95, 'label': 'car' },{'x_max': 100, 'x_min': 200, 'y_min': 50,\n   'y_max': 100, 'confidence': 90, 'label': 'dog' },], 'success': True}\n</code></pre>","tags":["api"]},{"location":"api/api_reference.html#custom-object-detector","title":"Custom Object Detector","text":"<p>Detects objects based on a custom model. </p> <p>There are multiple OBject Detection modules available, and each module will store its own set of custom models. Use the List Models API to list the models that a given module supports.</p> <p>To make a call to a specific model use /vision/custom/model-name, where 'model-name' is the name of the model file (without the file extension)</p> <pre><code>POST: http://localhost:32168/v1/vision/custom/model-name\n</code></pre> <p>The object detection modules generally use YOLO (You Only Look Once) to locate and  classify the objects the models have been trained on. The custom models  included by default are</p> <ul> <li>ipcam-animal - bird, cat, dog, horse, sheep, cow, bear, deer, rabbit, raccoon, fox, skunk, squirrel, pig</li> <li>ipcam-dark - Bicycle, Bus, Car, Cat, Dog, Motorcycle, Person</li> <li>ipcam-general - person, vehicle, plus objects in ipcam-dark</li> <li>ipcam-combined - person, bicycle, car, motorcycle, bus, truck, bird, cat, dog, horse, sheep, cow, bear, deer, rabbit, raccoon, fox, skunk, squirrel, pig</li> </ul> <p>The exception is the Tensorflow-Lite module which does not (yet) offer custom models.</p> <p>Platforms</p> <p>All</p> <p>Parameters</p> <ul> <li> <p>image (File): The HTTP file object (image) to be analyzed.</p> </li> <li> <p>min_confidence (Float): The minimum confidence level for an object will be detected. In the range 0.0 to 1.0. Default 0.4.</p> </li> </ul> <p>Response</p> JSON<pre><code>{\n  \"success\": (Boolean) // True if successful.\n  \"message\": (String) // A summary of the inference operation.\n  \"error\": (String) // (Optional) An description of the error if success was false.\n  \"predictions\": (Object) // An array of objects with the x_max, x_min, max, y_min, label and confidence.\n  \"count\": (Integer) // The number of objects found.\n  \"inferenceMs\": (Integer) // The time (ms) to perform the AI inference.\n  \"processMs\": (Integer) // The time (ms) to process the image (includes inference and image manipulation operations).\n  \"moduleId\": (String) // The Id of the module that processed this request.\n  \"moduleName\": (String) // The name of the module that processed this request.\n  \"command\": (String) // The command that was sent as part of this request. Can be detect, list, status.\n  \"executionProvider\": (String) // The name of the device or package handling the inference. eg CPU, GPU, TPU, DirectML.\n  \"canUseGPU\": (Boolean) // True if this module can use the current GPU if one is present.\n  \"analysisRoundTripMs\": (Integer) // The time (ms) for the round trip to the analysis module and back.\n}\n</code></pre>","tags":["api"]},{"location":"api/api_reference.html#example_6","title":"Example","text":"JavaScript<pre><code>// Assume we have a HTML INPUT type=file control with ID=fileChooser\nvar formData = new FormData();\nformData.append('image', fileChooser.files[0]);\nformData.append(\"min_confidence\", 0.0);\n\n// Assumes we have licence-plates.pt in the /assets dir\nvar url = 'http://localhost:32168/v1/vision/custom/licence-plates';\n\nfetch(url, { method: \"POST\", body: formData})\n      .then(response =&gt; {\n           if (response.ok) {\n               response.json().then(data =&gt; {\n                   console.log(\"success: \" + data.success)\n                   console.log(\"message: \" + data.message)\n                   console.log(\"predictions: \" + JSON.stringify(data.predictions))\n                   console.log(\"count: \" + data.count)\n                   console.log(\"command: \" + data.command)\n                   console.log(\"moduleId: \" + data.moduleId)\n                   console.log(\"executionProvider: \" + data.executionProvider)\n                   console.log(\"canUseGPU: \" + data.canUseGPU)\n                   console.log(\"inferenceMs: \" + data.inferenceMs)\n                   console.log(\"processMs: \" + data.processMs)\n                   console.log(\"analysisRoundTripMs: \" + data.analysisRoundTripMs)\n               })\n           }\n       });\n        .catch (error =&gt; {\n            console.log('Unable to complete API call: ' + error);\n       });\n</code></pre>","tags":["api"]},{"location":"api/api_reference.html#custom-object-detector-list-models","title":"Custom Object Detector List Models","text":"<p>Returns a list of models available.</p> <pre><code>POST: http://localhost:32168/v1/vision/custom/list\n</code></pre> <p>Platforms</p> <p>All</p> <p>Parameters</p> <p>(None)</p> <p>Response</p> JSON<pre><code>{\n  \"success\": (Boolean) // True if successful.\n  \"message\": (String) // A summary of the inference operation.\n  \"error\": (String) // (Optional) An description of the error if success was false.\n  \"predictions\": (Object) // An array of objects with the x_max, x_min, max, y_min, label and confidence.\n  \"count\": (Integer) // The number of objects found.\n  \"inferenceMs\": (Integer) // The time (ms) to perform the AI inference.\n  \"processMs\": (Integer) // The time (ms) to process the image (includes inference and image manipulation operations).\n  \"moduleId\": (String) // The Id of the module that processed this request.\n  \"moduleName\": (String) // The name of the module that processed this request.\n  \"command\": (String) // The command that was sent as part of this request. Can be detect, list, status.\n  \"executionProvider\": (String) // The name of the device or package handling the inference. eg CPU, GPU, TPU, DirectML.\n  \"canUseGPU\": (Boolean) // True if this module can use the current GPU if one is present.\n  \"analysisRoundTripMs\": (Integer) // The time (ms) for the round trip to the analysis module and back.\n}\n</code></pre>","tags":["api"]},{"location":"api/api_reference.html#example_7","title":"Example","text":"JavaScript<pre><code>var url = 'http://localhost:32168/v1/vision/custom/list';\n\nfetch(url, { method: \"POST\", body: formData})\n      .then(response =&gt; {\n           if (response.ok) {\n               response.json().then(data =&gt; {\n                   console.log(\"success: \" + data.success)\n                   console.log(\"models: \" + data.models)\n               })\n           }\n       });\n        .catch (error =&gt; {\n            console.log('Unable to complete API call: ' + error);\n       });\n</code></pre>","tags":["api"]},{"location":"api/api_reference.html#scene-classifier","title":"Scene Classifier","text":"<p>Classifies the scene in an image. It can recognise 365 different scenes.</p> <pre><code>POST: http://localhost:32168/v1/vision/scene\n</code></pre> <p>Platforms</p> <p>All, !Linux, !Raspberrypi, !Jetson</p> <p>Parameters</p> <ul> <li>image (File): The HTTP file object (image) to be analyzed.</li> </ul> <p>Response</p> JSON<pre><code>{\n  \"success\": (Boolean) // True if successful.\n  \"label\": (Text) // The classification of the scene such as 'conference_room'.\n  \"confidence\": (Float) // The confidence in the classification in the range of 0.0 to 1.0.\n  \"inferenceMs\": (Integer) // The time (ms) to perform the AI inference.\n  \"processMs\": (Integer) // The time (ms) to process the image (includes inference and image manipulation operations).\n  \"moduleId\": (String) // The Id of the module that processed this request.\n  \"moduleName\": (String) // The name of the module that processed this request.\n  \"command\": (String) // The command that was sent as part of this request. Can be detect, list, status.\n  \"executionProvider\": (String) // The name of the device or package handling the inference. eg CPU, GPU, TPU, DirectML.\n  \"canUseGPU\": (Boolean) // True if this module can use the current GPU if one is present.\n  \"analysisRoundTripMs\": (Integer) // The time (ms) for the round trip to the analysis module and back.\n}\n</code></pre>","tags":["api"]},{"location":"api/api_reference.html#example_8","title":"Example","text":"JavaScript<pre><code>// Assume we have a HTML INPUT type=file control with ID=fileChooser\nvar formData = new FormData();\nformData.append('image', fileChooser.files[0]);\n\nvar url = 'http://localhost:32168/v1/vision/scene';\n\nfetch(url, { method: \"POST\", body: formData})\n      .then(response =&gt; {\n           if (response.ok) {\n               response.json().then(data =&gt; {\n                   console.log(\"success: \" + data.success)\n                   console.log(\"label: \" + data.label)\n                   console.log(\"confidence: \" + data.confidence.toFixed(2))\n                   console.log(\"inferenceMs: \" + data.inferenceMs)\n                   console.log(\"processMs: \" + data.processMs)\n                   console.log(\"moduleId: \" + data.moduleId)\n                   console.log(\"moduleName: \" + data.moduleName)\n                   console.log(\"command: \" + data.command)\n                   console.log(\"executionProvider: \" + data.executionProvider)\n                   console.log(\"canUseGPU: \" + data.canUseGPU)\n                   console.log(\"analysisRoundTripMs: \" + data.analysisRoundTripMs)\n               })\n           }\n       });\n        .catch (error =&gt; {\n            console.log('Unable to complete API call: ' + error);\n       });\n</code></pre>","tags":["api"]},{"location":"api/api_reference.html#face-recognition","title":"Face Recognition","text":"","tags":["api"]},{"location":"api/api_reference.html#face-detection","title":"Face Detection","text":"<p>Detects faces in an image and returns the coordinates of the faces.</p> <pre><code>POST: http://localhost:32168/v1/vision/face\n</code></pre> <p>Platforms</p> <p>All, !Raspberrypi, !Jetson</p> <p>Parameters</p> <ul> <li> <p>image (File): The HTTP File Object (image) to be analyzed.</p> </li> <li> <p>min_confidence (Float): The minimum confidence level for an object will be detected. In the range 0.0 to 1.0.    Optional. Defaults to 0.4</p> </li> </ul> <p>Response</p> JSON<pre><code>{\n  \"success\": (Boolean) // True if successful.\n  \"message\": (String) // A summary of the inference operation.\n  \"error\": (String) // (Optional) An description of the error if success was false.\n  \"predictions\": (Object) // An array of objects with the x_max, x_min, max, y_min, label and confidence.\n  \"inferenceMs\": (Integer) // The time (ms) to perform the AI inference.\n  \"processMs\": (Integer) // The time (ms) to process the image (includes inference and image manipulation operations).\n  \"moduleId\": (String) // The Id of the module that processed this request.\n  \"moduleName\": (String) // The name of the module that processed this request.\n  \"command\": (String) // The command that was sent as part of this request. Can be detect, list, status.\n  \"executionProvider\": (String) // The name of the device or package handling the inference. eg CPU, GPU, TPU, DirectML.\n  \"canUseGPU\": (Boolean) // True if this module can use the current GPU if one is present.\n  \"analysisRoundTripMs\": (Integer) // The time (ms) for the round trip to the analysis module and back.\n}\n</code></pre>","tags":["api"]},{"location":"api/api_reference.html#example_9","title":"Example","text":"JavaScript<pre><code>// Assume we have a HTML INPUT type=file control with ID=fileChooser\nvar formData = new FormData();\nformData.append('image', fileChooser.files[0]);\nformData.append(\"min_confidence\", 0.4);\n\nvar url = 'http://localhost:32168/v1/vision/face';\n\nfetch(url, { method: \"POST\", body: formData})\n      .then(response =&gt; {\n           if (response.ok) {\n               response.json().then(data =&gt; {\n                   console.log(\"success: \" + data.success)\n                   console.log(\"message: \" + data.message)\n                   console.log(\"error: \" + data.error)\n                   console.log(\"predictions: \" + JSON.stringify(data.predictions))\n                   console.log(\"inferenceMs: \" + data.inferenceMs)\n                   console.log(\"processMs: \" + data.processMs)\n                   console.log(\"moduleId: \" + data.moduleId)\n                   console.log(\"moduleName: \" + data.moduleName)\n                   console.log(\"command: \" + data.command)\n                   console.log(\"executionProvider: \" + data.executionProvider)\n                   console.log(\"canUseGPU: \" + data.canUseGPU)\n                   console.log(\"analysisRoundTripMs: \" + data.analysisRoundTripMs)\n               })\n           }\n       });\n        .catch (error =&gt; {\n            console.log('Unable to complete API call: ' + error);\n       });\n</code></pre>","tags":["api"]},{"location":"api/api_reference.html#face-comparison","title":"Face Comparison","text":"<p>Compares two faces in two images and returns a value indicating how similar the faces are.</p> <pre><code>POST: http://localhost:32168/v1/vision/face/match\n</code></pre> <p>Platforms</p> <p>All, !Raspberrypi, !Jetson</p> <p>Parameters</p> <ul> <li> <p>image1 (File): First HTTP File Object (image) to be analyzed.</p> </li> <li> <p>image2 (File): Second HTTP File Object (image) to be analyzed.</p> </li> </ul> <p>Response</p> JSON<pre><code>{\n  \"success\": (Boolean) // True if successful.\n  \"similarity\": (Float) // How similar the two images are, in the range of 0.0 to 1.0.\n  \"inferenceMs\": (Integer) // The time (ms) to perform the AI inference.\n  \"processMs\": (Integer) // The time (ms) to process the image (includes inference and image manipulation operations).\n  \"moduleId\": (String) // The Id of the module that processed this request.\n  \"moduleName\": (String) // The name of the module that processed this request.\n  \"command\": (String) // The command that was sent as part of this request. Can be detect, list, status.\n  \"executionProvider\": (String) // The name of the device or package handling the inference. eg CPU, GPU, TPU, DirectML.\n  \"canUseGPU\": (Boolean) // True if this module can use the current GPU if one is present.\n  \"analysisRoundTripMs\": (Integer) // The time (ms) for the round trip to the analysis module and back.\n}\n</code></pre>","tags":["api"]},{"location":"api/api_reference.html#example_10","title":"Example","text":"JavaScript<pre><code>// Assume we have a HTML INPUT type=file control with ID=fileChooser\nvar formData = new FormData();\nformData.append('image1', fileChooser.files[0]);\nformData.append('image2', fileChooser.files[1]);\n\nvar url = 'http://localhost:32168/v1/vision/face/match';\n\nfetch(url, { method: \"POST\", body: formData})\n      .then(response =&gt; {\n           if (response.ok) {\n               response.json().then(data =&gt; {\n                   console.log(\"success: \" + data.success)\n                   console.log(\"similarity: \" + data.similarity.toFixed(2))\n                   console.log(\"inferenceMs: \" + data.inferenceMs)\n                   console.log(\"processMs: \" + data.processMs)\n                   console.log(\"moduleId: \" + data.moduleId)\n                   console.log(\"moduleName: \" + data.moduleName)\n                   console.log(\"command: \" + data.command)\n                   console.log(\"executionProvider: \" + data.executionProvider)\n                   console.log(\"canUseGPU: \" + data.canUseGPU)\n                   console.log(\"analysisRoundTripMs: \" + data.analysisRoundTripMs)\n               })\n           }\n       });\n        .catch (error =&gt; {\n            console.log('Unable to complete API call: ' + error);\n       });\n</code></pre>","tags":["api"]},{"location":"api/api_reference.html#list-registered-faces","title":"List Registered Faces","text":"<p>Lists the users that have images registered in the Face Recognition database.</p> <pre><code>POST: http://localhost:32168/v1/vision/face/list\n</code></pre> <p>Platforms</p> <p>All, !Raspberrypi, !Jetson</p> <p>Parameters</p> <p>(None)</p> <p>Response</p> JSON<pre><code>{\n  \"success\": (Boolean) // True if successful.\n  \"faces\": (Object) // An array of the userid strings for users with registered images.\n  \"moduleId\": (String) // The Id of the module that processed this request.\n  \"moduleName\": (String) // The name of the module that processed this request.\n  \"command\": (String) // The command that was sent as part of this request. Can be detect, list, status.\n  \"executionProvider\": (String) // The name of the device or package handling the inference. eg CPU, GPU, TPU, DirectML.\n  \"canUseGPU\": (Boolean) // True if this module can use the current GPU if one is present.\n  \"analysisRoundTripMs\": (Integer) // The time (ms) for the round trip to the analysis module and back.\n}\n</code></pre>","tags":["api"]},{"location":"api/api_reference.html#example_11","title":"Example","text":"JavaScript<pre><code>var url = 'http://localhost:32168/v1/vision/face/list';\n\nfetch(url, { method: \"POST\", body: formData})\n      .then(response =&gt; {\n           if (response.ok) {\n               response.json().then(data =&gt; {\n                   console.log(\"success: \" + data.success)\n                   console.log(\"faces: \" + JSON.stringify(data.faces))\n                   console.log(\"moduleId: \" + data.moduleId)\n                   console.log(\"moduleName: \" + data.moduleName)\n                   console.log(\"command: \" + data.command)\n                   console.log(\"executionProvider: \" + data.executionProvider)\n                   console.log(\"canUseGPU: \" + data.canUseGPU)\n                   console.log(\"analysisRoundTripMs: \" + data.analysisRoundTripMs)\n               })\n           }\n       });\n        .catch (error =&gt; {\n            console.log('Unable to complete API call: ' + error);\n       });\n</code></pre>","tags":["api"]},{"location":"api/api_reference.html#register-face","title":"Register Face","text":"<p>Registers one or more images for a user for recognition. This trains the face recognition model and allows the face recognition to report back a userId based on an image you supply that may or may not contain that user's face.</p> <pre><code>POST: http://localhost:32168/v1/vision/face/register\n</code></pre> <p>Platforms</p> <p>All, !Raspberrypi, !Jetson</p> <p>Parameters</p> <ul> <li> <p>imageN (File): The one or more HTTP File Objects (images) to be registered.</p> </li> <li> <p>userid (Text): The identifying string for the user.</p> </li> </ul> <p>Response</p> JSON<pre><code>{\n  \"success\": (Boolean) // True if successful.\n  \"Message\": (Text) // face added\n  \"inferenceMs\": (Integer) // The time (ms) to perform the AI inference.\n  \"processMs\": (Integer) // The time (ms) to process the image (includes inference and image manipulation operations).\n  \"moduleId\": (String) // The Id of the module that processed this request.\n  \"moduleName\": (String) // The name of the module that processed this request.\n  \"command\": (String) // The command that was sent as part of this request. Can be detect, list, status.\n  \"executionProvider\": (String) // The name of the device or package handling the inference. eg CPU, GPU, TPU, DirectML.\n  \"canUseGPU\": (Boolean) // True if this module can use the current GPU if one is present.\n  \"analysisRoundTripMs\": (Integer) // The time (ms) for the round trip to the analysis module and back.\n}\n</code></pre>","tags":["api"]},{"location":"api/api_reference.html#example_12","title":"Example","text":"JavaScript<pre><code>// Assume we have a HTML INPUT type=file control with ID=fileChooser\nvar formData = new FormData();\nformData.append('imageN', fileChooser.files[0]);\nformData.append(\"userid\", '');\n\nvar url = 'http://localhost:32168/v1/vision/face/register';\n\nfetch(url, { method: \"POST\", body: formData})\n      .then(response =&gt; {\n           if (response.ok) {\n               response.json().then(data =&gt; {\n                   console.log(\"success: \" + data.success)\n                   console.log(\"Message: \" + data.Message)\n                   console.log(\"inferenceMs: \" + data.inferenceMs)\n                   console.log(\"processMs: \" + data.processMs)\n                   console.log(\"moduleId: \" + data.moduleId)\n                   console.log(\"moduleName: \" + data.moduleName)\n                   console.log(\"command: \" + data.command)\n                   console.log(\"executionProvider: \" + data.executionProvider)\n                   console.log(\"canUseGPU: \" + data.canUseGPU)\n                   console.log(\"analysisRoundTripMs: \" + data.analysisRoundTripMs)\n               })\n           }\n       });\n        .catch (error =&gt; {\n            console.log('Unable to complete API call: ' + error);\n       });\n</code></pre>","tags":["api"]},{"location":"api/api_reference.html#delete-registered-face","title":"Delete Registered Face","text":"<p>Removes a userid and images from the Face Registration database.</p> <pre><code>POST: http://localhost:32168/v1/vision/face/delete\n</code></pre> <p>Platforms</p> <p>All, !Raspberrypi, !Jetson</p> <p>Parameters</p> <ul> <li>userid (Text): The identifying string for the user.</li> </ul> <p>Response</p> JSON<pre><code>{\n  \"success\": (Boolean) // True if successful.\n  \"moduleId\": (String) // The Id of the module that processed this request.\n  \"moduleName\": (String) // The name of the module that processed this request.\n  \"command\": (String) // The command that was sent as part of this request. Can be detect, list, status.\n  \"executionProvider\": (String) // The name of the device or package handling the inference. eg CPU, GPU, TPU, DirectML.\n  \"canUseGPU\": (Boolean) // True if this module can use the current GPU if one is present.\n  \"analysisRoundTripMs\": (Integer) // The time (ms) for the round trip to the analysis module and back.\n}\n</code></pre>","tags":["api"]},{"location":"api/api_reference.html#example_13","title":"Example","text":"JavaScript<pre><code>var formData = new FormData();\nformData.append(\"userid\", '');\n\nvar url = 'http://localhost:32168/v1/vision/face/delete';\n\nfetch(url, { method: \"POST\", body: formData})\n      .then(response =&gt; {\n           if (response.ok) {\n               response.json().then(data =&gt; {\n                   console.log(\"success: \" + data.success)\n                   console.log(\"moduleId: \" + data.moduleId)\n                   console.log(\"moduleName: \" + data.moduleName)\n                   console.log(\"command: \" + data.command)\n                   console.log(\"executionProvider: \" + data.executionProvider)\n                   console.log(\"canUseGPU: \" + data.canUseGPU)\n                   console.log(\"analysisRoundTripMs: \" + data.analysisRoundTripMs)\n               })\n           }\n       });\n        .catch (error =&gt; {\n            console.log('Unable to complete API call: ' + error);\n       });\n</code></pre>","tags":["api"]},{"location":"api/api_reference.html#face-recognition_1","title":"Face Recognition","text":"<p>Recognizes all faces in an image and returns the userId and coordinates of each face in the image. If a new (unregistered) face is detected then no userid for that face will be returned.</p> <pre><code>POST: http://localhost:32168/v1/vision/face/recognize\n</code></pre> <p>Platforms</p> <p>All, !Raspberrypi, !Jetson</p> <p>Parameters</p> <ul> <li> <p>image (File): The HTTP file object (image) to be analyzed.</p> </li> <li> <p>min_confidence (Float): The minimum confidence level for an object will be detected. In the range 0.0 to 1.0.    Optional. Defaults to 0.4</p> </li> </ul> <p>Response</p> JSON<pre><code>{\n  \"success\": (Boolean) // True if successful.\n  \"predictions\": (Object) // An array of objects with the userid, x_max, x_min, max, y_min, label and confidence.\n  \"inferenceMs\": (Integer) // The time (ms) to perform the AI inference.\n  \"processMs\": (Integer) // The time (ms) to process the image (includes inference and image manipulation operations).\n  \"moduleId\": (String) // The Id of the module that processed this request.\n  \"moduleName\": (String) // The name of the module that processed this request.\n  \"command\": (String) // The command that was sent as part of this request. Can be detect, list, status.\n  \"executionProvider\": (String) // The name of the device or package handling the inference. eg CPU, GPU, TPU, DirectML.\n  \"canUseGPU\": (Boolean) // True if this module can use the current GPU if one is present.\n  \"analysisRoundTripMs\": (Integer) // The time (ms) for the round trip to the analysis module and back.\n}\n</code></pre>","tags":["api"]},{"location":"api/api_reference.html#example_14","title":"Example","text":"JavaScript<pre><code>// Assume we have a HTML INPUT type=file control with ID=fileChooser\nvar formData = new FormData();\nformData.append('image', fileChooser.files[0]);\nformData.append(\"min_confidence\", 0.4);\n\nvar url = 'http://localhost:32168/v1/vision/face/recognize';\n\nfetch(url, { method: \"POST\", body: formData})\n      .then(response =&gt; {\n           if (response.ok) {\n               response.json().then(data =&gt; {\n                   console.log(\"success: \" + data.success)\n                   console.log(\"predictions: \" + JSON.stringify(data.predictions))\n                   console.log(\"inferenceMs: \" + data.inferenceMs)\n                   console.log(\"processMs: \" + data.processMs)\n                   console.log(\"moduleId: \" + data.moduleId)\n                   console.log(\"moduleName: \" + data.moduleName)\n                   console.log(\"command: \" + data.command)\n                   console.log(\"executionProvider: \" + data.executionProvider)\n                   console.log(\"canUseGPU: \" + data.canUseGPU)\n                   console.log(\"analysisRoundTripMs: \" + data.analysisRoundTripMs)\n               })\n           }\n       });\n        .catch (error =&gt; {\n            console.log('Unable to complete API call: ' + error);\n       });\n</code></pre>","tags":["api"]},{"location":"api/api_reference.html#image-processing","title":"Image Processing","text":"","tags":["api"]},{"location":"api/api_reference.html#background-remover","title":"Background Remover","text":"<p>Removes the background from behind the main subjects in images.</p> <pre><code>POST: http://localhost:32168/v1/image/removebackground\n</code></pre> <p>Platforms</p> <p>All, !Linux, !Raspberrypi, !Orangepi, !Jetson</p> <p>Parameters</p> <ul> <li> <p>image (File): The image to have its background removed.</p> </li> <li> <p>use_alphamatting (Boolean): Whether or not to use alpha matting.    Optional. Defaults to false</p> </li> </ul> <p>Response</p> JSON<pre><code>{\n  \"success\": (Boolean) // True if successful.\n  \"imageBase64\": (Base64ImageData) // The base64 encoded image that has had its background removed.\n  \"inferenceMs\": (Integer) // The time (ms) to perform the AI inference.\n  \"processMs\": (Integer) // The time (ms) to process the image (includes inference and image manipulation operations).\n  \"moduleId\": (String) // The Id of the module that processed this request.\n  \"moduleName\": (String) // The name of the module that processed this request.\n  \"command\": (String) // The command that was sent as part of this request. Can be detect, list, status.\n  \"executionProvider\": (String) // The name of the device or package handling the inference. eg CPU, GPU, TPU, DirectML.\n  \"canUseGPU\": (Boolean) // True if this module can use the current GPU if one is present.\n  \"analysisRoundTripMs\": (Integer) // The time (ms) for the round trip to the analysis module and back.\n}\n</code></pre>","tags":["api"]},{"location":"api/api_reference.html#example_15","title":"Example","text":"JavaScript<pre><code>// Assume we have a HTML INPUT type=file control with ID=fileChooser\nvar formData = new FormData();\nformData.append('image', fileChooser.files[0]);\nformData.append(\"use_alphamatting\", false);\n\nvar url = 'http://localhost:32168/v1/image/removebackground';\n\nfetch(url, { method: \"POST\", body: formData})\n      .then(response =&gt; {\n           if (response.ok) {\n               response.json().then(data =&gt; {\n                   console.log(\"success: \" + data.success)\n                   // Assume we have an IMG tag named img1\n                   img1.src = \"data:image/png;base64,\" + data.imageBase64;\n                   console.log(\"inferenceMs: \" + data.inferenceMs)\n                   console.log(\"processMs: \" + data.processMs)\n                   console.log(\"moduleId: \" + data.moduleId)\n                   console.log(\"moduleName: \" + data.moduleName)\n                   console.log(\"command: \" + data.command)\n                   console.log(\"executionProvider: \" + data.executionProvider)\n                   console.log(\"canUseGPU: \" + data.canUseGPU)\n                   console.log(\"analysisRoundTripMs: \" + data.analysisRoundTripMs)\n               })\n           }\n       });\n        .catch (error =&gt; {\n            console.log('Unable to complete API call: ' + error);\n       });\n</code></pre>","tags":["api"]},{"location":"api/api_reference.html#cartooniser","title":"Cartooniser","text":"<p>Convert a photo into an anime style cartoon.</p> <pre><code>POST: http://localhost:32168/v1/image/cartoonise\n</code></pre> <p>Platforms</p> <p>All, !Raspberrypi, !Orangepi, !Jetson</p> <p>Parameters</p> <ul> <li> <p>image (File): The image to be converted.</p> </li> <li> <p>model (String): Name of the model to use</p> </li> </ul> <p>Response</p> JSON<pre><code>{\n  \"success\": (Boolean) // True if successful.\n  \"imageBase64\": (Base64ImageData) // The base64 encoded image.\n  \"inferenceMs\": (Integer) // The time (ms) to perform the AI inference.\n  \"processMs\": (Integer) // The time (ms) to process the image (includes inference and image manipulation operations).\n  \"moduleId\": (String) // The Id of the module that processed this request.\n  \"moduleName\": (String) // The name of the module that processed this request.\n  \"command\": (String) // The command that was sent as part of this request. Can be detect, list, status.\n  \"executionProvider\": (String) // The name of the device or package handling the inference. eg CPU, GPU, TPU, DirectML.\n  \"canUseGPU\": (Boolean) // True if this module can use the current GPU if one is present.\n  \"analysisRoundTripMs\": (Integer) // The time (ms) for the round trip to the analysis module and back.\n}\n</code></pre>","tags":["api"]},{"location":"api/api_reference.html#example_16","title":"Example","text":"JavaScript<pre><code>// Assume we have a HTML INPUT type=file control with ID=fileChooser\nvar formData = new FormData();\nformData.append('image', fileChooser.files[0]);\nformData.append(\"model\", );\n\nvar url = 'http://localhost:32168/v1/image/cartoonise';\n\nfetch(url, { method: \"POST\", body: formData})\n      .then(response =&gt; {\n           if (response.ok) {\n               response.json().then(data =&gt; {\n                   console.log(\"success: \" + data.success)\n                   // Assume we have an IMG tag named img1\n                   img1.src = \"data:image/png;base64,\" + data.imageBase64;\n                   console.log(\"inferenceMs: \" + data.inferenceMs)\n                   console.log(\"processMs: \" + data.processMs)\n                   console.log(\"moduleId: \" + data.moduleId)\n                   console.log(\"moduleName: \" + data.moduleName)\n                   console.log(\"command: \" + data.command)\n                   console.log(\"executionProvider: \" + data.executionProvider)\n                   console.log(\"canUseGPU: \" + data.canUseGPU)\n                   console.log(\"analysisRoundTripMs: \" + data.analysisRoundTripMs)\n               })\n           }\n       });\n        .catch (error =&gt; {\n            console.log('Unable to complete API call: ' + error);\n       });\n</code></pre>","tags":["api"]},{"location":"api/api_reference.html#portrait-filter","title":"Portrait Filter","text":"<p>Blurs the background behind the main subjects in an image.</p> <pre><code>POST: http://localhost:32168/v1/image/portraitfilter\n</code></pre> <p>Platforms</p> <p>Windows</p> <p>Parameters</p> <ul> <li> <p>image (File): The image to be filtered.</p> </li> <li> <p>strength (Float): How much to blur the background (0.0 - 1.0).    Optional. Defaults to 0.5</p> </li> </ul> <p>Response</p> JSON<pre><code>{\n  \"success\": (Boolean) // True if successful.\n  \"filtered_image\": (Base64ImageData) // The base64 encoded image that has had its background blurred.\n  \"inferenceMs\": (Integer) // The time (ms) to perform the AI inference.\n  \"processMs\": (Integer) // The time (ms) to process the image (includes inference and image manipulation operations).\n  \"moduleId\": (String) // The Id of the module that processed this request.\n  \"moduleName\": (String) // The name of the module that processed this request.\n  \"command\": (String) // The command that was sent as part of this request. Can be detect, list, status.\n  \"executionProvider\": (String) // The name of the device or package handling the inference. eg CPU, GPU, TPU, DirectML.\n  \"canUseGPU\": (Boolean) // True if this module can use the current GPU if one is present.\n  \"analysisRoundTripMs\": (Integer) // The time (ms) for the round trip to the analysis module and back.\n}\n</code></pre>","tags":["api"]},{"location":"api/api_reference.html#example_17","title":"Example","text":"JavaScript<pre><code>// Assume we have a HTML INPUT type=file control with ID=fileChooser\nvar formData = new FormData();\nformData.append('image', fileChooser.files[0]);\nformData.append(\"strength\", 0.5);\n\nvar url = 'http://localhost:32168/v1/image/portraitfilter';\n\nfetch(url, { method: \"POST\", body: formData})\n      .then(response =&gt; {\n           if (response.ok) {\n               response.json().then(data =&gt; {\n                   console.log(\"success: \" + data.success)\n                   // Assume we have an IMG tag named img1\n                   img1.src = \"data:image/png;base64,\" + data.filtered_image;\n                   console.log(\"inferenceMs: \" + data.inferenceMs)\n                   console.log(\"processMs: \" + data.processMs)\n                   console.log(\"moduleId: \" + data.moduleId)\n                   console.log(\"moduleName: \" + data.moduleName)\n                   console.log(\"command: \" + data.command)\n                   console.log(\"executionProvider: \" + data.executionProvider)\n                   console.log(\"canUseGPU: \" + data.canUseGPU)\n                   console.log(\"analysisRoundTripMs: \" + data.analysisRoundTripMs)\n               })\n           }\n       });\n        .catch (error =&gt; {\n            console.log('Unable to complete API call: ' + error);\n       });\n</code></pre>","tags":["api"]},{"location":"api/api_reference.html#super-resolution","title":"Super Resolution","text":"<p>Increases the resolution of an image using AI to ensure no bluriness is introduced.</p> <pre><code>POST: http://localhost:32168/v1/image/superresolution\n</code></pre> <p>Platforms</p> <p>All</p> <p>Parameters</p> <ul> <li>image (File): The image to have its resolution increased.</li> </ul> <p>Response</p> JSON<pre><code>{\n  \"success\": (Boolean) // True if successful.\n  \"imageBase64\": (Base64ImageData) // The base64 encoded image that has had its resolution increased.\n  \"inferenceMs\": (Integer) // The time (ms) to perform the AI inference.\n  \"processMs\": (Integer) // The time (ms) to process the image (includes inference and image manipulation operations).\n  \"moduleId\": (String) // The Id of the module that processed this request.\n  \"moduleName\": (String) // The name of the module that processed this request.\n  \"command\": (String) // The command that was sent as part of this request. Can be detect, list, status.\n  \"executionProvider\": (String) // The name of the device or package handling the inference. eg CPU, GPU, TPU, DirectML.\n  \"canUseGPU\": (Boolean) // True if this module can use the current GPU if one is present.\n  \"analysisRoundTripMs\": (Integer) // The time (ms) for the round trip to the analysis module and back.\n}\n</code></pre>","tags":["api"]},{"location":"api/api_reference.html#example_18","title":"Example","text":"JavaScript<pre><code>// Assume we have a HTML INPUT type=file control with ID=fileChooser\nvar formData = new FormData();\nformData.append('image', fileChooser.files[0]);\n\nvar url = 'http://localhost:32168/v1/image/superresolution';\n\nfetch(url, { method: \"POST\", body: formData})\n      .then(response =&gt; {\n           if (response.ok) {\n               response.json().then(data =&gt; {\n                   console.log(\"success: \" + data.success)\n                   // Assume we have an IMG tag named img1\n                   img1.src = \"data:image/png;base64,\" + data.imageBase64;\n                   console.log(\"inferenceMs: \" + data.inferenceMs)\n                   console.log(\"processMs: \" + data.processMs)\n                   console.log(\"moduleId: \" + data.moduleId)\n                   console.log(\"moduleName: \" + data.moduleName)\n                   console.log(\"command: \" + data.command)\n                   console.log(\"executionProvider: \" + data.executionProvider)\n                   console.log(\"canUseGPU: \" + data.canUseGPU)\n                   console.log(\"analysisRoundTripMs: \" + data.analysisRoundTripMs)\n               })\n           }\n       });\n        .catch (error =&gt; {\n            console.log('Unable to complete API call: ' + error);\n       });\n</code></pre>","tags":["api"]},{"location":"api/api_reference.html#natural-language","title":"Natural Language","text":"","tags":["api"]},{"location":"api/api_reference.html#sentiment-analysis","title":"Sentiment Analysis","text":"<p>Determines if the supplied text has a positive or negative sentiment</p> <pre><code>POST: http://localhost:32168/v1/text/sentiment\n</code></pre> <p>Platforms</p> <p>Windows, macOS</p> <p>Parameters</p> <ul> <li>text (Text): The text to be analyzed.</li> </ul> <p>Response</p> JSON<pre><code>{\n  \"success\": (Boolean) // True if successful.\n  \"is_positive\": (Boolean) // Whether the input text had a positive sentiment.\n  \"positive_probability\": (Float) // The probability the input text has a positive sentiment.\n  \"inferenceMs\": (Integer) // The time (ms) to perform the AI inference.\n  \"processMs\": (Integer) // The time (ms) to process the image (includes inference and text manipulation operations).\n  \"moduleId\": (String) // The Id of the module that processed this request.\n  \"moduleName\": (String) // The name of the module that processed this request.\n  \"command\": (String) // The command that was sent as part of this request. Can be detect, list, status.\n  \"executionProvider\": (String) // The name of the device or package handling the inference. eg CPU, GPU, TPU, DirectML.\n  \"canUseGPU\": (Boolean) // True if this module can use the current GPU if one is present.\n  \"analysisRoundTripMs\": (Integer) // The time (ms) for the round trip to the analysis module and back.\n}\n</code></pre>","tags":["api"]},{"location":"api/api_reference.html#example_19","title":"Example","text":"JavaScript<pre><code>var formData = new FormData();\nformData.append(\"text\", '');\n\nvar url = 'http://localhost:32168/v1/text/sentiment';\n\nfetch(url, { method: \"POST\", body: formData})\n      .then(response =&gt; {\n           if (response.ok) {\n               response.json().then(data =&gt; {\n                   console.log(\"success: \" + data.success)\n                   console.log(\"is_positive: \" + data.is_positive)\n                   console.log(\"positive_probability: \" + data.positive_probability.toFixed(2))\n                   console.log(\"inferenceMs: \" + data.inferenceMs)\n                   console.log(\"processMs: \" + data.processMs)\n                   console.log(\"moduleId: \" + data.moduleId)\n                   console.log(\"moduleName: \" + data.moduleName)\n                   console.log(\"command: \" + data.command)\n                   console.log(\"executionProvider: \" + data.executionProvider)\n                   console.log(\"canUseGPU: \" + data.canUseGPU)\n                   console.log(\"analysisRoundTripMs: \" + data.analysisRoundTripMs)\n               })\n           }\n       });\n        .catch (error =&gt; {\n            console.log('Unable to complete API call: ' + error);\n       });\n</code></pre>","tags":["api"]},{"location":"api/api_reference.html#text-summary","title":"Text Summary","text":"<p>Summarizes some content by selecting a number of sentences that are most representative of the content.</p> <pre><code>POST: http://localhost:32168/v1/text/summarize\n</code></pre> <p>Platforms</p> <p>All</p> <p>Parameters</p> <ul> <li> <p>text (Text): The text to be summarized</p> </li> <li> <p>num_sentences (Integer): The number of sentences to produce.</p> </li> </ul> <p>Response</p> JSON<pre><code>{\n  \"success\": (Boolean) // True if successful.\n  \"summary\": (Text) // The summarized text.\n  \"inferenceMs\": (Integer) // The time (ms) to perform the AI inference.\n  \"processMs\": (Integer) // The time (ms) to process the image (includes inference and image manipulation operations).\n  \"moduleId\": (String) // The Id of the module that processed this request.\n  \"moduleName\": (String) // The name of the module that processed this request.\n  \"command\": (String) // The command that was sent as part of this request. Can be detect, list, status.\n  \"executionProvider\": (String) // The name of the device or package handling the inference. eg CPU, GPU, TPU, DirectML.\n  \"canUseGPU\": (Boolean) // True if this module can use the current GPU if one is present.\n  \"analysisRoundTripMs\": (Integer) // The time (ms) for the round trip to the analysis module and back.\n}\n</code></pre>","tags":["api"]},{"location":"api/api_reference.html#example_20","title":"Example","text":"JavaScript<pre><code>var formData = new FormData();\nformData.append(\"text\", '');\nformData.append(\"num_sentences\", 0);\n\nvar url = 'http://localhost:32168/v1/text/summarize';\n\nfetch(url, { method: \"POST\", body: formData})\n      .then(response =&gt; {\n           if (response.ok) {\n               response.json().then(data =&gt; {\n                   console.log(\"success: \" + data.success)\n                   console.log(\"summary: \" + data.summary)\n                   console.log(\"inferenceMs: \" + data.inferenceMs)\n                   console.log(\"processMs: \" + data.processMs)\n                   console.log(\"moduleId: \" + data.moduleId)\n                   console.log(\"moduleName: \" + data.moduleName)\n                   console.log(\"command: \" + data.command)\n                   console.log(\"executionProvider: \" + data.executionProvider)\n                   console.log(\"canUseGPU: \" + data.canUseGPU)\n                   console.log(\"analysisRoundTripMs: \" + data.analysisRoundTripMs)\n               })\n           }\n       });\n        .catch (error =&gt; {\n            console.log('Unable to complete API call: ' + error);\n       });\n</code></pre>","tags":["api"]},{"location":"api/api_reference.html#training","title":"Training","text":"","tags":["api"]},{"location":"api/api_reference.html#create-custom-dataset","title":"Create Custom Dataset","text":"<p>Create a custom dataset from the Open Images repository.</p> <pre><code>POST: http://localhost:32168/v1/train/create_dataset\n</code></pre> <p>Platforms</p> <p>All, !Raspberrypi, !Orangepi, !Jetson</p> <p>Parameters</p> <ul> <li> <p>name (String): The name of the model.</p> </li> <li> <p>classes (String): A comma delimited list of classes to include in the dataset.</p> </li> <li> <p>num_images (Integer): The max number of images to include for each class. Default 100.    Optional. Defaults to 100</p> </li> </ul> <p>Response</p> JSON<pre><code>{\n  \"success\": (Boolean) // True if creating a dataset started.\n  \"moduleId\": (String) // The Id of the module that processed this request.\n  \"moduleName\": (String) // The name of the module that processed this request.\n  \"command\": (String) // The command that was sent as part of this request. Can be detect, list, status.\n  \"executionProvider\": (String) // The name of the device or package handling the inference. eg CPU, GPU, TPU, DirectML.\n  \"canUseGPU\": (Boolean) // True if this module can use the current GPU if one is present.\n  \"analysisRoundTripMs\": (Integer) // The time (ms) for the round trip to the analysis module and back.\n}\n</code></pre>","tags":["api"]},{"location":"api/api_reference.html#example_21","title":"Example","text":"JavaScript<pre><code>var formData = new FormData();\nformData.append(\"name\", null);\nformData.append(\"classes\", null);\nformData.append(\"num_images\", 100);\n\nvar url = 'http://localhost:32168/v1/train/create_dataset';\n\nfetch(url, { method: \"POST\", body: formData})\n      .then(response =&gt; {\n           if (response.ok) {\n               response.json().then(data =&gt; {\n                   console.log(\"success: \" + data.success)\n                   console.log(\"moduleId: \" + data.moduleId)\n                   console.log(\"moduleName: \" + data.moduleName)\n                   console.log(\"command: \" + data.command)\n                   console.log(\"executionProvider: \" + data.executionProvider)\n                   console.log(\"canUseGPU: \" + data.canUseGPU)\n                   console.log(\"analysisRoundTripMs: \" + data.analysisRoundTripMs)\n               })\n           }\n       });\n        .catch (error =&gt; {\n            console.log('Unable to complete API call: ' + error);\n       });\n</code></pre>","tags":["api"]},{"location":"api/api_reference.html#train-custom-model-yolov5-62","title":"Train Custom Model (YOLOv5 6.2)","text":"<p>Create a custom model from a custom dataset.</p> <pre><code>POST: http://localhost:32168/v1/train/train_model\n</code></pre> <p>Platforms</p> <p>All, !Raspberrypi, !Orangepi, !Jetson</p> <p>Parameters</p> <ul> <li> <p>name (String): The name of the model.</p> </li> <li> <p>dataset (String): The name of the dataset.</p> </li> <li> <p>num_epochs (Integer): The epoch to train the model. Default 100.    Optional. Defaults to 100</p> </li> <li> <p>device (String): None or 'cpu' or 0 or '0' or '0,1,2,3'. Default: ''</p> </li> <li> <p>batch (Integer): The batch size. Default: 8    Optional. Defaults to 8</p> </li> <li> <p>freeze (Integer): The layers to freeze, 0-None, 10-Backbone, 24-All    Optional. Defaults to 0</p> </li> <li> <p>hyp (Integer): Hyper-Parameters: 0-finetune (VOC), 1-scratch low, 2-scratch medium, 3-scratch high    Optional. Defaults to 0</p> </li> </ul> <p>Response</p> JSON<pre><code>{\n  \"success\": (Boolean) // True if training started.\n  \"moduleId\": (String) // The Id of the module that processed this request.\n  \"moduleName\": (String) // The name of the module that processed this request.\n  \"command\": (String) // The command that was sent as part of this request. Can be detect, list, status.\n  \"executionProvider\": (String) // The name of the device or package handling the inference. eg CPU, GPU, TPU, DirectML.\n  \"canUseGPU\": (Boolean) // True if this module can use the current GPU if one is present.\n  \"analysisRoundTripMs\": (Integer) // The time (ms) for the round trip to the analysis module and back.\n}\n</code></pre>","tags":["api"]},{"location":"api/api_reference.html#example_22","title":"Example","text":"JavaScript<pre><code>var formData = new FormData();\nformData.append(\"name\", null);\nformData.append(\"dataset\", null);\nformData.append(\"num_epochs\", 100);\nformData.append(\"device\", );\nformData.append(\"batch\", 8);\nformData.append(\"freeze\", 0);\nformData.append(\"hyp\", 0);\n\nvar url = 'http://localhost:32168/v1/train/train_model';\n\nfetch(url, { method: \"POST\", body: formData})\n      .then(response =&gt; {\n           if (response.ok) {\n               response.json().then(data =&gt; {\n                   console.log(\"success: \" + data.success)\n                   console.log(\"moduleId: \" + data.moduleId)\n                   console.log(\"moduleName: \" + data.moduleName)\n                   console.log(\"command: \" + data.command)\n                   console.log(\"executionProvider: \" + data.executionProvider)\n                   console.log(\"canUseGPU: \" + data.canUseGPU)\n                   console.log(\"analysisRoundTripMs: \" + data.analysisRoundTripMs)\n               })\n           }\n       });\n        .catch (error =&gt; {\n            console.log('Unable to complete API call: ' + error);\n       });\n</code></pre>","tags":["api"]},{"location":"api/api_reference.html#training-status-yolov5-62","title":"Training Status (YOLOv5 6.2)","text":"<p>Gets the training status</p> <pre><code>POST: http://localhost:32168/v1/train/status\n</code></pre> <p>Platforms</p> <p>All, !Raspberrypi, !Orangepi, !Jetson</p> <p>Parameters</p> <p>(None)</p> <p>Response</p> JSON<pre><code>{\n  \"success\": (Boolean) // True if successful.\n  \"model_name\": (String) // The name of the last model training or trained.\n  \"dataset_name\": (String) // The name of the dataset used.\n  \"action\": (String) // The current action.\n  \"state\": (String) // The current state in the action processing.\n  \"message\": (String) // Any message, probably error, to display to the user.\n  \"progress\": (float) // The percentage of completion of current state.\n  \"moduleId\": (String) // The Id of the module that processed this request.\n  \"moduleName\": (String) // The name of the module that processed this request.\n  \"command\": (String) // The command that was sent as part of this request. Can be detect, list, status.\n  \"executionProvider\": (String) // The name of the device or package handling the inference. eg CPU, GPU, TPU, DirectML.\n  \"canUseGPU\": (Boolean) // True if this module can use the current GPU if one is present.\n  \"analysisRoundTripMs\": (Integer) // The time (ms) for the round trip to the analysis module and back.\n}\n</code></pre>","tags":["api"]},{"location":"api/api_reference.html#example_23","title":"Example","text":"JavaScript<pre><code>var url = 'http://localhost:32168/v1/train/status';\n\nfetch(url, { method: \"POST\", body: formData})\n      .then(response =&gt; {\n           if (response.ok) {\n               response.json().then(data =&gt; {\n                   console.log(\"success: \" + data.success)\n                   console.log(\"model_name: \" + data.model_name)\n                   console.log(\"dataset_name: \" + data.dataset_name)\n                   console.log(\"action: \" + data.action)\n                   console.log(\"state: \" + data.state)\n                   console.log(\"message: \" + data.message)\n                   console.log(\"progress: \" + data.progress.toFixed(2))\n                   console.log(\"moduleId: \" + data.moduleId)\n                   console.log(\"moduleName: \" + data.moduleName)\n                   console.log(\"command: \" + data.command)\n                   console.log(\"executionProvider: \" + data.executionProvider)\n                   console.log(\"canUseGPU: \" + data.canUseGPU)\n                   console.log(\"analysisRoundTripMs: \" + data.analysisRoundTripMs)\n               })\n           }\n       });\n        .catch (error =&gt; {\n            console.log('Unable to complete API call: ' + error);\n       });\n</code></pre>","tags":["api"]},{"location":"api/api_reference.html#cancel-dataset-or-model-creation","title":"Cancel Dataset or Model creation","text":"<p>Cancel the creation of Model or Dataset.</p> <pre><code>POST: http://localhost:32168/v1/train/cancel\n</code></pre> <p>Platforms</p> <p>All, !Raspberrypi, !Orangepi, !Jetson</p> <p>Parameters</p> <p>(None)</p> <p>Response</p> JSON<pre><code>{\n  \"success\": (Boolean) // True if successful.\n  \"moduleId\": (String) // The Id of the module that processed this request.\n  \"moduleName\": (String) // The name of the module that processed this request.\n  \"command\": (String) // The command that was sent as part of this request. Can be detect, list, status.\n  \"executionProvider\": (String) // The name of the device or package handling the inference. eg CPU, GPU, TPU, DirectML.\n  \"canUseGPU\": (Boolean) // True if this module can use the current GPU if one is present.\n  \"analysisRoundTripMs\": (Integer) // The time (ms) for the round trip to the analysis module and back.\n}\n</code></pre>","tags":["api"]},{"location":"api/api_reference.html#example_24","title":"Example","text":"JavaScript<pre><code>var url = 'http://localhost:32168/v1/train/cancel';\n\nfetch(url, { method: \"POST\", body: formData})\n      .then(response =&gt; {\n           if (response.ok) {\n               response.json().then(data =&gt; {\n                   console.log(\"success: \" + data.success)\n                   console.log(\"moduleId: \" + data.moduleId)\n                   console.log(\"moduleName: \" + data.moduleName)\n                   console.log(\"command: \" + data.command)\n                   console.log(\"executionProvider: \" + data.executionProvider)\n                   console.log(\"canUseGPU: \" + data.canUseGPU)\n                   console.log(\"analysisRoundTripMs: \" + data.analysisRoundTripMs)\n               })\n           }\n       });\n        .catch (error =&gt; {\n            console.log('Unable to complete API call: ' + error);\n       });\n</code></pre>","tags":["api"]},{"location":"api/api_reference.html#resume-training-model","title":"Resume Training Model","text":"<p>Resume training of a model.</p> <pre><code>POST: http://localhost:32168/v1/train/resume_training\n</code></pre> <p>Platforms</p> <p>All, !Raspberrypi, !Orangepi, !Jetson</p> <p>Parameters</p> <ul> <li>model_name (String): The name of the model.</li> </ul> <p>Response</p> JSON<pre><code>{\n  \"success\": (Boolean) // True if successful.\n  \"moduleId\": (String) // The Id of the module that processed this request.\n  \"moduleName\": (String) // The name of the module that processed this request.\n  \"command\": (String) // The command that was sent as part of this request. Can be detect, list, status.\n  \"executionProvider\": (String) // The name of the device or package handling the inference. eg CPU, GPU, TPU, DirectML.\n  \"canUseGPU\": (Boolean) // True if this module can use the current GPU if one is present.\n  \"analysisRoundTripMs\": (Integer) // The time (ms) for the round trip to the analysis module and back.\n}\n</code></pre>","tags":["api"]},{"location":"api/api_reference.html#example_25","title":"Example","text":"JavaScript<pre><code>var formData = new FormData();\nformData.append(\"model_name\", null);\n\nvar url = 'http://localhost:32168/v1/train/resume_training';\n\nfetch(url, { method: \"POST\", body: formData})\n      .then(response =&gt; {\n           if (response.ok) {\n               response.json().then(data =&gt; {\n                   console.log(\"success: \" + data.success)\n                   console.log(\"moduleId: \" + data.moduleId)\n                   console.log(\"moduleName: \" + data.moduleName)\n                   console.log(\"command: \" + data.command)\n                   console.log(\"executionProvider: \" + data.executionProvider)\n                   console.log(\"canUseGPU: \" + data.canUseGPU)\n                   console.log(\"analysisRoundTripMs: \" + data.analysisRoundTripMs)\n               })\n           }\n       });\n        .catch (error =&gt; {\n            console.log('Unable to complete API call: ' + error);\n       });\n</code></pre>","tags":["api"]},{"location":"api/api_reference.html#get-model-information-yolov5-62","title":"Get Model information (YOLOv5 6.2)","text":"<p>Gets info about the model.</p> <pre><code>POST: http://localhost:32168/v1/train/model_info\n</code></pre> <p>Platforms</p> <p>All, !Raspberrypi, !Orangepi, !Jetson</p> <p>Parameters</p> <ul> <li>model_name (String): The name of the model.</li> </ul> <p>Response</p> JSON<pre><code>{\n  \"success\": (Boolean) // True if successful.\n  \"model_name\": (String) // The name of the model.\n  \"complete\": (Boolean) // True if the training was completed, can restart if not.\n  \"training_dir\": (String) // The training directory containing the custom model file and the training results.\n  \"model_path\": (String) // The path to best the custom model file.\n  \"results_graph_path\": (String) // The path the results.png file if it exists.\n  \"results_csv_path\": (String) // The path the results.csv file if it exists.\n  \"pr_curve_path\": (String) // The path PR_curve.png file if it exists.\n  \"results_graph_image\": (Base64ImageData) // The base64 encoded image of the result graphs.\n  \"pr_curve_image\": (Base64ImageData) // The base64 encoded image of the PR Curve graph.\n  \"results_csv_file\": (Base64ImageData) // The base64 encoded data for the results.csv file.\n  \"moduleId\": (String) // The Id of the module that processed this request.\n  \"moduleName\": (String) // The name of the module that processed this request.\n  \"command\": (String) // The command that was sent as part of this request. Can be detect, list, status.\n  \"executionProvider\": (String) // The name of the device or package handling the inference. eg CPU, GPU, TPU, DirectML.\n  \"canUseGPU\": (Boolean) // True if this module can use the current GPU if one is present.\n  \"analysisRoundTripMs\": (Integer) // The time (ms) for the round trip to the analysis module and back.\n}\n</code></pre>","tags":["api"]},{"location":"api/api_reference.html#example_26","title":"Example","text":"JavaScript<pre><code>var formData = new FormData();\nformData.append(\"model_name\", null);\n\nvar url = 'http://localhost:32168/v1/train/model_info';\n\nfetch(url, { method: \"POST\", body: formData})\n      .then(response =&gt; {\n           if (response.ok) {\n               response.json().then(data =&gt; {\n                   console.log(\"success: \" + data.success)\n                   console.log(\"model_name: \" + data.model_name)\n                   console.log(\"complete: \" + data.complete)\n                   console.log(\"training_dir: \" + data.training_dir)\n                   console.log(\"model_path: \" + data.model_path)\n                   console.log(\"results_graph_path: \" + data.results_graph_path)\n                   console.log(\"results_csv_path: \" + data.results_csv_path)\n                   console.log(\"pr_curve_path: \" + data.pr_curve_path)\n                   // Assume we have an IMG tag named img1\n                   img1.src = \"data:image/png;base64,\" + data.results_graph_image;\n                   // Assume we have an IMG tag named img2\n                   img2.src = \"data:image/png;base64,\" + data.pr_curve_image;\n                   // Assume we have an IMG tag named img3\n                   img3.src = \"data:image/png;base64,\" + data.results_csv_file;\n                   console.log(\"moduleId: \" + data.moduleId)\n                   console.log(\"moduleName: \" + data.moduleName)\n                   console.log(\"command: \" + data.command)\n                   console.log(\"executionProvider: \" + data.executionProvider)\n                   console.log(\"canUseGPU: \" + data.canUseGPU)\n                   console.log(\"analysisRoundTripMs: \" + data.analysisRoundTripMs)\n               })\n           }\n       });\n        .catch (error =&gt; {\n            console.log('Unable to complete API call: ' + error);\n       });\n</code></pre>","tags":["api"]},{"location":"api/api_reference.html#get-dataset-information-yolov5-62","title":"Get Dataset information (YOLOv5 6.2)","text":"<p>Gets info about the dataset.</p> <pre><code>POST: http://localhost:32168/v1/train/dataset_info\n</code></pre> <p>Platforms</p> <p>All, !Raspberrypi, !Orangepi, !Jetson</p> <p>Parameters</p> <ul> <li>dataset_name (String): The name of the dataset.</li> </ul> <p>Response</p> JSON<pre><code>{\n  \"success\": (Boolean) // True if successful.\n  \"complete\": (Boolean) // True if the training was completed, can restart if not.\n  \"moduleId\": (String) // The Id of the module that processed this request.\n  \"moduleName\": (String) // The name of the module that processed this request.\n  \"command\": (String) // The command that was sent as part of this request. Can be detect, list, status.\n  \"executionProvider\": (String) // The name of the device or package handling the inference. eg CPU, GPU, TPU, DirectML.\n  \"canUseGPU\": (Boolean) // True if this module can use the current GPU if one is present.\n  \"analysisRoundTripMs\": (Integer) // The time (ms) for the round trip to the analysis module and back.\n}\n</code></pre>","tags":["api"]},{"location":"api/api_reference.html#example_27","title":"Example","text":"JavaScript<pre><code>var formData = new FormData();\nformData.append(\"dataset_name\", null);\n\nvar url = 'http://localhost:32168/v1/train/dataset_info';\n\nfetch(url, { method: \"POST\", body: formData})\n      .then(response =&gt; {\n           if (response.ok) {\n               response.json().then(data =&gt; {\n                   console.log(\"success: \" + data.success)\n                   console.log(\"complete: \" + data.complete)\n                   console.log(\"moduleId: \" + data.moduleId)\n                   console.log(\"moduleName: \" + data.moduleName)\n                   console.log(\"command: \" + data.command)\n                   console.log(\"executionProvider: \" + data.executionProvider)\n                   console.log(\"canUseGPU: \" + data.canUseGPU)\n                   console.log(\"analysisRoundTripMs: \" + data.analysisRoundTripMs)\n               })\n           }\n       });\n        .catch (error =&gt; {\n            console.log('Unable to complete API call: ' + error);\n       });\n</code></pre>","tags":["api"]},{"location":"api/api_reference.html#settings","title":"Settings","text":"","tags":["api"]},{"location":"api/api_reference.html#change-a-setting","title":"Change a setting","text":"<p><pre><code>`POST: localhost:32168/v1/settings/&lt;ModuleId&gt;`\n</code></pre> Sets the value of a setting for the given module.</p> <p>Platforms</p> <p>All</p> <p>Parameters</p> <ul> <li>name - the name of a setting. Please refer to Module Settings for the modules and global settings that can be changed.</li> <li>value - The new value of the setting.</li> </ul> <p>Response</p> JSON<pre><code>{\n  \"success\": (Boolean) // True if successful.\n}\n</code></pre> <p>Note that when this API is called, the server will restart the given module automatically, and the changes will be persisted across server and module restarts.</p>","tags":["api"]},{"location":"api/api_reference.html#list-settings","title":"List settings","text":"<p><pre><code>`GET: localhost:32168/v1/settings/&lt;ModuleId&gt;`\n</code></pre> Gets all the settings for the given module.</p> <p>Platforms</p> <p>All</p> <p>Response</p> <p>The response will be a Json object with the general 'success' property, as well as two collections: <code>environmentVariables</code> are the environment variables passed to the module's code, and <code>settings</code> which define how the module will be started.</p> JSON<pre><code>{\n    \"success\": true,\n\n    \"environmentVariables\": {\n        \"CPAI_APPROOTPATH\": \"C:\\\\Program Files\\\\CodeProject\\\\AI\",\n        \"CPAI_PORT\": \"32168\",\n        \"APPDIR\": \"%CURRENT_MODULE_PATH%\",\n        \"CPAI_HALF_PRECISION\": \"enable\",\n        \"CPAI_MODULE_SUPPORT_GPU\": \"False\",\n        \"CUSTOM_MODELS_DIR\": \"%CURRENT_MODULE_PATH%\\\\custom-models\",\n        \"MODELS_DIR\": \"%CURRENT_MODULE_PATH%\\\\assets\",\n        \"MODEL_SIZE\": \"medium\",\n        \"USE_CUDA\": \"False\",\n        \"YOLOV5_AUTOINSTALL\": \"false\",\n        \"YOLOV5_VERBOSE\": \"false\"\n    },\n\n    \"settings\": {\n        \"autostart\": true,\n        \"supportGPU\": true,\n        \"logVerbosity\": null,\n        \"halfPrecision\": \"enable\",\n        \"parallelism\": 0,\n        \"postStartPauseSecs\": 1\n    }\n}\n</code></pre>","tags":["api"]},{"location":"api/api_reference.html#status","title":"Status","text":"","tags":["api"]},{"location":"api/api_reference.html#server-logs","title":"Server Logs","text":"<pre><code>GET: /v1/log/list?count=&lt;count&gt;&amp;last_id=&lt;lastid&gt;\n</code></pre> <p>Gets up to 20 log entries, starting from id = . The \"\" value can be omitted. What's returned is an array of entries <p>Platforms</p> <p>Windows, Linux, macOS, macOS-Arm, Docker</p> <p>Parameters</p> <ul> <li>lastid - the ID of the last entry that was retrieved, in order to send only new log entries</li> <li>count - The number of entries to return</li> </ul> <p>Response</p> JSON<pre><code>{\n    \"id\": Integer, The id of the log entry\n    \"timestamp\": A datetime value. The timestamp as UTC time of the log entry\n    \"entry\": Text. The text of the entry itself\n}\n</code></pre>","tags":["api"]},{"location":"api/api_reference.html#server-ping","title":"Server Ping","text":"<p>A server ping. Just so you can easily tell if it's alive</p> <p><pre><code>GET: /v1/status/ping\n</code></pre> Platforms</p> <p>Windows, Linux, macOS, macOS-Arm, Docker</p> <p>Response</p> JSON<pre><code>{ \n    \"success\": true \n}\n</code></pre> <p>If all is good.</p>","tags":["api"]},{"location":"api/api_reference.html#server-version","title":"Server Version","text":"<p>Returns the current version of the server</p> <p><pre><code>GET: /v1/status/version\n</code></pre> Platforms</p> <p>Windows, Linux, macOS, macOS-Arm, Docker</p> <p>Response</p> JSON<pre><code>{\n    \"success\": true,\n    \"version\": {\n        \"major\": 2,\n        \"minor\": 2,\n        \"patch\": 4,\n        \"preRelease\": \"Beta\",\n        \"securityUpdate\": false,\n        \"build\": 0,\n        \"file\": \"CodeProject.AI.Server-2.2.4.zip\",\n        \"releaseNotes\": \"Features and improvements\"\n    },\n    \"message\": \"2.2.4-Beta\"\n}\n</code></pre>","tags":["api"]},{"location":"api/api_reference.html#server-update-available","title":"Server Update Available","text":"<p>A note on whether an update is available</p> <p><pre><code>GET: /v1/status/updateavailable\n</code></pre> Platforms</p> <p>Windows, Linux, macOS, macOS-Arm, Docker</p> <p>Response </p> JSON<pre><code>{\n    \"success\"         : true/false,\n    \"message\"         : \"An update to version X  is available\" / \"You have the latest\",\n    \"version\"         : &lt;version object&gt;, // [Deprecated] The latest available version\n    \"current\"         : &lt;version object&gt;, // The current installed version\n    \"latest\"          : &lt;version object&gt;, // The latest available version\n    \"updateAvailable\" : true/false\n};\n</code></pre> <p>Where version object is</p> JSON<pre><code>\"versionInfo\": {\n    \"major\": 2,\n    \"minor\": 2,\n    \"patch\": 4,\n    \"preRelease\": \"Beta\",\n    \"securityUpdate\": false,\n    \"build\": 0,\n    \"file\": \"CodeProject.AI.Server-2.2.4.zip\",\n    \"releaseNotes\": \"Features and improvements.\"\n}\n</code></pre>","tags":["api"]},{"location":"api/how_to.html","title":"How-to guide for common tasks","text":"","tags":["api"]},{"location":"api/how_to.html#vision","title":"Vision","text":"","tags":["api"]},{"location":"api/how_to.html#setup-custom-object-detection","title":"Setup Custom Object Detection","text":"<p>TBD</p>","tags":["api"]},{"location":"api/module_settings.html","title":"Configuring Server and Module Settings","text":"","tags":["settings"]},{"location":"api/module_settings.html#how-are-settings-provided-to-the-server-and-modules","title":"How are settings provided to the Server and Modules?","text":"<p>Each module is passed a collection of settings when it is started. These settings come from multiple sources: the server itself, settings files, environment variables and the command line. The settings in each source will overwrite existing settings, so sources are loaded in order of most general to most specific to allow you to fine tune settings to specific targets</p> <ol> <li> <p>All appsettings.json files under the /server directory. These are loaded in order:</p> <ol> <li>appsettings..json</li> <li>appsettings.mode.json file, where mode = release or development.</li> <li>appsettings.platform.json file, where platform = windows, linux, macOS, or each of these with -arm64 for Arm variants, or docker.</li> <li>appsettings.platform.mode.json</li> </ol> <p>Settings in the appsettings.json files are typically server related.</p> </li> <li> <p>All modulesettings.json files within the module's folder, loaded in the following order:</p> <ol> <li>modulesettings.json</li> <li>modulesettings.mode.json where mode = release or development.</li> <li>modulesettings.os.json where os = windows, linux, macOS</li> <li>modulesettings.os.mode.json </li> <li>modulesettings.os.architecture.json where architecture = x86_64 or arm64</li> <li>modulesettings.os.architecture.mode.json</li> <li>modulesettings.docker.json</li> <li>modulesettings.docker.mode.json</li> <li>modulesettings.device.json where device = raspberrypi, orangepi or jetson</li> <li>modulesettings.device.mode.json</li> </ol> <p>Settings in the modulesettings.json files are typically module related.</p> </li> <li> <p>If you change a module or server setting via the API (or view the dashboard) these settings will be saved in a settings 'override' file in %PROGRAMDATA%\\modulesettings.json and %PROGRAMDATA%\\serversettings.json. The <code>PROGRAMDATA</code> location will be</p> <ul> <li>Windows: C:\\ProgramData\\CodeProject\\AI</li> <li>Linux / Docker: /usr/share/CodePrpoject/AI</li> <li>macOS: /Library/Application Support/CodeProject/AI</li> </ul> </li> <li>Environment variables</li> </ol> <p>See Individual Module settings for information on environment variables for modules.</p> <ol> <li>Command line variables</li> </ol>","tags":["settings"]},{"location":"api/module_settings.html#changing-a-modules-settings","title":"Changing a Module's Settings","text":"<p>There are multiple ways in which a module can be configured</p> <ol> <li>Editing the modulesettings.json file (or files) in the module's directory</li> <li>Setting environment variables on the system</li> <li>Using command line parameters (if running as a Windows application or service)</li> <li>Setting Docker run parameters (if running the Docker image)</li> <li>Via the Settings API</li> </ol> <p>There are also global settings stored in the server's appsettings.json file that provides settings shared by all modules.</p> <p>We will go through the naming convention for settings, the global server  settings, individual module settings and legacy override settings, then finish with a walk through of how to actually change these settings.</p>","tags":["settings"]},{"location":"api/module_settings.html#the-naming-convention","title":"The Naming Convention","text":"<p>To change a setting we need to refer to it by name, and that name depends on the context of where we are making the change. </p> <p>If changing a setting in the modulesettings.json file, then the setting is  referenced within the Json struncture by name</p> <p>For example, the <code>MODEL_SIZE</code> setting for the ObjectDetectionYolo module would be in the <code>modules/ObjectDetectionYolo/modulesettings.json</code> file:</p> JSON<pre><code>{\n  \"Modules\": {\n    \"ObjectDetectionYolo\": {\n      ...\n      \"EnvironmentVariables\": {\n        \"MODEL_SIZE\": \"Medium\"\n        ...\n      }\n    }\n  }\n}\n</code></pre> <p>If setting a value via the command line, as an environment variable, or when launching a Docker container, the setting is accessed via its fully qualified name.</p> <p>For common module settings the form is <code>--Modules:&lt;ModuleId&gt;:Setting-Name=Value</code></p> <p>For module specific settings the form is <code>--Modules:&lt;ModuleId&gt;:EnvironmentVariables:Setting-Name=Value</code></p> <p>For legacy overrides use the format <code>--Setting-Name=Value</code></p> <p>The current pre-installed modules are</p> Module ModuleId Face Processing FaceProcessing Object Detection (.NET, ONNX) ObjectDetectionNet Object Detection (Python, PyTorch) ObjectDetectionYolo <p>and downloadable (installable at runtime) modules are</p> Module ModuleId Background Remover BackgroundRemoval Cartooniser Cartooniser Image Super Resolution SuperResolution Licence Plate reader ALPR Object Detection (Coral) ObjectDetectionCoral Object Detection (Rockchip) ObjectDetectionYoloRKNN Object Detection (YOLOv5 3.1) YOLOv5-3.1 Optical Character Recognition OCR Portrait Filter (background blur) PortraitFilter Scene Classification SceneClassification Text Sentiment Analysis SentimentAnalysis Text Summary TextSummary Training (YOLO) TrainingYolov5","tags":["settings"]},{"location":"api/module_settings.html#macros-for-settings","title":"Macros for settings","text":"<p>There are a number of macros that can be used in string values when providing values. These macros will be replaced by their actual value at runtime.</p> Macro Description %ROOT_PATH% The path to the application's root. For a default Windows install this is <code>C:\\Program Files\\CodeProject\\AI</code>. For Docker this is <code>/app</code> %RUNTIMES_PATH% The path to the installed, shared runtimes. Default is /src/runtimes in development, /runtimes in production (relative to ROOT_PATH) %MODULES_PATH% The path to the Analysis modules. Default is /src/AnalysisLmodulesayer in development, /modules in production (relative to ROOT_PATH) %CURRENT_MODULE_PATH% The path to the current module relative to %ROOT_PATH% %PLATFORM% windows, macos, macos-arm64, linux, linux-arm64 %OS% Windows, macOS or Linux %DATA_DIR% The path to the directory that contains persisted data. This default to<code>C:\\ProgramData\\CodeProject\\AI</code>\u00a0(Windows)<code>/usr/share/CodePrpoject/AI</code>\u00a0(Linux)<code>/Library/Application Support/CodeProject/AI</code>\u00a0(macOS) %PYTHON_RUNTIME% Python37 or Python39, Depending on the Runtime value in modulesettings.json %PYTHON_BASEPATH% The path to the root of the virtual environment for the given version","tags":["settings"]},{"location":"api/module_settings.html#the-settings","title":"The Settings","text":"","tags":["settings"]},{"location":"api/module_settings.html#global-server-settings","title":"Global (server) settings","text":"<p>Use the naming format <code>Modules:&lt;ModuleId&gt;:EnvironmentVariables:Setting-Name=Value</code></p> Parameter Default Description CPAI_PORT 32168 The port that both client apps and back-end modules communicate with the server CPAI_APPROOTPATH %ROOT_PATH% The absolute path to the application root directory <p>eg Command line parameter: <code>--Modules:BackgroundRemoval:EnvironmentVariables:CPAI_PORT=32168</code></p> <p>eg System environment variable   <code>set Modules:BackgroundRemoval:EnvironmentVariables:CPAI_PORT=32168</code></p>","tags":["settings"]},{"location":"api/module_settings.html#common-module-settings","title":"Common Module settings","text":"<p>Use the naming format <code>Modules:&lt;ModuleId&gt;:&lt;Section&gt;:Setting-Name=Value</code>. For example, AutoStart in the LaunchSettings section for module \"TextSummary\" will be specified as <code>Modules:TextSummary:LaunchSettings:AutoStart</code>.</p> <p>Each module will have the following settings</p> Section:Parameter Default Description LaunchSettings:AutoStart varies True or False. Determines whether or not this module is started automatically when the server starts LaunchSettings:Parallelism 0 The number of concurrent execution units (threads, tasks, processes - depends on the module and language) to launch. Default of 0 means \"Number of CPUs - 1\" LaunchSettings:PostStartPauseSecs 0\u00a0(CPU)5\u00a0(GPU) The number of seconds to pause after starting this module. This provides initialisation time GpuOptions:InstallGPU false True or False. Determines whether or not this module is installed with GPU support. If this is False, then EnableGPU will have no effect GpuOptions:EnableGPU varies True or False. Whether or not this module should enable GPU support. Setting to False means GPU support will be disabled. Note that GPU support depends on the module and the platform. A module is not guaranteed to support GPUs, or to support GPUs on all platforms GpuOptions:AcceleratorDeviceName Module dependant, but for modules that use CUDA, for example, this could be \"cuda:0\" or \"cuda:1\" to specify the first or second CUDA enabled GPU GpuOptions:HalfPrecision Module dependant, but for modules that use CUDA this specifies whether half-precision operations should be used. Valid only for cards with CUDA compute capability &gt;= 6.0 <p>eg Command line parameter: <code>--Modules:TextSummary:LaunchSettings:AutoStart=true</code></p> <p>eg System environment variable   <code>set Modules:TextSummary:LaunchSettings:AutoStart=true</code></p>","tags":["settings"]},{"location":"api/module_settings.html#individual-module-settings","title":"Individual Module settings","text":"<p>Many modules have individual settings that are changed via environment variables.  Use the naming format <code>Modules:&lt;ModuleId&gt;:EnvironmentVariables:Setting-Name=Value</code>.</p>","tags":["settings"]},{"location":"api/module_settings.html#object-detector-yolo","title":"Object Detector (Yolo)","text":"Key Default Description MODELS_DIR %MODULES_PATH%\\ObjectDetectionYolo\\assets The path to the folder containing model files CUSTOM_MODELS_DIR %MODULES_PATH%\\ObjectDetectionYolo\\custom-models The path to the folder containing custom model files MODEL_SIZE Medium The detection model size. Tiny, Small, Medium or Large. This setting has no effect for custom models. USE_CUDA True Whether or not to use CUDA if available <p>For example, to set the <code>MODEL_SIZE</code> to Large via the command line use</p> <p><code>--Modules:ObjectDetectionYolo:EnvironmentVariables:MODEL_SIZE=Large</code></p> <p>To set the model directory to point to a different set of custom models, use</p> <p><code>--Modules:ObjectDetectionYolo:EnvironmentVariables:CUSTOM_MODELS_DIR=\"C:\\My Custom Models\"</code></p>","tags":["settings"]},{"location":"api/module_settings.html#object-detector-net","title":"Object Detector (.NET)","text":"Key Default Description MODEL_SIZE Medium The detection model size. Small, Medium or Large. <p>For example, to set the <code>MODEL_SIZE</code> to Large via the command line use</p> <p><code>--Modules:ObjectDetectionNet:EnvironmentVariables:MODEL_SIZE=Large</code></p>","tags":["settings"]},{"location":"api/module_settings.html#scene-classifier","title":"Scene Classifier","text":"Key Default Description MODELS_DIR %MODULES_PATH%\\Vision\\assets The path to the folder containing model files MODE MEDIUM The detection resolution. Low, Medium or High <p>For example, to set the <code>MODE</code> to High use</p> <p><code>--Modules:SceneClassification:EnvironmentVariables:MODE=High</code></p>","tags":["settings"]},{"location":"api/module_settings.html#face-detection","title":"Face Detection","text":"Key Default Description MODELS_DIR %MODULES_PATH%\\Vision\\assets The path to the folder containing model files MODE MEDIUM The detection resolution. Low, Medium or High DATA_DIR %DATA_DIR% The path to the folder containing persisted face data USE_CUDA True Whether or not to use CUDA if available <p>For example, to set the MODE to High use</p> <p><code>--Modules:FaceProcessing:EnvironmentVariables:MODE=High</code></p> <p>These environment variables are shared by all Face processing modules </p>","tags":["settings"]},{"location":"api/module_settings.html#text-summary","title":"Text Summary","text":"<p>For environment variables, the parameter prefix is:  <code>--Modules:TextSummary:EnvironmentVariables:</code></p> Key Default Description NLTK_DATA %MODULES_PATH%\\TextSummary\\nltk_data The path to the folder containing the model file <p>For example, to set the NLTK_DATA to 'my-data' use</p> <p><code>--Modules:TextSummary:EnvironmentVariables:NLTK_DATA=my-data</code></p>","tags":["settings"]},{"location":"api/module_settings.html#global-legacy-setting-overrides","title":"Global legacy setting overrides","text":"<p>In order to provide legacy support for some modules a number of command line parameters are supported that provide overrides for multiple modules. </p> <p>These settings apply to Face Processing, Scene Classification, and  Object Detection modules only.</p> <p>Use the format <code>--Setting-Name=Value</code> on the command line, or  <code>Setting-name=Value</code> as a system environment variable.</p> Parameter Default Description PORT 32168 (standard)5000 (alt for Windows, Linux)5500 (alt for macOS) The port on which the server listens for requests MODE Medium The detection mode for vision operations. High, medium or low resolution inference DATA_DIR <code>C:\\ProgramData\\CodeProject\\AI</code>\u00a0(Windows)<code>/usr/share/CodePrpoject/AI</code>\u00a0(Linux)<code>/Library/Application Support/CodeProject/AI</code>\u00a0(macOS) The directory containing persisted data for face recognition MODELSTORE-DETECTIONMODELSTORE_DETECTION <code>%MODULES_PATH%\\\\ObjectDetectionYolo\\\\custom</code> The directory containing the custom AI models VISION-FACEVISION_FACE True Whether face detection operations are enabled VISION-SCENEVISION_SCENE True Whether scene classification is enabled VISION-DETECTIONVISION_DETECTION True Whether object detection operations are enabled CUDA_MODE True Whether CUDA support is to be enabled <p>Where <code>%MODULES_PATH%</code> is a macro that expands to be the absolute path to the analysis modules directory. By default this is <code>C:\\Program Files\\CodeProject\\AI\\modules</code> for a Windows install, and <code>/app/modules</code> for Docker.</p> <p>eg <code>--MODE=Medium</code></p>","tags":["settings"]},{"location":"api/module_settings.html#making-changes-to-settings","title":"Making changes to settings","text":"","tags":["settings"]},{"location":"api/module_settings.html#option-1-editing-the-modulesettingsjson-files","title":"Option 1: Editing the modulesettings.json files","text":"<p>Proceed with caution</p> <p>While configuring modules is very straightforward, if you make a mistake you may cause a module to no longer function correctly. Always make a backup of any files you modify just in case you need to undo your changes.</p> <p>The modules included with CodeProject.AI are configued via the <code>modulesettings.json</code> files in the module's directory, typically located at <code>C:\\Program Files\\CodeProject\\AI\\modules\\&lt;ModuleName&gt;\\modulesettings.json</code>, where <code>ModuleName</code> is the name of the module.</p> <p>There are variants of this file for each supported operating system, in the form <code>modulesettings.&lt;platform&gt;.json</code>, where platform will be windows, macos, macos-arm, linux or docker.</p> <p>To read more about the modulesettings file, please refer to the  developer's guide</p> <p>To edit values simply open this file in a text editor such as Notepad or Notepad++, make your  changes and then save the file. Please, please, please make a backup first.</p>","tags":["settings"]},{"location":"api/module_settings.html#option-2-setting-environment-variables-on-the-system","title":"Option 2: Setting environment variables on the system","text":"<p>For Windows:</p> <ul> <li>Click the Start menu and type 'Environment'. Select 'Edit the system Environment variables'.</li> <li>Click 'Environment variables'</li> <li>Click either User or System variables. User variables will be in effect only when you are logged in. System variables are in effect for everyone.</li> <li>Click New to add a new variable name and value, or click an existing variable, and then click Edit to change its name or value.</li> </ul> <p>For macOS:</p> <ul> <li>Open a terminal window and type <code>touch .zprofile</code> to ensure a .zprofile file exists (This assumes you are using zsh in macOS. For bash use .bash_profile)</li> <li>Run <code>nano .profile</code> to edit the file</li> <li>Add or edit variables in the form <code>export MY_VARIABLE='my value'</code> </li> <li>Save and close the file</li> <li>Run <code>source .zprofile</code> to force the changes to be loaded</li> </ul>","tags":["settings"]},{"location":"api/module_settings.html#option-3-restarting-the-codeprojectai-service-and-supplying-parameters","title":"Option 3: Restarting the CodeProject.AI service and supplying parameters","text":"<p>By default the Windows installer will install CodeProject.AI as a Windows Service that will start when Windows starts, and restart automatically on failure. If you wish to customise settings then you can opt to start the CodeProject.AI Server's service manually and pass command line parameters to override settings</p> <p>First, stop the service from a terminal:</p> Command line<pre><code>sc stop \"CodeProject.AI Server\"\n</code></pre> <p>Then restart the service, passing in the parameters you wish:</p> Command line<pre><code>sc start \"CodeProject.AI Server\" --MODE=Low\n</code></pre> <p>This will restart the service and set the AI mode for Vision inference to \"low\".</p>","tags":["settings"]},{"location":"api/module_settings.html#option-4-docker","title":"Option 4. Docker","text":"<p>Refer to the instructions for launching a Docker image.</p> <p>The command to run the Docker image is Command line<pre><code>docker run --name CodeProject.AI-Server -d -p 32168:32168 ^\n --mount type=bind,source=C:\\ProgramData\\CodeProject\\AI\\docker\\data,target=/etc/codeproject/ai ^\n --mount type=bind,source=C:\\ProgramData\\CodeProject\\AI\\docker\\modules,target=/app/modules ^\n   codeproject/ai-server\n</code></pre></p> <p>To have the Docker manager pass settings to CodeProject.AI, use the <code>-e setting=value</code> option. For instance, to pass in MODE=Low you would use:</p> <p>Command line<pre><code>docker run -e MODE=Low --name CodeProject.AI-Server -d -p 32168:32168 ...\n</code></pre> (or use port 5000 for legacy users)</p> <p>The settings that can be changed in Docker are the same that are avaiable in a native Windows installation. See the  API documentation for module specific settings which are set using the form <code>Modules:ModuleName:Setting=value</code> like</p> <p>Command line<pre><code>docker run -e Modules:TextSummary:AutoStart=False --name CodeProject.AI-Server ...\n</code></pre> Also see the section Global Command Line Parameters for a list of global parameters that can be modified using the form <code>--setting=value</code></p>","tags":["settings"]},{"location":"api/module_settings.html#option-5-via-the-settings-api","title":"Option 5. Via the Settings API","text":"<p>As of CodeProject.AI 1.5.7.3 there is an API that allows you to modify module settings on the fly.</p> <p>The endpoint is:</p> <p><code>POST: localhost:32168/v1/settings/&lt;ModuleId&gt;</code></p> <p>where moduleID is given by</p> <p>The current pre-installed modules are</p> Module ModuleId Face Processing FaceProcessing Object Detection (.Net, ONNX) ObjectDetectionNet Object Detection (Python, PyTorch) ObjectDetectionYolo Background Remover BackgroundRemoval Cartooniser Cartooniser Image Super Resolution SuperResolution Licence Plate reader ALPR Object Detection (Coral) ObjectDetectionCoral Object Detection (Rockchip) ObjectDetectionYoloRKNN Object Detection (YOLOv5 3.1) YOLOv5-3.1 Optical Character Recognition OCR Portrait Filter (background blur) PortraitFilter Scene Classification SceneClassification Text Sentiment Analysis SentimentAnalysis Text Summary TextSummary Training (YOLO) TrainingYolov5 <p>To set the value of a setting for a module, you specify the module via the route (the <code>&lt;ModuleId&gt;</code> bit), set the name as the undecorated setting name (eg  <code>MODEL_SIZE</code>, or <code>CPAI_PORT</code>), and the value.</p> <p>So to set the model size to Small (for instance) for the YOLO Object Detection module, you would do </p> <p><code>POST: localhost:32168/v1/settings/ObjectDetectionYolo</code></p> <p>and pass name = 'MODEL_SIZE', value = 'Small':</p> JavaScript<pre><code>var myHeaders = new Headers();\nmyHeaders.append(\"Content-Type\", \"application/x-www-form-urlencoded\");\n\nvar urlencoded = new URLSearchParams();\nurlencoded.append(\"name\", \"MODEL_SIZE\");\nurlencoded.append(\"value\", \"Small\");\n\nvar requestOptions = {\n  method: 'POST',\n  headers: myHeaders,\n  body: urlencoded\n};\n\nfetch(\"localhost:32168/v1/settings/objectdetectionyolo\", requestOptions)\n  .then(response =&gt; response.text())\n  .then(result =&gt; console.log(result))\n  .catch(error =&gt; console.log('error', error));\n</code></pre> <p>The settings that can changed are listed in the Common Module settings and the Individual Module settings sections at the top of this page.</p>","tags":["settings"]},{"location":"devguide/install_dev.html","title":"Installing CodeProject.AI Development Environment","text":"<p> Windows \u00a0  macOS \u00a0  Ubuntu \u00a0  Debian \u00a0  Visual Studio \u00a0  VS Code</p>","tags":["setup","macOS","Windows","Ubuntu","Docker","Visual-Studio","VS-Code"]},{"location":"devguide/install_dev.html#setting-up-the-development-environment","title":"Setting up the Development Environment","text":"<p>A quick guide to setting you up for Debugging, testing and adding your own additions to CodeProject.AI.</p> <p>This current release works in Visual Studio Code on Windows 10+. Ubuntu and macOS (both Intel and  Apple Silicon), and even directly on a Raspberry Pi or Jetson. Visual Studio 2019+ on Windows 10+ is also supported.</p> <p> <p>Download the code Fork on GitHub</p> <p></p>","tags":["setup","macOS","Windows","Ubuntu","Docker","Visual-Studio","VS-Code"]},{"location":"devguide/install_dev.html#download-and-setup-the-code","title":"Download and Setup the code:","text":"<ol> <li> <p>Download or fork the code.</p> </li> <li> <p>In the Solution folder go to <code>/src</code> folder and run the installation scripts:</p> <ol> <li>For Windows: <code>setup.bat</code> </li> <li>For Ubuntu and macOS: <code>bash setup.sh</code></li> </ol> <p>This will download required assets and setup the runtime environments.</p> </li> </ol>","tags":["setup","macOS","Windows","Ubuntu","Docker","Visual-Studio","VS-Code"]},{"location":"devguide/install_dev.html#development-using-visual-studio-code","title":"Development using Visual Studio Code","text":"<p>You'll need the following extensions</p> <ol> <li> <p>Python extension for Visual Studio Code</p> </li> <li> <p>C# extension for Visual Studio Code.</p> </li> </ol>","tags":["setup","macOS","Windows","Ubuntu","Docker","Visual-Studio","VS-Code"]},{"location":"devguide/install_dev.html#to-build-and-debug","title":"To Build and Debug","text":"<ol> <li> <p>Open the main application folder in VS Code</p> </li> <li> <p>Click the \"Run and Debug\" button on the left hand tab bar (the arrow)</p> </li> <li> <p>From the dropdown at the top of the window, select Build All &amp; Launch Server. Select x64 for x86_64 chips (eg Intel, AMD), or Arm64 for Apple Silicon or Raspberry Pi.</p> <p></p> </li> <li> <p>Click the green arrow next to the dropdown</p> </li> </ol> <p>The dashboard webpage should launch after the code has built and the server has started.</p>","tags":["setup","macOS","Windows","Ubuntu","Docker","Visual-Studio","VS-Code"]},{"location":"devguide/install_dev.html#using-the-windows-subsystem-for-linux","title":"Using the Windows Subsystem for Linux?","text":"<p>The Windows Subsystem for Linux (WSL) may first need to be installed on your Windows machine.  Installing is as simple as opening a Powershell terminal and entering</p> PowerShell<pre><code>wsl --install\n</code></pre> <p>More information can be found in Microsoft's guide. </p> <p>You will also need to install Visual Studio Code for Ubuntu. The downloads for each platform are at Visual Studio Code Download page. For WSL select Ubuntu, 64 bit  (under \"Debian, Ubuntu\" select '64 bit')</p> <p>Once you have WSL and VS Code installed, you will also need to install the following VS code extensions:</p> <ol> <li> <p>The Remote WSL Extension.</p> </li> <li> <p>The C# Extension     for WSL, even if you've already installed it for Windows.</p> <p>Open an Ubuntu Terminal (we strongly recommend Windows Terminal for this) and type <code>code .</code>. Then head to the C# Extension page,  choose install, and the extension should be installed. If you get a warning that there is an  instance of VS Code already running, close VS code and hit the install button again.</p> </li> <li> <p>You probably want to also install a browser. For Google Chrome use</p> </li> </ol> Bash<pre><code>wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\nsudo apt -y install ./google-chrome-stable_current_amd64.deb\n</code></pre> <p>Once you have VSCode installed and have cloned the project, setup the development environment as if you were in a native Linux environment.</p> <p>A word of warning In WSL you can have multiple Linux instances. Be sure that if you open a terminal outside of VSCode that it's running on the same instance as that VSCode is using for its integrated terminal.</p> <p>You're now ready to edit and Debug CodeProject.AI inside WSL using VS Code. </p> <p>For this demo we will use the same solution files we use in Windows. Editing  and debugging the same files in Windows and in Linux makes life easy. Just be  careful of those CRLF vs LF line endings.</p> <ol> <li> <p>Navigate to your repo and launch VS Code. If your solution is in    <code>C:\\Dev\\CodeProject\\CodeProject.AI</code> then you would use</p> Bash<pre><code>cd /mnt/c/Dev/CodeProject/CodeProject.AI\ncode .\n</code></pre> </li> <li> <p>Re-open in WSL by hitting Ctrl+Shift P for the command pallete, select \"Remote-WSL: Reopen Folder in WSL\" and hit enter.</p> </li> </ol> <p>You are now coding against the existing Windows repository, but using a remote connection to the WSL system from within VS Code. From within this current environment it's all Linux. Run the setup scripts (the Linux version) and you're on your way. Easy, right?</p>","tags":["setup","macOS","Windows","Ubuntu","Docker","Visual-Studio","VS-Code"]},{"location":"devguide/install_dev.html#development-using-visual-studio-2019-or-later","title":"Development using Visual Studio 2019 or later","text":"<ol> <li>Open the solution in Visual Studio and build the entire solution</li> <li>Start a new instance of the <code>/src/server</code> project</li> </ol> <p>The dashboard should appear and you're on your way.</p> <p>You may wish to have the Python workflow enabled in Visual Studio. While not critical, it does  help with debugging.</p>","tags":["setup","macOS","Windows","Ubuntu","Docker","Visual-Studio","VS-Code"]},{"location":"devguide/install_dev.html#to-run-and-debug-the-server-and-the-demo-net-application","title":"To run and debug the Server and the demo .NET application","text":"<p>There are two ways you can do this:</p> <ol> <li> <p>Separately start both the projects in debug mode: </p> <ol> <li>In Solution Explorer, open demos / clients / .NET and right-click on <code>CodeProject.AI.Explorer</code> and choose Debug -&gt; Start new instance.</li> <li>In Solution Explorer, open src / server and right-click on <code>Server</code> and choose Debug -&gt; Start new instance. </li> </ol> </li> <li> <p>Configure Visual Studio to start multiple projects:</p> <ol> <li> <p>In Solution Explorer, right-click on the solution and select Set Startup Projects....     The Solution property page will appear</p> <p></p> </li> <li> <p>Check \"Multiple Startup Projects\" and select the <code>Server</code> and     <code>CodeProject.AI.Explorer</code> projects.</p> </li> </ol> <p>Now when you start with or without debugging, both the Server and demo projects with start.</p> </li> </ol>","tags":["setup","macOS","Windows","Ubuntu","Docker","Visual-Studio","VS-Code"]},{"location":"devguide/install_scripts.html","title":"Writing install scripts for CodeProject.AI Server","text":"","tags":["server","install"]},{"location":"devguide/install_scripts.html#installation-scripts-notes","title":"Installation Scripts notes","text":"","tags":["server","install"]},{"location":"devguide/install_scripts.html#general","title":"General","text":"<p>The install scripts for each module will be contained in <code>install.sh</code> for Linux/macOS, and <code>install.bat</code> for windows. These two variations mirror each other in form and variable naming.</p> <p>The install scripts are call via the main <code>setup.sh</code>/<code>setup.bat</code> script. </p> <p>If <code>setup</code> is called from the <code>/src</code> folder then it runs in 'Development Environment setup' mode,  meaning it sets up the entire environment in preparation for coding and debugging. .NET is installed, folders are created, and every module in the /src/modules folder is setup by calling each module's  <code>install</code> script in turn. The demo modules in the /demos will then be setup.</p> <p>If the <code>setup</code> script is called from within a module's home directory (via <code>bash ../../setup.sh</code> or <code>..\\..\\setup.bat</code>) then the setup script operates in 'Install module' mode, meaning it will only run the necessary commands to setup the current module.</p>","tags":["server","install"]},{"location":"devguide/install_scripts.html#module-setup-lifecycle","title":"Module Setup Lifecycle","text":"<p>Regardless of whether the setup script is running in dev or install mode, it will carry out the same  steps when installing a single module or all modules.</p> <ol> <li> <p>The setup life cycle starts be detecting the current environment. The Operating system, whether it's Intel x64 or arm64 architecture, the presence of known GPUs, the type of system (eg Jetson or  Raspberry Pi, or just plain Windows), and the environment (eg Docker, WSL, or native).</p> </li> <li> <p>The module's <code>modulesettings.json</code> file is then read to determine the module's runtime. If the module uses Python then the version of python given by the <code>runtime</code> setting will be installed and a virtual environment setup.</p> </li> <li> <p>If no errors ocurred, the module's <code>install</code> script will be run</p> </li> <li> <p>If no errors ocurred, and if the module uses Python and has an appropriate requirements.txt file, the packages in the requirements.txt file will be installed.</p> </li> <li> <p>If no errors ocurred, and if the module has a <code>post_install.sh</code> / <code>postinstall.bat</code> file then this script will be run</p> </li> </ol>","tags":["server","install"]},{"location":"devguide/install_scripts.html#variables-available","title":"Variables available","text":"Variable Description <code>os</code> \"linux\", \"macos\" or \"windows\" <code>architecture</code> \"x86_64\" or \"arm64\" <code>platform</code> \"linux\", \"linux-arm64\", \"macos\" or \"macos-arm64\", \"windows\" or \"windows-arm64\" <code>systemName</code> General name for the system. Value can be one of: Windows, Linux, macOS, WSL, Raspberry Pi, Orange Pi, Jetson, or Docker <code>rootDirPath</code> the root path of the installation (eg: ~/CodeProject/AI) <code>sdkScriptsDirPath</code> the path to the installation utility scripts ($rootDirPath/SDK/Scripts) <code>downloadDirPath</code> the path to where downloads will be stored ($sdkScriptsDirPath/downloads) <code>runtimesDirPath</code> the path to the installed runtimes ($rootDirPath/src/runtimes) <code>modulesDirPath</code> the path to all the AI modules ($rootDirPath/src/modules) <code>moduleName</code> the name of the current module <code>moduleVersion</code> the version of the current module <code>moduleDirName</code> the name of the directory containing this module <code>moduleDirPath</code> the path to this module ($modulesDirPath/$moduleDirName) <code>runtime</code> The runtime this module uses. Either dotnet, pythonX.Y <code>runtimeLocation</code> The location of the virtual environment for this module. It can either be 'Local' meaning it's sandboxed within the module itself, or 'Shared' meaning the venv in use will be used by other modules <code>pythonVersion</code> The version of python used for this module on the current system <code>virtualEnvDirPath</code> The path to the virtual environment for this module <code>venvPythonCmdPath</code> The path to the python executable for the venv for this module <code>packagesDirPath</code> The path to the python packages installed for this module <code>verbosity</code> quiet, info or loud. Use this to determines the noise level of output. <code>forceOverwrite</code> if true then ensure you force a re-download and re-copy of downloads. <code>getFromServer</code> will honour this value. Make sure to honour this value if you are calling <code>downloadAndExtract</code> directly","tags":["server","install"]},{"location":"devguide/install_scripts.html#methods-available","title":"Methods available","text":"Method Params <code>write</code> text [foreground [background]] (eg write \"Hi\" \"green\") <code>writeLine</code> text [foreground [background]] <code>installAptPackages</code> \"list of packages to install\"Installs a list of apt packages. eg <code>installAptPackages \"libjpeg-dev zlib1g-dev libpython3-dev\"</code> <code>installPythonPackagesByName</code> <code>PIP module names, separated by space</code> or <code>Wheel file path</code>, <code>Module name</code>- <code>PIP module name</code> - the name of the Python module to install, eg 'torch' - <code>Wheel file path</code> - instead of a module name you can supply a wheel file path for installation - <code>Module description</code> - (optional) the name or description of the module being installedeg <code>installPythonPackagesByName \"torch\"</code><code>installPythonPackagesByName \"my-torch-wheel.whl\" \"torch\"</code> - <code>PIP options</code> - (optional) Any otions you wish to pass to PIP (eg --index-url) <code>downloadAndExtract</code> storageUrl filename downloadDirPath dirNameToSave message<code>storageUrl</code> - Url that holds the compressed archive to Download<code>filename</code> - Name of the compressed archive to be downloaded <code>downloadDirPath</code> - path to where the downloaded compressed archive should be downloaded<code>dirNameToSave</code> - name of directory, relative to downloadDirPath, where contents of archive will be extracted and saved<code>message</code> - Message to display during download <code>Download</code> storageUrl downloadDirPath filename moduleDirName message - <code>storageUrl</code> Url that holds the compressed archive to Download&lt; - <code>downloadDirPath</code> Path to where the downloaded compressed archive should be downloade - <code>filename</code>      name of the compressed archive to be downloaded - <code>dirNameToSave</code> name of directory, relative to downloadDirPath, where contents of archive will be extracted and saved - <code>message</code> Message to display during download <code>getFromServer</code> folder filename moduleAssetDir message<code>folder</code> - name of the folder in the S3 bucket where the file lives<code>filename</code> - Name of the compressed archive to be downloaded<code>moduleAssetDir</code>- Name of folder in module's directory where archive will be extracted<code>message</code> - Message to display during download Tnis method will pull an archive from the current CodeProject cloud storage. This storage is read-only and subject to change. <code>setupPython</code> (Never called directly) Installs the version of python given in <code>pythonVersion</code> and sets up a virtual environment in the location set by <code>runtimeLocation</code> <code>installRequiredPythonPackages</code> (Never called directly) Installs the python packages included in the appropriate requirements.txt file in the current module into the current virtual environment for this module<code>requirements-file-directory</code> - an optional parameter specifying the directory containing the requirements.txt file <p>To call a method in a Linux/macOS install script, use <code>method parameters</code> (eg writeLine \"Hello, World!\"). To call a method in a Windows install script, use call <code>\"%sdkScriptsDirPath%\\utils.bat\" method params</code> (eg %sdkScriptsDirPath%\\utils.bat WriteLine \"Hello, World!\").</p>","tags":["server","install"]},{"location":"devguide/integration.html","title":"Integrating CodeProject.AI with your Application","text":"<p>TBD</p>","tags":["api","integration"]},{"location":"devguide/modulesettings_json.html","title":"The Module Settings files","text":"<p>The way a module is installed, launched, and managed is defined in the modulesettings.json file or files for that module. You can create a separate modulesettings file for each device, operating system, architecture or GPU combo you need to support. See Module Settings for more information.</p> <p>Assuming we're labelling our module as DemoModulePython, the basic format of a settings file will be:</p> JSON<pre><code>{\n  \"Modules\": {\n\n    \"DemoModulePython\": {\n      \"Name\": \"My module name\",\n      \"Version\": \"2.0.0\",\n\n      \"PublishingInfo\" : {\n         ...\n      },\n\n      \"LaunchSettings\": {\n         ...\n      },\n\n      \"EnvironmentVariables\": {\n         ...\n      },\n\n      \"GpuOptions\" : {\n         ...\n      },\n\n      \"InstallOptions\" : {\n         ...\n      },\n\n      \"UIElements\" : {\n         ...\n      },\n\n      \"RouteMaps\": [\n      {\n         ...\n      }]\n    }\n  }\n}\n</code></pre> <p>Let's walk through this and fill in the values.</p> <p>Name and version should be self explanatory, and the module ID is any ID you wish to provide that doesn't conflict with another module. We're labelling our module as\u00a0DemoModulePython.</p> <p>Our PublishingInfo\u00a0section will contain: JSON<pre><code>\"PublishingInfo\"\u00a0: {\n  \"Description\" : \"Provides Object Detection YOLOv8\",\n  \"Category\"    : \"Computer Vision\",\n  \"Stack\"       : \"Python, PyTorch, YOLO\",\n  \"License\"     : \"AGPL-3.0\",\n  \"LicenseUrl\"  : \"https://www.gnu.org/licenses/agpl-3.0.en.html\",\n  \"Author\"      : \"Chris Maunder\",\n  \"Homepage\"    : \"https://codeproject.ai\",\n  \"BasedOn\"     : \"ultralytics\",                         // What project is this based on?\n  \"BasedOnUrl\"  : \"https://github.com/ultralytics/ultralytics\" // The URL for that project\n}\n</code></pre></p> <p>The\u00a0LaunchSettings will contain</p> JSON<pre><code>\"LaunchSettings\": {\n\u00a0 \"AutoStart\"          : true,                // Start this when the server starts\n  \"FilePath\"           : \"detect_adapter.py\", // Name of file that gets launched  \n  \"Runtime\"            : \"python3.8\",         // file is launched by Python\n  \"RuntimeLocation\"    : \"Local\",             // We want our own virt. env., not a shared one\n  \"PostStartPauseSecs\" : 1,                   // Generally 1 if using GPU, 0 for CPU\n  \"Queue\"              : \"objdetect_queue\",   // Can be left as null if you wish\n  \"Parallelism\"        : 0                    // 0 = Default (number of CPUs - 1)\n}\n</code></pre> <ul> <li>FilePath and Runtime are the most important fields here. Everything else can be omitted if you wish.</li> <li>Queue specifies where the server will place requests from clients, and the name of the queue that the module will be looking in for requests to process. You can leave this blank, or you can provide a name in case you wish to have multiple modules service the same queue.</li> <li>Parallelism denotes the number of parallel tasks that will be running to service this queue.</li> <li>RuntimeLocation is currently only used for Python, and can be \"shared\", meaning this module will use a shared virtual environment to save space, or \"local\" meaning the venv will be local to this module (to avoid package conflicts)</li> </ul> <p>The EnvironmentVariables section contains any environment variables we wish to have set before launch</p> JSON<pre><code>\"EnvironmentVariables\": {\n\u00a0 \"MODEL_SIZE\" : \"Medium\",   // tiny, small, medium, large\n  \"MODELS_DIR\" : \"%CURRENT_MODULE_PATH%/assets\"\n}\n</code></pre> <p>Note the use of the\u00a0<code>CURRENT_MODULE_PATH</code> macro. See Macros for more information on this.</p> <p>GpuOptions\u00a0is solely for GPU setup</p> JSON<pre><code>\"GpuOptions\": {\n\u00a0 \"InstallGPU\"            : true,    // Should we install GPU enabled code?\n  \"EnableGPU\"             : true,    // Should we enable GPU support (if installed)\n  \"AcceleratorDeviceName\" : null,    // eg \"cuda:0\" or just leave blank for default\n  \"HalfPrecision\"         : \"enable\" // For code that supports half-precision. Use 'Force', 'Enable', 'Disable'\n}\n</code></pre> <ul> <li>HalfPrecision allows you to disable half precision operations on older CUDA cards that don't support it. It's generally safe to omit this because CodeProject.AI server can make some educated guesses on your behalf</li> </ul> <p>InstallOptions describe how and where and if the module should or could be installed</p> JSON<pre><code>\"InstallOptions\": {\n  \"Platforms\":      [ \"windows\", \"linux\" ], \n  \"ModuleReleases\": [ \n      { \"ModuleVersion\": \"1.0.0\", \"ServerVersionRange\": [ \"1.0.0\", \"2.4.9\" ],\n\u00a0       \"ReleaseDate\": \"2023-01-01\", \"ReleaseNotes\": \"Initial Release\", \n\u00a0       \"Importance\": \"Major\" },\n     { \"ModuleVersion\": \"2.0.0\", \"ServerVersionRange\": [ \"2.5.0\", \"\" ],\n\u00a0      \"ReleaseDate\": \"2023-02-14\", \"ReleaseNotes\": \"Updated for Server 2.5.0\",\n\u00a0      \"Importance\": \"Major\" } \n  ]\n}\n</code></pre> <ul> <li>Platforms\u00a0is a list of platforms. Supported options are: windows, macos, linux, raspberrypi, orangepi, jetson, plus the arm64 variants:\u00a0windows-arm64, macos-arm64, linux-arm64. Or, use \"all\" to signify it can run anywhere.</li> <li>ModuleReleases is an array of versions and the server versions it's compatible with. In this case version 1 was compatible with CodeProject.AI Server version 1 to 2.4.9, and version 2 of the module is compatible with CodeProject.AI version 2.5 and above</li> </ul> <p>UIElements provides the server with information on UI options. We'll only discuss the context menu that appears on a module's row in the dashboard.\u00a0 We'll provide a single menu that offers model size.</p> <p>JSON<pre><code>\"UIElements\" : {\n  \"Menus\": [{\n     \"Label\": \"Half Precision\",\n     \"Options\": [\n       { \"Label\": \"Force on\",    \"Setting\": \"CPAI_HALF_PRECISION\", \"Value\": \"force\"   },\n       { \"Label\": \"Use Default\", \"Setting\": \"CPAI_HALF_PRECISION\", \"Value\": \"enable\"  },\n       { \"Label\": \"Disable\",     \"Setting\": \"CPAI_HALF_PRECISION\", \"Value\": \"disable\" }\n     ]\n  }]\n},\n</code></pre> Adding a second menu option is as is simple as adding another menu object (<code>{ \"Label\": .... \"Options\": ...}</code>)</p> <p>RouteMaps are what define the API route for your module.</p> JSON<pre><code>\"RouteMaps\": [{\n  \"Name\": \"Python YOLOv8 Object Detector\",\n  \"Route\": \"vision/detection\",\n  \"Method\": \"POST\",\n  \"Command\": \"detect\",\n  \"Description\": \"Detects objects\",\n\n  \"Inputs\": [\n    {\n      \"Name\": \"image\",\n      \"Type\": \"File\",\n      \"Description\": \"The HTTP file object (image) to be analyzed.\"\n    },\n    {\n      \"Name\": \"min_confidence\",\n      \"Type\": \"Float\",\n      \"Description\": \"The minimum confidence level (0.0 - 1.0) for an object will be detected.\",\n      \"DefaultValue\": 0.4,\n      \"MinValue\": 0.0,\n      \"MaxValue\": 1.0\n    }],\n\n  \"Outputs\": [{\n    \"Name\": \"success\",\n    \"Type\": \"Boolean\",\n    \"Description\": \"True if successful.\"\n    },\n    ... \n  ]\n }\n]\n</code></pre> <ul> <li>Route is route a caller will use to call this module. The route\u00a0vision/detection\u00a0will correspond to the API call\u00a0http://localhost:32168/v1/vision/detection</li> <li>Command\u00a0is the command that will be sent to this route. You can setup routes that go to the same path but send different commands. It's up to your module to handle the command appropriately</li> <li>Inputs\u00a0and Outputs\u00a0define the input\u00a0parameters and output results and are\u00a0used for automatically documenting your module</li> </ul>","tags":["settings"]},{"location":"devguide/modulesettings_json.html#adding-a-variant-modulesettings-file","title":"Adding a variant modulesettings file","text":"<p>Suppose you wished to provide a different setting for a module when it runs on, say, Windows. For example you may wish to use Python 3.9 for Windows. Just create a variant of the modulesettings with a file name the ensures it is selected only for Windows (See module settings files for  information). In this case <code>modulesettings.windows.json</code> will work for a general Windows install, and we only need override the values we wish to change:</p> JSON<pre><code>{\n  \"Modules\": {\n    \"MyModuleId\": {\n      \"LaunchSettings\": {\n        \"Runtime\": \"python3.9\"\n      }\n    }\n  }\n}\n</code></pre>","tags":["settings"]},{"location":"devguide/python_req_text.html","title":"Python requirements.txt files","text":"<p>There is no \"one-size-fits-all\" for Python packages. The Operating system,  hardware, libraries, the version of Python and many more things factor into what packages can and should be installed.</p> <p>To provide full coverage for all the possibilities there are two tactics. First, you can install python modules directly via the <code>install</code> script. The <code>installPythonPackagesByName</code> provides an easy way to achieve this.</p> <p>Second, you can provide system specific <code>requirements.txt</code> files, with the installer choosing the correct one at install time. The files are named based on device, OS, architecture, GPU type, and optionally CUDA library type.</p> <p>The order of preference for choosing a requirements file is as follows.</p> <ul> <li>requirements.device.txt</li> <li>requirements.os.architecture.cudaMajor_Minor.txt</li> <li>requirements.os.architecture.cudaMajor.txt</li> <li>requirements.os.architecture.(cuda|rocm).txt</li> <li>requirements.os.cudaMajor_Minor.txt</li> <li>requirements.os.cudaMajor.txt</li> <li>requirements.os.(cuda|rocm).txt</li> <li>requirements.cudaMajor_Minor.txt</li> <li>requirements.cudaMajor.txt</li> <li>requirements.(cuda|rocm).txt</li> <li>requirements.os.architecture.gpu.txt</li> <li>requirements.os.gpu.txt</li> <li>requirements.gpu.txt</li> <li>requirements.os.architecture.txt</li> <li>requirements.os.txt</li> <li>requirements.txt</li> </ul> <p>Specifiers:</p> <ul> <li>device is one of raspberrypi, orangepi or jetson. </li> <li>os is linux, macos, or windows</li> <li>architecture is x86_64 or arm64</li> <li>cudaMajor_Minor is the major/minor version of CUDA currently installed (eg cuda12.2).</li> <li>cudaMajor is just the major version (eg cuda12).</li> <li>rocm refers to AMD ROCm GPU support, and cuda refers to NVIDIA CUDA support. </li> <li>gpu is a generic identifier meaning \"use if GPU support is enabled, but no CUDA or ROCm GPUs have been detected\". This is great for packages that support multiple GPUs such as OpenVINO and DirectML.</li> </ul> <p>As an example, <code>requirements.linux.arm64.cuda11_7.txt</code> would be a requirements file specifically for Linux on arm64 systems, targeting CUDA 11.7. <code>requirements.windows.gpu.txt</code> would be for targeting Windows where a GPU was found. If, in this case, no GPU was found but there was a <code>requirements.windows.txt</code> file, then that would be used as a fallback. It's wise to always provide a generic, safe <code>requirements.txt</code> fallback.</p>","tags":["server","install"]},{"location":"devguide/queues_and_routes.html","title":"Queues and Routes in CodeProject.AI Server","text":"<p>To make a request to CodeProject.AI server you send your request to a specific API endpoint on the server. For example, <code>http://localhost:32168/v1/vision/custom/ipcam-general</code>, using POST. This route identifies the API version (v1), the module (vision), the command (custom), and a parameter (ipcam-general). For CodeProject.AI Server we refer to the <code>v1/vision/detect</code> part as the path, <code>vision/custom</code> as the route, and <code>ipcam-general</code>, as mentioned earlier, as the parameter. There may be more than 1 parameter.</p> <p>The server takes the data sent with the request and places that request on a queue defined by the module that has that route.</p> <p>Each module defines its own routes and queue names. The server determines which queue to use by searching all modules that service that route, and selects the queue name of the first module it finds that services that route. This means it's very important to (a) keep your routes unique between modules, and (b) keep queue names unique between modules. By default the queue name is just the module's ID with a '_queue' suffix.</p> <p>We've made an exception for the ObjectDetection modules. We have all Object detection modules use the same queue and the same routes. This allows you to start any Object detection module and have integrations like Agent DVR or Blue Iris work with any of them without the need to make any changes. Please note, however, that we recommend only having one Object detection module running at once.</p> <p>It's easy enough to modify the modulesettings files in each module to change this. You just define a separate route for each, and assign the, a separate queue. You then call the API using the route for the given module.</p> <p>You can have two modules running at the same time that share the same queue. The request from the client will be handled by which ever module happened to get to the queue first.</p> <p>Alternatively, and to sow confusion, you could have modules have different queues, but share the same route. The server places requests on the queue specified by the route that matches that queue. This search is deterministic, though, so the same module's queue would always be selected. This means only the module servicing the selected queue will get the request, even though the other has the same route as the first.</p>","tags":["server","Queues","Routes"]},{"location":"devguide/triggers.html","title":"Adding Triggers to CodeProject.AI Server","text":"<p>Running an AI inference operation on a piece of data to get predictions on the form or content of the data is one thing, but ultimately one wants to be able to act on those predictions.</p> <p>Typically this is done via the API. A client application sends an image, for instance, to an object detection or classification module and, based on the response for the server, that client will then take (or not take) appropriate action.</p> <p>Sometimes, though, you just want something simple, like calling a script or a shell command. For this you can use Triggers. </p> <p>Triggers are defined in the /Server/FrontEnd/triggers.json file. An example is</p> JSON<pre><code>{\n  \"Queue\"                     : \"objectdetection_queue\",\n  \"PredictionsCollectionName\" : \"predictions\",\n  \"PropertyName\"              : \"label\",\n  \"PropertyValue\"             : \"car\",\n  \"PropertyComparison\"        : \"equals\",\n  \"Confidence\"                : 0.5,\n  \"ConfidenceComparison\"      : \"greaterthan\",\n  \"PlatformTasks\" : {\n    \"Windows\" : { \n      \"Command\": \"cmd\",\n      \"Args\": \"/c echo Hi Windows. I see a car\",\n      \"Type\": \"Command\"\n    },\n    \"Linux\" : {\n      \"Command\": \"bash\",\n      \"Args\": \"echo Hi Linux. I see a car\",\n      \"Type\": \"Command\"\n    },\n    \"LinuxArm64\" : {\n      \"Command\": \"bash\",\n      \"Args\": \"echo Hi Linux. I see a car\",\n      \"Type\": \"Command\" \n    },\n    \"macOS\" : {\n      \"Command\": \"zsh\",\n      \"Args\": \"echo Hi Linux. I see a car\",\n      \"Type\": \"Command\"\n    },\n  }\n}\n</code></pre> <p>When an inference occurs, a response from the AI module is returned to the server on the same queue that the request was placed. Each queue can be watched individually to avoid undue processing.</p> <p>When a response comes back, the payload of that response is checked to see if any trigger conditions are met. The trigger conditions are as follows:</p> Condition Explanation Queue The queue to watch PropertyName The name of the property in the response to check PropertyValue The value of the property in the response to check. PropertyComparison The property value comparison method. This is the operation used to compare the <code>PropertyValue</code> with the value of the <code>PropertyName</code> property in the response. See below for comparison types. PredictionsCollectionName If specified, the system will look for a collection with this name, and apply the <code>PropertyName</code> / <code>PropertyValue</code> checks on each object in that collection, rather than looking for a property <code>PropertyName</code> at the root of the response object Confidence The confidence value to compare to the confidence score returned by the inference ConfidenceComparison The confidence comparison method. This is the operation used to compare the <code>Confidence</code> value with the confidence score returned by the inference PlatformTasks A set of tasks to perform depending on platform. See below <p>The Comparison method can be one of </p> <ul> <li>Equals</li> <li>LessThan</li> <li>LessThanOrEquals</li> <li>GreaterThan</li> <li>GreaterThanOrEquals</li> <li>NotEquals</li> </ul> <p>Suppose we wanted to watch the queue named <code>objectdetection_queue</code> and we were interested in spotting results with a label of 'car' with a confidence of at least 80%. This would translate to</p> <p>JSON<pre><code>{\n  \"Queue\"                     : \"objectdetection_queue\",\n  \"PropertyName\"              : \"label\",\n  \"PropertyValue\"             : \"car\",\n  \"PropertyComparison\"        : \"equals\",\n  \"Confidence\"                : 0.8,\n  \"ConfidenceComparison\"      : \"greaterthanorequals\",\n  ...\n}\n</code></pre> Suppose, however, our object detection module returns a collection of objects it has found, in the the <code>predictions</code> collection. Each item in that collection has a label and confidence. We would simply provide a <code>PredictionsCollectionName</code> value:</p> JSON<pre><code>{\n  \"Queue\"                     : \"objectdetection_queue\",\n  \"PredictionsCollectionName\" : \"predictions\",\n  \"PropertyName\"              : \"label\",\n  \"PropertyValue\"             : \"car\",\n  \"PropertyComparison\"        : \"equals\",\n  \"Confidence\"                : 0.8,\n  \"ConfidenceComparison\"      : \"greaterthanorequals\",\n  ...\n}\n</code></pre> <p>If a trigger condition for the given queue is met, then the task for the given platform will be executed. Tasks are listed under the <code>PlatformTasks</code> collection, with each platform having its own entry. </p> JSON<pre><code>{\n  \"Queue\"                     : \"objectdetection_queue\",\n  \"PredictionsCollectionName\" : \"predictions\",\n  \"PropertyName\"              : \"label\",\n  \"PropertyValue\"             : \"car\",\n  \"PropertyComparison\"        : \"equals\",\n  \"Confidence\"                : 0.8,\n  \"ConfidenceComparison\"      : \"greaterthanorequals\",\n\n  \"PlatformTasks\" : {\n    \"Windows\" : { \n      \"Command\": \"cmd\",\n      \"Args\": \"/c echo Hi Windows. I see a car\",\n      \"Type\": \"Command\"\n    },\n    \"Linux\" : {\n      \"Command\": \"bash\",\n      \"Args\": \"echo Hi Linux. I see a car\",\n      \"Type\": \"Command\"\n    }\n  }\n}\n</code></pre> <p>Supported platforms are</p> <ul> <li>Windows</li> <li>WindowsArm64</li> <li>Linux</li> <li>LinuxArm64</li> <li>MacOS</li> <li>MacOSArm64</li> </ul> Task Property Explanation Command The command that will be executed on the given platform Args The arguments to supply to that command Type The type of command. Currently only \"Command\" is supported","tags":["server","triggers"]},{"location":"devguide/module_examples/add_net_module-portrait.html","title":"Adding your own .NET module to CodeProject.AI","text":"","tags":["module"]},{"location":"devguide/module_examples/add_net_module-portrait.html#introduction","title":"Introduction","text":"<p>This article will show how to create a Module in .NET for the CodeProject.AI Server by adapting existing code and adding an adapter so that the functionality of the original code is exposed as  an HTTP endpoint by the CodeProject.AI Server. The process is similar to Adding your own Python module to CodeProject.AI, except we'll be using .NET instead pf Python and go a little deeper into some of the gotchas you may encounter.</p> <p>Again: ensure you've read [Adding new modules to CodeProject.AI](adding_new_modules.md] before you start.</p>","tags":["module"]},{"location":"devguide/module_examples/add_net_module-portrait.html#choosing-a-module","title":"Choosing a module","text":"<p>You can write your own module from scratch, or use one of the large number of Open Source AI projects currently available. There are thousands out there and for this article I chose the Portrait Mode project on GitHub written by Valery Asiryan. This project takes an image as an input, detects the people in it, and blurs the background behind them. The result is returned as an image. An easy way to up your selfie game.</p> <p>An example of the <code>Portrait Mode</code> in action from the GitHub repository with the original (left) and result (right)</p> <p></p>","tags":["module"]},{"location":"devguide/module_examples/add_net_module-portrait.html#writing-the-module","title":"Writing the Module","text":"<p>This tutorial assumes that you have already cloned the CodeProject.AI repository from GitHub. This is located at https://github.com/codeproject/CodeProject.AI-Server.</p> <p>To get the code we are going to include in our module, clone the <code>Portrait Mode</code> project on from GitHub to a directory of your choice.</p>","tags":["module"]},{"location":"devguide/module_examples/add_net_module-portrait.html#review-the-repository-code","title":"Review the Repository Code","text":"<p>Looking at the code, we see that the project consists of a Windows Form, Form1.cs, and the code to perform the background blurring. For our purposes, we do not need the Form. All we need are the 3 files located in the <code>cscharp\\Lib</code> directory and the ONNX model file located in the <code>deeplabv3_mnv2_pascal_train_aug_2018_01_29</code> directory</p>","tags":["module"]},{"location":"devguide/module_examples/add_net_module-portrait.html#create-a-new-module-project","title":"Create a new Module Project","text":"<p>When you write a CodeProject.AI Module in NET 7, you are creating something that polls the CodeProject.AI  Server for commands from a queue created for the module. The easiest way to do this is to create a Worker Service project in a folder under the <code>src/modules</code>. The CodeProject.AI Server scans the directories in this folder for Module metadata which allows the server to start the Modules.</p> <p>The steps to do this are:</p> <ul> <li> <p>Right click on the src/modules folder in the Solution Explorer</p> <ul> <li>Select <code>Add</code> -&gt; <code>New Project</code></li> <li>Select the Worker Service project template for C#</li> <li>Click <code>Next</code></li> </ul> </li> <li> <p>This will open the <code>Project Configuration</code> dialog </p> <ul> <li>Set the <code>Project Name</code> to PortrailFilter</li> <li>Set the <code>Location</code> to the src\\modules directory in your copy of the CodeProject.AI solution.</li> <li>click <code>Next</code>. </li> </ul> </li> <li> <p>This will open the <code>Additional Information</code> dialog </p> <ul> <li>We don't need to change anything here, so click <code>Create</code>.</li> </ul> </li> <li> <p>This will create the project with the following structure   </p> </li> </ul>","tags":["module"]},{"location":"devguide/module_examples/add_net_module-portrait.html#copy-code-from-the-repository","title":"Copy Code from the Repository","text":"<p>Since we only need the library code from the original <code>Portrait Mode</code> repo, we will create a folder called <code>Lib</code> to hold this code. - Right click on the <code>PortraitFilter</code> project     - Select <code>Add</code> -&gt; <code>New Folder</code>     - Name the folder <code>Lib</code>. - Copy the relevant code from the <code>Portrait Mode</code> repo.     - Right click on the <code>Lib</code> folder     - Select <code>Add</code> -&gt; <code>Existing Item</code>     - Navigate to you copy of the <code>Portrait Mode</code> repo and the <code>csharp\\Lib</code> directory     - Select all the files and click <code>Add</code> to copy the code into the new project - Copy the AI model from the <code>Portrait Mode</code> repo.     - Right click on the <code>Lib</code> folder     - Select <code>Add</code> -&gt; <code>Existing Item</code>     - Navigate to you copy of the <code>Portrait Mode</code> repo and the <code>deeplabv3_mnv2_pascal_train_aug_2018_01_29</code> directory     - Select <code>All Files</code> in the file filter at the bottom right     - Select the <code>.onnx</code> file and click <code>Add</code> to copy the modle into the new project</p>","tags":["module"]},{"location":"devguide/module_examples/add_net_module-portrait.html#add-the-required-nuget-packages","title":"Add the required NuGet packages","text":"<p>If you were to build the project now, you would get a bunch of errors about missing types or namespaces. This is due to missing NuGet packages used by the code we just copied into the project. This is solved by:</p> <ul> <li>Right click on the <code>PortraitFilter</code> project and select <code>Manage NuGet Packages...</code>. This will open a window where you can, as it says, manage your NuGet packages.</li> <li>Add the following packages<ul> <li><code>Microsoft.ML.OnnxRuntime</code> for running the <code>ONNX</code> model</li> <li><code>UMapX</code> - for performing operations on the images.</li> </ul> </li> <li>Building now will result in error complaining about compiling unsafe code. To allow this<ul> <li>Right click on the <code>Portrait Filter</code> project and select <code>Properties</code>.</li> <li>Check the <code>Allow code that uses the unsafe keyword to compile</code>.</li> </ul> </li> <li>Building now will result in a <code>'Maths' does not contain a definition for 'Double'</code> error. This is due to changes in <code>UMapX</code> from the version the <code>Portrait Mode</code> repo used and the current version. Correct this by changing the changing the type of <code>Strength</code> from <code>Double</code> to <code>Float</code> in the  <code>PortraitModeFilter.cs</code> file in the <code>Lib</code> directory. </li> <li>change the type of the <code>_strength</code> field to <code>float</code> on line 17</li> <li>change the type of the <code>strength</code> parameter to <code>float</code> on line 25</li> <li>change the type of the Strength property to float on line 35</li> <li>change the call to <code>Maths.Double</code> to <code>Maths.Float</code> on line 43. This is the UMapX change in the its API the required these changes.</li> </ul> C#<pre><code>using System.Drawing;\nusing UMapx.Core;\nusing UMapx.Imaging;\n\nnamespace CodeProject.AI.Modules.PortraitFilter\n{\n    /// &lt;summary&gt;\n    /// Defines \"portrait mode\" filter.\n    /// &lt;/summary&gt;\n    public class PortraitModeFilter\n    {\n        #region Private data\n        BoxBlur _boxBlur;\n        AlphaChannelFilter _alphaChannelFilter;\n        Merge _merge;\n        float _strength;\n        #endregion\n\n        #region Class components\n        /// &lt;summary&gt;\n        /// Initializes \"portrait mode\" filter.\n        /// &lt;/summary&gt;\n        /// &lt;param name=\"strength\"&gt;Strength&lt;/param&gt;\n        public PortraitModeFilter(float strength)\n        {\n            _boxBlur = new BoxBlur();\n            _alphaChannelFilter = new AlphaChannelFilter();\n            _merge = new Merge(0, 0, 255);\n            _strength = strength;\n        }\n        /// &lt;summary&gt;\n        /// Gets or sets strength.\n        /// &lt;/summary&gt;\n        public float Strength\n        {\n            get\n            {\n                return _strength;\n            }\n            set\n            {\n                _strength = Maths.Float(value);\n            }\n        }\n        ...\n</code></pre> <p>Building now should result in no errors.</p>","tags":["module"]},{"location":"devguide/module_examples/add_net_module-portrait.html#create-the-modulesettingsjson-file","title":"Create the modulesettings.json file","text":"<p>To provide the CodeProject.AI Server with enough information to launch and service our module we need to create the modulesettings.json file. This will define the name of the module, how to start it, what queue to setup and process, and the routes that should be watched for requests to this module.</p> <p>Our runtime will be .NET, and we'll set our queue name as <code>portraitfilter_queue</code>. The route for this module will be /v1/image/portraitfilter, and will have two inputs, a File with an named  \"image\", and Float value with a named \"strength\". There will be  two outputs, a Boolean value named \"success\", and the resulting image File named \"filtered_image\".</p> <p>Below is the contents of our <code>modulesettings.json</code> file. Note that we're only supporting Windows for this module due to issues with the Onnx and drawing libraries we're using. Portint to Linux and macOS is an exercise for the reader.</p> <p>JSON<pre><code>{\n  \"Modules\": {\n    \"PortraitFilter\": {\n      \"Name\": \"Portrait Filter\",\n      \"Version\": \"1.1\",\n\n      \"Description\": \"Provides a depth-of-field (bokeh) effect on images. Great for selfies.\", \n      \"Platforms\": [ \"windows\" ], \n      \"License\": \"MIT\",\n      \"LicenseUrl\": \"https://opensource.org/licenses/MIT\",\n\n      // Launch instructions\n      \"AutoStart\": true,\n      \"FilePath\": \"PortraitFilter.exe\",\n      \"Runtime\": \"execute\",\n\n      // Which server version is compatible with each version of this module.\n      \"ModuleReleases\": [\n        { \"ModuleVersion\": \"1.1\", \"ServerVersionRange\": [ \"2.1\", \"\" ], \"ReleaseDate\": \"2023-03-20\" }\n      ],\n\n      \"RouteMaps\": [\n        {\n          \"Name\": \"Portrait Filter\",\n          \"Path\": \"image/portraitfilter\",\n          \"Command\": \"filter\",\n          \"Description\": \"Blurs the background behind people in an image.\",\n          \"Inputs\": [\n            {\n              \"Name\": \"image\",\n              \"Type\": \"File\",\n              \"Description\": \"The image to be filtered.\"\n            }, \n            {\n              \"Name\": \"strength\",\n              \"Type\": \"Float\",\n              \"Description\": \"How much to blur the background 0.0 - 1.0.\",\n              \"DefaultValue\": \"0.5\"\n            }\n          ],\n          \"Outputs\": [\n            {\n              \"Name\": \"success\",\n              \"Type\": \"Boolean\",\n              \"Description\": \"True if successful.\"\n            },\n            {\n              \"Name\": \"filtered_image\",\n              \"Type\": \"Base64ImageData\",\n              \"Description\": \"The filtered image.\"\n            }\n          ]\n        }\n      ]\n    }\n  }\n}\n</code></pre> Because the location of the executable in the Development environment is different that the Production environment, we need to create a <code>modulesettings.development.json</code> file that overrides the  FilePath value when run in Debug.  </p> <p>This file overrides some of the <code>modulesettings.json</code> file values for the Development environment. In this case, the location of the executable will be found in the <code>bin\\debug\\net6.0</code> directory rather than the Module's root folder, so we force the working directory to be the directory of the module, and update the location of the file to execute, relative to the module's folder. JSON<pre><code>{\n    \"Modules\": {\n        \"PortraitFilter\": {\n           \"FilePath\": \"bin\\\\debug\\\\net7.0\\\\SentimentAnalysis.exe\"\n        }\n    }\n}\n</code></pre></p>","tags":["module"]},{"location":"devguide/module_examples/add_net_module-portrait.html#create-the-background-worker","title":"Create the Background Worker","text":"<p>The next step is to create the Background Worker. This will poll the CodeProject.AI Server for commands, process any commands it finds, and return the result to the CodeProject.AI Server.</p> <p>The Worker will use the CodeProject.AI .NET SDK to communicate with the CodeProject.AI Server and process the request and response values.  Add a Project Reference to the <code>CodeProject.AI.SDK</code> project. This is a preliminary implementation and will change in the future, mainly to add features, so this code will require minimal changes going forward.</p>","tags":["module"]},{"location":"devguide/module_examples/add_net_module-portrait.html#add-required-references","title":"Add required references","text":"<p>In addition, we will be using the SkiaSharp library to provide cross platform graphics support as well as support for images not supported by the Windows <code>System.Drawing</code> library. Add the following NuGet packages to the project:</p> <ul> <li><code>SkiaSharp.Views.Desktop.Common</code> to include the SkiaSharp core and interoperability with System.Drawing.</li> <li><code>SkiaSharp.NativeAssets.Linux</code> to provide support when running under Linux.</li> </ul>","tags":["module"]},{"location":"devguide/module_examples/add_net_module-portrait.html#create-the-portraitfilter-worker","title":"Create the PortraitFilter Worker","text":"<p>Open the Worker.cs file and to the following:    - rename the class to <code>PortraitFilterWorker</code>. The file should now look like  </p> C#<pre><code>namespace CodeProject.AI.Modules.PortraitFilter\n{\n    public class PortraitFilterWorker : ModuleWorkerBase\n    {\n        private readonly ILogger&lt;PortraitFilterWorker&gt; _logger;\n\n        /// &lt;summary&gt;\n        /// Initializes a new instance of the PortraitFilterWorker.\n        /// &lt;/summary&gt;\n        /// &lt;param name=\"logger\"&gt;The Logger.&lt;/param&gt;\n        /// &lt;param name=\"deepPersonLab\"&gt;The deep Person Lab.&lt;/param&gt;\n        /// &lt;param name=\"configuration\"&gt;The app configuration values.&lt;/param&gt;\n        public PortraitFilterWorker(ILogger&lt;PortraitFilterWorker&gt; logger,\n                                    IConfiguration configuration)\n            : base(logger, configuration)\n        {\n\n        public override BackendResponseBase ProcessRequest(BackendRequest request)\n        {\n        }\n    }\n}\n</code></pre> <ul> <li>Ensure that the <code>Program.cs</code> file has also been updated</li> </ul> C#<pre><code>using CodeProject.AI.Modules.PortraitFilter;\n\nIHost host = Host.CreateDefaultBuilder(args)\n    .ConfigureServices(services =&gt;\n    {\n        services.AddHostedService&lt;PortraitFilterWorker&gt;();\n    })\n    .Build();\n\nawait host.RunAsync();\n</code></pre> <ul> <li>Add the following using statements at the top of the file to reference the libraries and projects this class needs.</li> </ul> C#<pre><code>using System.Drawing;\nusing CodeProject.AI.SDK;\n\nusing Microsoft.ML.OnnxRuntime;\n\nusing SkiaSharp;\nusing SkiaSharp.Views.Desktop;\n</code></pre> <ul> <li>At the top of the namespace add a definition for the response that will be returned to the <code>CodeProject.AI Server</code>. The BackendSuccessResponse base class is defined in the SDK and provides the boolean \"success\" property. The JSON serializer will encode the byte[] filtered_image value to Base64.</li> </ul> C#<pre><code>namespace CodeProject.AI.Modules.PortraitFilter\n{\n    class PortraitResponse : BackendSuccessResponse\n    {\n        public byte[]? filtered_image { get; set; }\n    }\n</code></pre> <ul> <li>Next, we will derive our class from the SDK's <code>ModuleWorkerBase</code> class and   add some fields that will be use in the class.<ul> <li><code>_modelPath</code> which defines the location of the ONNX model.</li> <li><code>_defaultQueueName</code> which identifies the default queue from which this module will pull its requests. Used only if it's not passed in by the server.</li> <li><code>_defaultModuleId</code> which is a default unique identifier for this module.</li> <li><code>_moduleName</code> which is a default name for this module.</li> <li><code>_deepPersonLab</code> which will hold a reference to the code we imported from the <code>Portrait Mode</code> repo.</li> </ul> </li> </ul> C#<pre><code>public class PortraitFilterWorker : ModuleWorkerBase\n{\n    private const string _modelPath = \"Lib\\\\deeplabv3_mnv2_pascal_train_aug.onnx\";\n    private DeepPersonLab? _deepPersonLab;\n</code></pre> <ul> <li>Create the class constructor to initialize the fields. Note that when creating the <code>DeepPersonLab</code> instance, the <code>_modulePath</code> has the directory separator characters adjusted for the platform on which we are running. The      GetSessionOptions method provides an opportunity to setup GPU support for     the module. This is outside of the scope of this article for now.</li> </ul> C#<pre><code>public PortraitFilterWorker(ILogger&lt;PortraitFilterWorker&gt; logger,\n                            IConfiguration configuration)\n    : base(logger, configuration, _moduleName, _defaultQueueName, _defaultModuleId)\n{\n    string modelPath = _modelPath.Replace('\\\\', Path.DirectorySeparatorChar);\n\n    // if the support is not available for the Execution Provider DeepPersonLab will throw\n    // So we try, then fall back\n    try\n    {\n        var sessionOptions = GetSessionOptions();\n        _deepPersonLab = new DeepPersonLab(modelPath, sessionOptions);\n    }\n    catch\n    {\n        // use the defaults\n        _deepPersonLab = new DeepPersonLab(modelPath);\n        ExecutionProvider = \"CPU\";\n        HardwareType      = \"CPU\";\n    }\n}\n\nprivate SessionOptions GetSessionOptions()\n{\n    var sessionOpts = new SessionOptions();\n\n    // add GPU support here if you wish\n\n    sessionOpts.AppendExecutionProvider_CPU();\n    return sessionOpts;\n}\n\n// Override if you have updated graphics hardware to report to the server\nprotected override void GetHardwareInfo()\n{\n}\n</code></pre> <ul> <li> <p>When the <code>PortraitFilterWorker</code> is run, the <code>ProcessRequest</code> method will be called each time a request is placed on the queue for this module. Our <code>ProcessRequest</code> method will process this request, and return the results.     The server will handle the communication back to the client.</p> <p>Replace the existing <code>ExecuteAsync</code> method with</p> </li> </ul> C#<pre><code>public override BackendResponseBase ProcessRequest(BackendRequest request)\n{\n    if (_deepPersonLab == null)\n        return new BackendErrorResponse(-1, $\"{ModuleName} missing _deepPersonLab object.\");\n\n    // ignoring the file name\n    var file        = request.payload?.files?.FirstOrDefault();\n    var strengthStr = request.payload?.values?\n                                        .FirstOrDefault(x =&gt; x.Key == \"strength\")\n                                        .Value?[0] ?? \"0.5\";\n\n    if (!float.TryParse(strengthStr, out var strength))\n        strength = 0.5f;\n\n    if (file?.data is null)\n        return new BackendErrorResponse(-1, \"Portrait Filter File or file data is null.\");\n\n    Logger.LogInformation($\"Processing {file.filename}\");\n\n    // dummy result\n    byte[]? result = null;\n\n    try\n    {\n        var portraitModeFilter = new PortraitModeFilter(strength);\n\n        byte[]? imageData = file.data;\n        Bitmap? image     = GetImage(imageData);\n\n        if (image is null)\n            return new BackendErrorResponse(\"Portrait Filter unable to get image from file data.\");\n\n        Stopwatch stopWatch = Stopwatch.StartNew();\n        Bitmap mask = _deepPersonLab.Fit(image);\n        stopWatch.Stop();\n\n        if (mask is not null)\n        {\n            Bitmap? filteredImage = portraitModeFilter.Apply(image, mask);\n            result = ImageToByteArray(filteredImage);\n        }\n    }\n    catch (Exception ex)\n    {\n        return new BackendErrorResponse($\"Portrait Filter Error for {file.filename}: {ex.Message}.\");\n    }\n\n    if (result is null)\n        return new BackendErrorResponse(\"Portrait Filter returned null.\");\n\n    return new PortraitResponse { \n        filtered_image = result,\n        inferenceMs    = sw.ElapsedMilliseconds\n    };\n}\n</code></pre> <ul> <li>Additionally we need to add a few methods to the end of the class to<ol> <li>output to the log when the worker shuts down.</li> <li>create a Bitmap from the request data</li> <li>convert an Image to a byte[]</li> </ol> </li> </ul> C#<pre><code>// Using SkiaSharp as it handles more formats.\nprivate static Bitmap? GetImage(byte[] imageData)\n{\n    if (imageData == null)\n        return null;\n\n    var skiaImage = SKImage.FromEncodedData(imageData);\n    if (skiaImage is null)\n        return null;\n\n    return skiaImage.ToBitmap();\n}\n\npublic static byte[]? ImageToByteArray(Image img)\n{\n    if (img is null)\n        return null;\n\n    using var stream = new MemoryStream();\n\n    // See https://github.com/dotnet/designs/blob/main/accepted/2021/system-drawing-win-only/system-drawing-win-only.md\n#pragma warning disable CA1416 // Validate platform compatibility\n    img.Save(stream, System.Drawing.Imaging.ImageFormat.Png);\n#pragma warning restore CA1416 // Validate platform compatibility\n\n    return stream.ToArray();\n}\n</code></pre> <p>That's all the code that is needed to get a fully functional AI Module created for CodeProject.AI Server.  This code is included in the public https://github.com/codeproject/CodeProject.AI-Server GitHub repo.</p>","tags":["module"]},{"location":"devguide/module_examples/add_net_module-portrait.html#testing-with-the-debugger","title":"Testing with the Debugger","text":"<p>When you run the CodeProject.AI Server in the debugger, the Server will detect the module because:</p> <ul> <li>it is in a subdirectory of src\\AnalysisLayer</li> <li>it has a modulesettings.json file</li> </ul> <p>The Server will start all the modules it finds and have AutoStart=true in the configuration. The Dashboard will start and after a few seconds you should see  </p> <p></p>","tags":["module"]},{"location":"devguide/module_examples/add_net_module-portrait.html#running-on-linux","title":"Running on Linux","text":"<p>The module uses <code>System.Drawing</code> objects and methods</p> <p>As of NET 6, System.Drawing is not officially supported on non-Windows platforms as documented in https://docs.microsoft.com/en-us/dotnet/core/compatibility/core-libraries/6.0/system-drawing-common-windows-only. Running on Linux can enabled by creating a runtimeconfig.json file with the following contents JSON<pre><code>{\n   \"configProperties\": {\n      \"System.Drawing.EnableUnixSupport\": true\n   }\n}\n</code></pre> Also SkiaSharp has dependencies on a couple of libraries that may not be installed by default on you Linux installation. if you get the error </p> <p>System.TypeInitializationException: The type initializer for 'SkiaSharp.SKAbstractManagedStream' threw an exception. ---&gt; System.DllNotFoundException: Unable to load shared library 'libSkiaSharp' or one of its dependencies.</p> <p>Then you need to install the missing dependencies</p> Bash<pre><code>apt-get install -y libfontconfig1\napt-get install -y libgdiplus\n</code></pre>","tags":["module"]},{"location":"devguide/module_examples/add_net_module-portrait.html#create-a-testhtml-file","title":"Create a test.html file.","text":"<p>While this module is currently included in the install as of v1.3.0 with support in the CodeProject.AI Playground page, for new modules you would create a simple test page to send a request to the CodeProject.AI Server and display the response.</p> <p>I have include one such file, test.html, in the project. I displays a simple form the allows the user to select an image, set the blur strength and submit the form. The returned response is displayed. It include all the JavaScript code required to send the form to the Server and display the Base64 encoded image response. This is listed below.</p> <p>Just start the Server in the debugger and open the test.html file.</p> <p>HTML<pre><code>&lt;!DOCTYPE html&gt;\n\n&lt;html lang=\"en\" xmlns=\"http://www.w3.org/1999/xhtml\"&gt;\n&lt;head&gt;\n    &lt;!-- Required meta tags --&gt;\n    &lt;meta charset=\"utf-8\"&gt;\n    &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1\"&gt;\n\n    &lt;title&gt;Portrait Filter Test Page&lt;/title&gt;\n\n    &lt;!-- Bootstrap 5 CSS only --&gt;\n    &lt;link href=\"https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css\" rel=\"stylesheet\"\n          integrity=\"sha384-1BmE4kWBq78iYhFldvKuhfTAU6auU8tT94WrHftjDbrCEXSU1oBoqyl2QvZ6jIW3\"\n          crossorigin=\"anonymous\"&gt;\n    &lt;script type=\"text/javascript\"&gt;\n        const apiServiceUrl = \"http://localhost:32168\";\n\n        function setStatus(text, color) {\n            if (color)\n                document.getElementById(\"status\").innerHTML = \"&lt;span style='color:\" + color + \"'&gt;\" + text + \"&lt;/span&gt;\";\n            else\n                document.getElementById(\"status\").innerHTML = \"&lt;span&gt;\" + text + \"&lt;/span&gt;\";\n        }\n\n        function submitRequest(controller, apiName, images, parameters, doneFunc) {\n\n            setStatus(\"Sending request to AI server\", \"blue\");\n\n            var formData = new FormData();\n\n            // Check file selected or not\n            if (images &amp;&amp; images.length &gt; 0) {\n                for (var i = 0; i &lt; images.length; i++) {\n                    file = images[i];\n                    formData.append('image' + (i + 1), file);\n                }\n            }\n\n            if (parameters &amp;&amp; parameters.length &gt; 0) {\n                for (var i = 0; i &lt; parameters.length; i++) {\n                    keypair = parameters[i];\n                    formData.append(keypair[0], keypair[1]);\n                }\n            }\n\n            var url = apiServiceUrl + '/v1/' + controller + '/' + apiName;\n\n            //result.innerHTML = \"\";\n\n            fetch(url, {\n                method: \"POST\",\n                body: formData\n            })\n                .then(response =&gt; {\n                    if (!response.ok)\n                        setStatus('Error contacting API server', \"red\");\n                    else {\n                        response.json().then(data =&gt; {\n                            if (data) {\n                                doneFunc(data)\n                                setStatus(\"Call to \" + apiName + \" complete\", \"green\");\n                            } else\n                                setStatus('No data was returned', \"red\");\n                        })\n                            .catch(error =&gt; {\n                                setStatus(\"Unable to read response: \" + error, \"red\");\n                            })\n                    }\n                })\n                .catch(error =&gt; {\n                    setStatus('Unable to complete API call: ' + error, \"red\")\n                });\n        }\n\n        function onSubmit(image, strength) {\n            if (image.files.length == 0) {\n                alert(\"No file was selected for scene detection\");\n                return;\n            }\n\n            var images = [image.files[0]];\n            filteredImage.src = \"\";\n\n            submitRequest(\"image\", \"portraitfilter\", images, [[\"strength\", strength.value]], function (data) {\n                // alert(\"got response\");\n                filteredImage.src = \"data:image/png;base64,\" + data.filtered_image;\n            });\n        }\n\n        function onFileSelect(image) {\n            if (image.files.length == 0) {\n                originalImage.src = \"\";\n                return;\n            }\n\n            originalImage.src = URL.createObjectURL(image.files[0]);\n            filteredImage.src = \"\";\n        }\n\n        function onStrengthChange() {\n            filteredImage.src = \"\";\n        }\n    &lt;/script&gt;\n\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;div class=\"container\"&gt;\n        &lt;h1 class=\"text-center\"&gt;Portrait Filter Test Page&lt;/h1&gt;\n        &lt;form method=\"post\" action=\"\" enctype=\"multipart/form-data\" id=\"myform\"&gt;\n            &lt;div class=\"row\"&gt;\n                &lt;div class=\"mb-3 col\"&gt;\n                    &lt;label class=\"form-label\"&gt;Image&lt;/label&gt;\n                    &lt;input class=\"form-control btn-light\" id=\"image\" type=\"file\" onchange=\"onFileSelect(image)\" /&gt;\n                &lt;/div&gt;\n                &lt;div class=\"mb-3 col-2\"&gt;\n                    &lt;label class=\"form-label\"&gt;Strength&lt;/label&gt;\n                    &lt;input class=\"form-control\" id=\"strength\" type=\"number\" min=\"0.0\" max=\"1.0\" step=\"0.1\" value=\"0.5\" \n                           onchange=\"onStrengthChange()\"/&gt;\n                &lt;/div&gt;\n            &lt;/div&gt;\n            &lt;div class=\"row\"&gt;\n                &lt;div class=\"mb-3 text-center\"&gt;\n                    &lt;input class=\"btn btn-primary\" type=\"button\" value=\"Submit\" onclick=\"onSubmit(image, strength)\" /&gt;\n                &lt;/div&gt;\n            &lt;/div&gt;\n        &lt;/form&gt;\n        &lt;div id=\"status\" class=\"text-center\"&gt;&lt;/div&gt;\n\n        &lt;div class=\"row\"&gt;\n            &lt;label id=\"originallbl\" class=\"col-6 text-center\"&gt;Original&lt;/label&gt;\n            &lt;label id=\"filteredlbl\" class=\"col-6 text-center\"&gt;Filtered&lt;/label&gt;\n        &lt;/div&gt;\n        &lt;div class=\"row\"&gt;\n            &lt;img id=\"originalImage\" class=\"col-6\" /&gt;\n            &lt;img id=\"filteredImage\" class=\"col-6\" /&gt;\n        &lt;/div&gt;\n    &lt;/div&gt;\n    &lt;!-- Bootstrap 5 JavaScript Bundle with Popper --&gt;\n    &lt;script src=\"https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js\"\n            integrity=\"sha384-ka7Sk0Gln4gmtz2MlQnikT1wXgYsOg+OMhuP+IlRH9sENBO0LRn5q+8nbTov4+1p\"\n            crossorigin=\"anonymous\"&gt;&lt;/script&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre> And it will look something like this </p>","tags":["module"]},{"location":"devguide/module_examples/add_net_module-portrait.html#added-bonus-bin-deploy-module-on-windows","title":"Added Bonus - Bin deploy Module on Windows.","text":"<p>NET 6 modules do not depend on the installation of any runtimes or the setup of virtual environments the same way Python modules are. The NET 6 runtime is already installed by the CodeProject.AI Server installer. Because of this, the Release version of the build can be bin deployed into an existing Windows installation of the CodeProject.AI Server. The steps to do this are:</p> <ul> <li>Build the module project in <code>Release</code> mode.</li> <li>Create a folder in the <code>c:\\Program Files\\CodeProject\\eAI\\AnalysisLayer</code> directory. This directory should have the same name as the FilePath's directory in the <code>modulesettings.json</code> file. For this example that would be \"PortraitFilter\".</li> <li>Copy the contents of the project's bin\\Release\\net6.0 directory to the directory created in the previous step.</li> <li>Using the <code>Service</code> app, restart the CodeProject.AI Server service. The new module will now be expose on the endpoint defined in the modulesettings.json file.</li> </ul>","tags":["module"]},{"location":"devguide/module_examples/add_net_module-sentiment.html","title":"Adding your own .NET module to CodeProject.AI","text":"","tags":["module"]},{"location":"devguide/module_examples/add_net_module-sentiment.html#introduction","title":"Introduction","text":"<p>This article will show how to create a Module in .NET for the CodeProject.AI Server by adapting existing code and adding an adapter. This will enable the functionality of the original code to be exposed as an HTTP endpoint by the CodeProject.AI Server. The process is similar to Adding your own Python module to CodeProject.AI, except we'll be using .NET instead of Python.</p> <p>Again: ensure you've read Adding new modules to CodeProject.AI before you start.</p>","tags":["module"]},{"location":"devguide/module_examples/add_net_module-sentiment.html#choosing-a-module","title":"Choosing a module","text":"<p>You can write your own module from scratch, or use one of the large number of Open Source AI projects currently available. There are thousands out there and for this article I chose the TextClassificationTF sample in the DotNet Samples repository. This project takes some text and returns whether the text has a positive or negative sentiment.</p>","tags":["module"]},{"location":"devguide/module_examples/add_net_module-sentiment.html#adding-the-module-to-codeprojectai","title":"Adding the Module to CodeProject.AI","text":"<p>First, make sure you've cloned (or downloaded) the CodeProject.AI code from GitHub. </p> <p>To add our chosen module to CodeProject.AI we will need to do the following (we'll go through each step in detail in a moment):</p> <ol> <li> <p>Create a new project in the CodeProject.AI solution to hold our module.</p> </li> <li> <p>Create a modulesettings.json file in the project to configure the endpoint that will be exposed by the CodeProject.AI Server for this Module. The CodeProject.AI Server will discover this file on startup, configure the defined endpoints, and start the module's executable.</p> </li> <li> <p>Copy over the code from the module you wish to add into the Project you just created.    For neatness it can be preferable to create a sub-folder in the Project that houses the    entire code from the other module.</p> </li> </ol> <p>The Goal is to ensure it's easy to include updates. The code you use will no     doubt be updated over time. It's nice to be able to take the updated code and just    drop it into the same sub-folder. Instant upgrade. Give or take a little plumbing.</p> <ol> <li> <p>Add references to any NuGet packages the original code referenced.</p> </li> <li> <p>If required, refactor the copied code for general use. This is usually needed because many   pieces of code you might find might have hardwired values or locations, may be designed to   be called as an API, or as a command line, or be in a web application's controller somewhere   deep. </p> <p>You may need to make some (hopefully) minor changes to expose a function that your adapter   can call, or replace hardwired values with values provided through environment variables.   Again: the less changes you need to make the better, but some may be unavoidable.</p> </li> <li> <p>Create a CodeProject.AI adapter by deriving a class to process the requests receive from the CodeProject.AI Server and return the response. You will derive from an abstract base class and only have to provide a method that processes the request. All the boilerplate Server/Module communication and error handling is taken care of for you.</p> </li> <li> <p>Make minor changes to the project's <code>Program.cs</code> file to configure the program to run the above code.</p> </li> <li> <p>Test. This can be done by using tools such as Postman or writing a simple web page to call the new endpoint on the CodeProject.AI Server.</p> </li> </ol>","tags":["module"]},{"location":"devguide/module_examples/add_net_module-sentiment.html#create-the-modules-project","title":"Create the Module's Project","text":"<p>When you write a CodeProject.AI Module in NET 6, you are creating something that polls the CodeProject.AI Server for commands from a queue created for the module. The easiest way to do this is to create a Worker Service project in a folder under the <code>src/modules</code>. The CodeProject.AI Server scans the directories in this folder for Module metadata which allows the server to start the Modules.</p> <p>The steps to do this are:</p> <ul> <li> <p>Right click on the src/modules folder in the Solution Explorer</p> <ul> <li>Select <code>Add</code> -&gt; <code>New Project</code></li> <li>Select the Worker Service project template for C#</li> <li>Click <code>Next</code></li> </ul> </li> </ul> <p>This will open the <code>Project Configuration</code> dialog  </p> <p></p> <ul> <li>Set the <code>Project Name</code> to SentimentAnalysis</li> <li>Set the <code>Location</code> to the src\\modules directory in your copy of the       CodeProject.AI solution.</li> <li>click <code>Next</code>. </li> </ul> <p>This will open the <code>Additional Information</code> dialog  </p> <p></p> <p>We don't need to change anything here, so click <code>Create</code>. This will create the project with the following structure  </p> <p></p>","tags":["module"]},{"location":"devguide/module_examples/add_net_module-sentiment.html#create-the-modulesettingsjson-file","title":"Create the modulesettings.json file","text":"<p>The <code>modulesettings.json</code> files configures the Module for</p> <ul> <li>whether it should be started</li> <li>how to start it</li> <li>what platforms it runs on</li> <li>the endpoint(s) that will be exposed by the CodeProject.AI Server for this Module.  In this case we will be <ul> <li>exposing <code>http://localhost:32168/v1/text/sentiment</code></li> <li>using a the HTTP POST method</li> <li>sending one form variable <code>text</code> that will contain the text to be analyzed</li> <li>and expect a JSON payload response with<ul> <li>a Boolean <code>success</code> property indicating if the operation was successfully completed</li> <li>a Boolean <code>is_positive</code> property indicating if the input text had a positive sentiment</li> <li>a float <code>positive_probability</code> of the probability the input text had a positive sentiment where 0.5 is neutral.</li> </ul> </li> </ul> </li> </ul> <p>In our case the <code>modulesettings.json</code> file will look like this</p> JSON<pre><code>{\n  \"Modules\": {\n    \"SentimentAnalysis\": {\n      \"Name\": \"Sentiment Analysis\",\n      \"Version\": \"1.1\",\n\n      // Publishing info\n      \"Description\": \"Provides an analysis of the sentiment of a piece of text. Positive or negative?\", \n      \"Platforms\": [ \"windows\", \"macos\" ], \n      \"License\": \"CC-BY-4.0\",\n      \"LicenseUrl\": \"https://github.com/dotnet/samples/blob/main/LICENSE\",\n\n      // Launch instructions\n      \"AutoStart\": true,\n      \"FilePath\": \"SentimentAnalysis.exe\",\n      \"Runtime\": \"execute\",\n      \"RuntimeLocation\": \"Shared\", // Can be Local or Shared. .NET so moot point here\n\n      // Which server version is compatible with each version of this module.\n      \"ModuleReleases\": [\n        { \"ModuleVersion\": \"1.1\", \"ServerVersionRange\": [ \"2.1\", \"\" ], \"ReleaseDate\": \"2023-03-20\" }\n      ],\n\n      \"RouteMaps\": [\n        {\n          \"Name\": \"Sentiment Analysis\",\n          \"Path\": \"text/sentiment\",\n          \"Method\": \"POST\",\n          \"Command\": \"sentiment\",\n          \"Description\": \"Determines if the supplied text has a positive or negative sentiment\",\n          \"Inputs\": [\n            {\n              \"Name\": \"text\",\n              \"Type\": \"Text\",\n              \"Description\": \"The text to be analyzed.\"\n            }\n          ],\n          \"Outputs\": [\n            {\n              \"Name\": \"success\",\n              \"Type\": \"Boolean\",\n              \"Description\": \"True if successful.\"\n            },\n            {\n              \"Name\": \"is_positive\",\n              \"Type\": \"Boolean\",\n              \"Description\": \"Whether the input text had a positive sentiment.\"\n            },\n            {\n              \"Name\": \"positive_probability\",\n              \"Type\": \"Float\",\n              \"Description\": \"The probability the input text has a positive sentiment.\"\n            },\n            {\n              \"Name\": \"inferenceMs\",\n              \"Type\": \"Integer\",\n              \"Description\": \"The time (ms) to perform the AI inference.\"\n            },\n            {\n              \"Name\": \"processMs\",\n              \"Type\": \"Integer\",\n              \"Description\": \"The time (ms) to process the image (includes inference and text manipulation operations).\"\n            },\n            {\n              \"Name\": \"analysisRoundTripMs\",\n              \"Type\": \"Integer\",\n              \"Description\": \"The time (ms) for the round trip to the analysis module and back.\"\n            }\n          ]\n        }\n      ]\n    }\n  }\n}\n</code></pre>","tags":["module"]},{"location":"devguide/module_examples/add_net_module-sentiment.html#modulesettingsdevelopmentjson","title":"modulesettings.development.json","text":"<p>This file overrides some of the <code>modulesettings.json</code> file values for the Development environment. In this case, the location of the executable will be found in the <code>bin\\debug\\net7.0</code> directory rather than the Module's root folder, so we force the working directory to be the directory of the module, and update the location of the file to execute, relative to the module's folder. JSON<pre><code>{\n  \"Modules\": {\n    \"SentimentAnalysis\": {\n      \"FilePath\": \"bin\\\\debug\\\\net7.0\\\\SentimentAnalysis.exe\"\n    }\n  }\n}\n</code></pre></p>","tags":["module"]},{"location":"devguide/module_examples/add_net_module-sentiment.html#copy-code-and-assets-from-the-sample-code","title":"Copy code and assets from the sample code","text":"<p>The data and models uses by the sentiment analysis code are contained in the <code>sentiment_model</code> folder of the sample's repository. Copy this folder to the new module project.</p> <p>The code to use the data and modules is all contained in the <code>Program.cs</code> file of the sample code. To copy over this code</p> <ul> <li>create a new class file <code>TextClassifier.cs</code></li> <li>replace the contents of the <code>TextClassifier</code> class in this file with the contents of the <code>Program</code> class in the <code>Program.cs</code> file of the sample code. We will fix this up in the next step</li> </ul>","tags":["module"]},{"location":"devguide/module_examples/add_net_module-sentiment.html#include-additional-nuget-and-project-dependencies","title":"Include additional NuGet and Project dependencies","text":"<p>In order to build this project, there are a few dependencies that must be included:</p> <ul> <li>NuGet Packages that are required to use Microsoft's ML.NET framework and its support for TensorFlow models.<ul> <li><code>Microsoft.ML</code></li> <li><code>Microsoft.ML.SampleUtils</code></li> <li><code>Microsoft.ML.TensorFlow</code></li> <li><code>SciScharp.TensorFlow.Redist</code></li> </ul> </li> <li>Projects to use the CodeProject.AI NET SDK<ul> <li><code>CodeProject.AI.AnalsisLayer.SDK</code></li> </ul> </li> </ul>","tags":["module"]},{"location":"devguide/module_examples/add_net_module-sentiment.html#refactor-the-sample-code-for-our-use","title":"Refactor the sample code for our use","text":"<p>The code from the sample code is intended as a specific example with hard-coded inputs and lots of <code>Console.WriteLine</code> statements to show lots of details of the operation of the code. We updated the code by</p> <ul> <li>turning the main() method into the class constructor TextClassifier</li> <li>making some variables into fields</li> <li>changing the PredictSentiment method to take parameters instead of using hard-coded values.</li> </ul> <p>As the actual details of the changes are not what we are trying to accomplish with this article, the code is not shown here. The results of these changes can be seen in the code in the repository.</p>","tags":["module"]},{"location":"devguide/module_examples/add_net_module-sentiment.html#create-the-request-processor-class","title":"Create the Request Processor class","text":"<p>The next to last coding step is to create the background worker that will retrieve requests from the CodeProject.AI Server, process the request, and return the result. With the updated SDK, most of this has been encapsulated in an abstract class <code>ModuleWorkerBase</code>. All we have to do is create a new class file <code>SentimentAnalysisWorker.cs</code> and in this file</p> <ul> <li>create a response class <code>SentimentAnalysisResponse</code>, derived from <code>BackendSuccessResponse</code> which defines the structure of the modules response.</li> <li>override the <code>SentimentAnalysisWorker.ProcessRequest</code> method and </li> <li>create the <code>SentimentAnalysisWorker</code> constructor to initialize the functionality specific to the Module.</li> </ul> <p>The completed SentimentAnalysisWorker.cs file is</p> C#<pre><code>using CodeProject.AI.SDK;\n\nnamespace SentimentAnalysis\n{\n    class SentimentAnalysisResponse : BackendSuccessResponse\n    {\n        /// &lt;summary&gt;\n        /// Gets or set a value indicating whether the text is positive.\n        /// &lt;/summary&gt;\n        public bool? is_positive { get; set; }\n\n        /// &lt;summary&gt;\n        /// Gets or sets the probability of being positive.\n        /// &lt;/summary&gt;\n        public float? positive_probability { get; set; }\n    }\n\n    public class SentimentAnalysisWorker : ModuleWorkerBase\n    {\n        private readonly TextClassifier _textClassifier;\n\n        /// &lt;summary&gt;\n        /// Initializes a new instance of the SentimentAnalysisWorker.\n        /// &lt;/summary&gt;\n        /// &lt;param name=\"logger\"&gt;The Logger.&lt;/param&gt;\n        /// &lt;param name=\"textClassifier\"&gt;The TextClassifier.&lt;/param&gt;\n        /// &lt;param name=\"configuration\"&gt;The app configuration values.&lt;/param&gt;\n        public SentimentAnalysisWorker(ILogger&lt;SentimentAnalysisWorker&gt; logger,\n                                       TextClassifier textClassifier,  \n                                       IConfiguration configuration)\n            : base(logger, configuration)\n        {\n            _textClassifier  = textClassifier;\n        }\n\n        /// &lt;summary&gt;\n        /// The work happens here.\n        /// &lt;/summary&gt;\n        /// &lt;param name=\"request\"&gt;The request.&lt;/param&gt;\n        /// &lt;returns&gt;The response.&lt;/returns&gt;\n        public override BackendResponseBase ProcessRequest(BackendRequest request)\n        {\n            string text = request.payload.GetValue(\"text\");\n            if (text is null)\n                return new BackendErrorResponse($\"{ModuleName} missing 'text' parameter.\");\n\n            Stopwatch sw = Stopwatch.StartNew();\n            var result = _textClassifier.PredictSentiment(text);\n            long inferenceMs = sw.ElapsedMilliseconds;\n\n            if (result is null)\n                return new BackendErrorResponse($\"{ModuleName} PredictSentiment returned null.\");\n\n            var response = new SentimentAnalysisResponse\n            {\n                is_positive          = result?.Prediction?[1] &gt; 0.5f,\n                positive_probability = result?.Prediction?[1],\n                processMs            = inferenceMs,\n                inferenceMs          = inferenceMs\n            };\n\n            return response;\n        }\n    }\n}\n</code></pre>","tags":["module"]},{"location":"devguide/module_examples/add_net_module-sentiment.html#hook-it-all-up","title":"Hook it all up","text":"<p>Hooking everything together trivial.  In the Program.cs file </p> <ul> <li>Change the line  C#<pre><code>services.AddHostedService&lt;Worker&gt;();\n</code></pre>   to C#<pre><code>services.AddHostedService&lt;SentimentAnalysisWorker&gt;();\n</code></pre></li> <li>Add TextClassifier to the DI container by adding the line C#<pre><code>services.AddSingleton&lt;TextClassifier&gt;();\n</code></pre>    just before the previously line. The file should look like    C#<pre><code>using SentimentAnalysis;\n\nIHost host = Host.CreateDefaultBuilder(args)\n    .ConfigureServices(services =&gt;\n    {\n        services.AddSingleton&lt;TextClassifier&gt;();\n        services.AddHostedService&lt;SentimentAnalysisWorker&gt;();\n    })\n    .Build();\n\nawait host.RunAsync();\n</code></pre></li> </ul> <p>You will want to make the SentimentAnalysis a Build Dependency of the Frontend projects so that it is built when the CodeProject.AI Server is built.</p>","tags":["module"]},{"location":"devguide/module_examples/add_net_module-sentiment.html#test-it","title":"Test it.","text":"<p>To test this, I created a simple <code>test.html</code> page that takes some text, sends it to the CodeProject.AI Server, and handles and displays the result. It's as bare bones as I could make it to show how easy it is to use the new feature.</p> HTML<pre><code>&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\" xmlns=\"http://www.w3.org/1999/xhtml\"&gt;\n&lt;head&gt;\n    &lt;meta charset=\"utf-8\" /&gt;\n    &lt;title&gt;Sentiment Analysis Module Test&lt;/title&gt;\n    &lt;script type=\"text/javascript\"&gt;\n\n        function doAnalysis(textToSend) {\n            var formData = new FormData();\n            formData.append('text', textToSend);\n            fetch('http://localhost:32168/v1/text/sentiment', {\n                    method: 'POST',\n                    body: formData,\n                    cache: \"no-cache\"\n                })\n                .then(response =&gt; {\n                    if (!response.ok) {\n                        result.innerText = `Http error! Status : ${response.status}`;\n                    }\n                    return response.json().then(data =&gt; {\n                        var resultHTML = data.is_positive\n                                ? `&lt;p&gt;The text sentiment was positive with a probablity of ${data.positive_probability}&lt;/p&gt;`\n                                : `&lt;p&gt;The text sentiment was negative with a probablity of ${1.0 - data.positive_probability}&lt;/p&gt;`;\n                        result.innerHTML = resultHTML;\n                    });\n                });\n        }\n    &lt;/script&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;h1&gt;Sentiment Analysis Module Test&lt;/h1&gt;\n    &lt;form method=\"post\" action=\"\" enctype=\"multipart/form-data\" id=\"myform\"&gt;\n        &lt;div&gt;\n            &lt;label for=\"textToAnalyze\"&gt;Text to analyze:&lt;/label&gt;\n            &lt;div&gt;\n                &lt;textarea id=\"textToAnalyze\" name=\"textToAnalyze\" rows=\"8\" cols=\"80\" style=\"border:solid thin black\"&gt;&lt;/textarea&gt;\n            &lt;/div&gt;        \n        &lt;/div&gt;\n        &lt;div&gt;\n            &lt;button type=\"button\" onclick=\"doAnalysis(textToAnalyze.value)\"&gt;Submit&lt;/button&gt;\n        &lt;/div&gt;\n        &lt;br /&gt;\n        &lt;div&gt;\n            &lt;label for=\"result\"&gt;Result&lt;/label&gt;\n            &lt;div id=\"result\" name=\"result\" style=\"border:solid thin black\"&gt;&lt;/div&gt;\n        &lt;/div&gt;\n    &lt;/form&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre> <p>To see this in action, run the <code>Frontend</code> project (CodeProject.AI Server) in the Debugger in Debug configuration and then open the <code>test.html</code> file in the browser of choice. Copy some text into the text box and press submit. I used text from an Amazon review. You should see something similar to this:</p> <p></p>","tags":["module"]},{"location":"devguide/module_examples/add_net_module-sentiment.html#added-bonus-xcopy-deploy-module-on-windows","title":"Added Bonus - XCOPY deploy Module on Windows.","text":"<p>NET 6 modules do not depend on the installation of any runtimes or the setup of virtual  environments the same way Python modules are. The NET 6 runtime is already installed by the  CodeProject.AI Server installer. Because of this, the Release version of the build can be bin deployed into an existing Windows installation of the CodeProject.AI Server. The steps to do this are:</p> <ul> <li>Build the module project in <code>Release</code> mode.</li> <li>Create a folder in the <code>c:\\Program Files\\CodeProject\\AI\\modules</code> directory. This directory should have the same name as the module ID in the <code>modulesettings.json</code> file. For this example that would be \"SentimentAnalysis\".</li> <li>Copy the contents of the project's bin\\Release\\net7.0 directory to the directory created in the previous step.</li> <li>Using the <code>Service</code> app, restart the CodeProject.AI Server service. The new module will now be expose on the endpoint defined in the modulesettings.json file.</li> </ul>","tags":["module"]},{"location":"devguide/module_examples/add_net_module-sentiment.html#summing-up","title":"Summing up","text":"<p>Adding a new module to CodeProject.AI is generally not that difficult. You need to</p> <ol> <li>Choose a module that is self contained and can expose functionality as a method call.</li> <li>Create a project in the AnalysisServices folder to house your projects, and copy over the code</li> <li>Create an adapter that will interface between the CodeProject.AI server and your code</li> <li>Ensure you have the models and dependencies in place</li> <li>Create a modulesettings.json file to describe to the server how to start the module</li> <li>Make any minor changes needed to the module you're adding to allow it to function with      your adapter and modify the Program.cs file to get things rolling</li> </ol> <p>Your module is now part of your CodeProject.AI ecosystem and any clients that make use of the  server can now access your new module seamlessly. Congratulations!</p>","tags":["module"]},{"location":"devguide/module_examples/add_python_module.html","title":"Adding your own Python module to CodeProject.AI","text":"","tags":["module"]},{"location":"devguide/module_examples/add_python_module.html#adding-your-own-python-module-to-codeprojectai","title":"Adding your own Python module to CodeProject.AI","text":"<p>Recall from Adding new modules to CodeProject.AI that there are 6 main  tasks for adding a module (in development mode) to CodeProject.AI server</p> <ol> <li>Find or write the code you want to include. This could be a project you find online, a project you've written yourself you wish to include, or you might just start from scratch on a new project.</li> <li>Write an adapter that handles communication between the AI code you've written or are including in the module, and the CodeProject.AI server itself.</li> <li>Provide a <code>modulesettings.json</code> file that describes the module and provides instruction    to CodeProject.AI on how to launch the module.</li> <li>Create an install script (usually short) to setup the pre-requisites (download models, install necessary runtimes)</li> <li>(Optional but recommended) Create a simple <code>explore.html</code> file for testing your module, and to provide integration with the CodeProject.AI Explorer</li> <li>(Optional but recommended) Create a packager so your module can be packaged up and included in the main CodeProject.AI registry.</li> </ol> <p>We'll walk through each of these steps below, but first let's cover how a module communicates with the rest of the world. </p>","tags":["module"]},{"location":"devguide/module_examples/add_python_module.html#how-does-a-client-of-codeprojectai-server-know-how-to-call-a-given-module","title":"How does a client of CodeProject.AI Server know how to call a given module?","text":"<p>Each module defines a route in the form of <code>category/operation/command</code>. For instance, a computer vision module that offers object detection using a model called \"animals\" might define a route <code>vision/detect/animals</code>. This route is specified in the module's <code>modulesettings.json</code> file. The server takes care of the routing. A module can have as many routes as it wishes.</p>","tags":["module"]},{"location":"devguide/module_examples/add_python_module.html#how-does-the-client-know-what-inputoutput-to-provide-and-hows-does-that-get-to-and-from-the-module","title":"How does the client know what input/output to provide, and hows does that get to and from the module?","text":"<p>Again, the <code>modulesettings.json</code> defines the input/output for each route for each module. It is up to the client to make a HTTP POST call to CodeProject.AI Server using the module's route (explained above) and a request body that has the name/value pairs the module is expecting. This HTTP call is then converted to a <code>RequestData</code> object and passed to the module, and the module, in turn, uses  <code>RequestData</code> methods to get values and files from this object. </p> <p>Once the module has the input data, it processes that data using its <code>process</code> method and then passes the results back to the server as a JSON package. The return data is documented in the <code>modulesettings.json</code> file, but there are no hard and fast rules on what should be returned, apart from a requirement that the response must include a <code>success</code> flag to indicate whether the inference operation succeeded. </p> <p>The server itself will also add data to this return package such as timing data and meta data such as the name of the module that processed the request.</p> <p>What this boils down to is the <code>process</code> method should accept a <code>RequestData</code> object and return a  <code>JSON</code> object.</p> <p>For long running processes, a module may simply return a 'succeeded' message as the return response, and the server can then provide further updates on the progress of the request via a <code>status</code> method. This is a subject for another article.</p>","tags":["module"]},{"location":"devguide/module_examples/add_python_module.html#choosing-the-code-you-will-add-as-a-module","title":"Choosing the code you will add as a module","text":"<p>We want a module that will drop in easily, be self contained, and which will work within the current ecosystem of Python and / or .NET on Windows, macOS and Ubuntu.</p> <p>For this demonstration we'll use the rembg module. <code>rembg</code> is a simple but fun AI module that takes any photo containing a subject and removes the background from the image. It runs under Python 3.9 or above. It exposes a simple API, works offline, and is fully  self-contained.</p>","tags":["module"]},{"location":"devguide/module_examples/add_python_module.html#writing-the-module","title":"Writing the Module","text":"<p>To make writing a module simpler and faster we'll use the CodeProject.AI Server's Python SDK and create a child class of the <code>ModuleRunner</code> class. This class provides 5 methods for overriding (though only 1 is mandatory) and takes care of all the plumbing to handle the communication with the server. </p> <p>These methods are:</p> <ul> <li><code>initialise</code> (optional) - This provides module initialisation (if any is needed)</li> <li><code>process</code> (required) - The does the actual processing of an AI request. </li> <li><code>status</code> (optional) - Override this if you wish to provide status updates back to the server</li> <li><code>selftest</code> (optional) - This method is called at install time and provides a chance to test the system</li> <li><code>cleanup</code> (optional) - Provides a chance to clean up any resources in use while the module was running</li> </ul> <p>For our example we'll add a <code>rembg_adapter.py</code> file to our project in order to create our <code>ModuleRunner</code> class. This class will override the <code>process</code> and <code>selftest</code> methods only.  We'll then drop in the code we grabbed from the original rembg project. To keep things neat we'll drop all the rembg code into a rembg subfolder, and have our adapter at the root.</p>","tags":["module"]},{"location":"devguide/module_examples/add_python_module.html#what-about-python-packages-and-a-virtual-environment","title":"What about Python packages and a Virtual Environment?","text":"<p>If we're working with Python then we should use a virtual environment and install all our Python packages within this virtual environment. Happily that's all automatically done as part of the module install process: All we need to do is specify that we're going to be using Python within the <code>modulesettings.json</code> file (discussed below), and include a `requirements.txt file (also discussed below). The setup system will ensure python is installed, create the virtual environment, and install all the Python packages  for us. It will even check they installed properly and let us know if there was a problem.</p> <p>When importing modules from your Virtual Environment you should ensure you include the CodeProject.AI server SDK files first. This ensure the path to your installed modules is set correctly.</p>","tags":["module"]},{"location":"devguide/module_examples/add_python_module.html#logging","title":"Logging","text":"<p>There are logging methods that can be access in a module, but all output to stdout (eg console output)  is captured by the server and displayed in the dashboard, so if your original code dumps stuff to console,  that's fine: it will all be visible to the end user.</p>","tags":["module"]},{"location":"devguide/module_examples/add_python_module.html#the-code","title":"The Code","text":"<p>The guts of the background remover are in the <code>rembg/bg.py</code> module so we'll need to import that into our adapter, as well as the CodeProject.AI Server SDK, along with any other modules we need. We will add the methods we'll be overriding, and fill in those methods as appropriate.</p> <p>In our case there's no initialisation, no cleanup, and we won't pass back any status info (not a lot to report, really), so that leaves us with <code>process</code> and <code>selftest</code>. </p> <p>The code for these is very simple. For <code>process</code> we will take the input data from the <code>RequestData</code> object passed to us and extract a file and a value (named <code>use_alphamatting</code>) from this object.  These two parameters will be listed in the modulesettings.json file, so we hope our client who made the call included these values. If they didn't, the call will fail. Assuming all is good, we call the <code>remove</code> method from the original rembg project, and send the result back to the server as a JSON object. Super simple.</p> <p>The <code>selftest</code> method is equally simple. We create a <code>RequestData</code> object, fill it with test data, call the <code>process</code> method, and report success or failure. Care must be take to ensure that if environment variables are required, these will need to be set before the call is made.</p> <p>Finally, we need to ensure our adapter starts up. This is done by via the <code>start_loop</code> method of the Module runner</p> <p>The final, complete code, is below.</p> Python<pre><code># Import our general libraries\nimport sys\nimport time\n\n# Import the CodeProject.AI SDK. This will add to the PATH var for future imports\nsys.path.append(\"../../SDK/Python\")\nfrom request_data import RequestData\nfrom module_runner import ModuleRunner\nfrom common import JSON\n\n# Import modules we need from our virtual environment\nfrom PIL import Image\n\n# Import the method of the module we're wrapping\nfrom rembg.bg import remove\n\nclass rembg_adapter(ModuleRunner):\n\n    def process(self, data: RequestData) -&gt; JSON:\n        \"\"\" Processes a request from the client and returns the results\"\"\"\n        try:\n            # Get the input data. In this case an image and a bool value for the\n            # 'use_alphamatting' param  \n            img: Image             = data.get_image(0)\n            use_alphamatting: bool = data.get_value(\"use_alphamatting\", \"false\") == \"true\"\n\n            # Make the call to the AI code we're wrapping (and time it)\n            start_time = time.perf_counter()\n            (processed_img, inferenceTime) = remove(img, use_alphamatting)\n            processMs = int((time.perf_counter() - start_time) * 1000)\n\n            # Return a JSON object\n            return { \n                \"success\":      True, \n                \"imageBase64\":  RequestData.encode_image(processed_img),\n                \"processMs\" :   processMs,\n                \"inferenceMs\" : inferenceTime\n            }\n\n        except Exception as ex:\n            self.report_error(ex, __file__)\n            return {\"success\": False, \"error\": \"unable to process the image\"}\n\n    def selftest(self) -&gt; JSON:\n\n        import os\n        os.environ[\"U2NET_HOME\"] = os.path.join(self.module_path, \"models\")\n        file_name = os.path.join(\"test\", \"chris-hemsworth-2.jpg\")\n\n        request_data = RequestData()\n        request_data.add_file(file_name)\n        request_data.add_value(\"use_alphamatting\", \"true\")\n\n        result = self.process(request_data)\n        print(f\"Info: Self-test for {self.module_id}. Success: {result['success']}\")\n        # print(f\"Info: Self-test output for {self.module_id}: {result}\")\n\n        return { \"success\": result['success'], \"message\": \"Remove background test successful\" }\n\nif __name__ == \"__main__\":\n    rembg_adapter().start_loop()\n</code></pre> <p>A couple of notes on this code:</p> <ol> <li>We've used the ModuleRunner <code>report_error</code> method for reporting exceptions. This method takes the    exception and the name of the current file and reports the error to the server. It handles all    the formatting and stack tracing needed to provide a sensible error report.</li> <li> <p>We made one small (optional) change to the <code>remove</code> method in the rembg project to allows us     to report on inference time. This isn't needed: it just is a nice touch</p> <p>In <code>/rembg/bg.py</code> we wrap the <code>masks = session.predict</code> line with some timing: Python<pre><code>start_time = time.perf_counter()\nmasks = session.predict(img)\ninference_time: int = int((time.perf_counter() - start_time) * 1000)\n</code></pre> and we return a tuple containing the modified image and the inference time <code>python return (bio.read(), inference_time)</code>  This is the only code we've added. The rembg module has been copied and pasted as-is, and we're creating a child class of the ModuleRunner class in the CodeProject.AI SDK module module_runner.py. Nothing else (code-wise) needs to be added.</p> </li> </ol>","tags":["module"]},{"location":"devguide/module_examples/add_python_module.html#the-modulesettingsjson-file","title":"The modulesettings.json file","text":"<p>We need to tell client applications how they will call our module, and the CodeProject.AI server what route it needs to watch. This route can be whatever we like, so let's choose the route  <code>/v1/image/removebackground</code>. </p> <p>Our module settings file (<code>modulesettings.json</code>) also allows us to define environment variables that  are local to the process. Here, we'll let rembg know where the model file that we downloaded in the  setup script is stored.</p> <p>The module settings provides the server with publishing information that is used to create packages and package listings, which in turn allows the dashboard to display human readable information such as a description, version, author, the project that it's based on and licensing information.</p> <p>Any custom menu that should be added to the module's listing's dropdown menu on the dashboard is defined in the module settings file under the <code>Menus</code> object.</p> <p>Finally, as the module evolves, and as the server itself evolves, version compatibility between module and server is important. The <code>ModuleReleases</code> object defines what version of the module is compatible with which server version.</p> <p>Since this is a Python module we'll also set the runtime as Python3.9, and ask it to call our adapter file to start the process.</p> JSON<pre><code>{\n  \"Modules\": {\n\n    \"BackgroundRemover\": {\n      \"Name\": \"Background Remover\",\n      \"Version\": \"1.1.0\",\n\n      \"PublishingInfo\" : {\n        \"Description\": \"Automatically removes the background from a picture\", \n        \"IconURL\": null,\n        \"Category\": \"Image Processing\",\n        \"Stack\": \"Python, ONNX\",\n        \"License\": \"SSPL\",\n        \"LicenseUrl\": \"https://www.mongodb.com/licensing/server-side-public-license\",\n        \"Author\": \"Chris Maunder\",\n        \"Homepage\": \"https://codeproject.ai\",\n        \"BasedOn\": \"rembg\",\n        \"BasedOnUrl\": \"https://github.com/danielgatis/rembg\"\n      },\n\n      \"LaunchSettings\": {\n        \"AutoStart\": false,\n        \"FilePath\": \"rembg_adapter.py\",\n        \"Runtime\": \"python3.9\",\n        \"RuntimeLocation\": \"Local\",       // Can be Local or Shared\n        \"PostStartPauseSecs\": 0           // Generally 1 if using GPU, 0 for CPU\n      },\n\n      \"EnvironmentVariables\": {\n        \"U2NET_HOME\": \"%CURRENT_MODULE_PATH%/models\" // where to store the models\n      },\n\n      \"GpuOptions\" : {\n        \"InstallGPU\": false,              // GPU support not provided\n        \"EnableGPU\": true,                // Will be coerced to false if InstallGPU = false\n        \"AcceleratorDeviceName\": null,    // = default\n        \"Parallelism\": 1,                 // 0 = Default (number of CPUs - 1)\n        \"HalfPrecision\": \"enable\"         // 'Force', 'Enable', 'Disable': whether to force on, allow, or disable half-precision ops\n      },\n\n      \"InstallOptions\" : {\n        \"Platforms\": [ \"all\", \"!linux\", \"!raspberrypi\", \"!orangepi\", \"!jetson\" ], // issues with numpy on linux\n        \"PreInstalled\":   false,          // Is this module pre-installed with the server (eg Docker containers)\n        \"ModuleReleases\": [               // Which server version is compatible with each version of this module.\n          { \"ModuleVersion\": \"1.0.0\", \"ServerVersionRange\": [ \"1.0\",   \"2.0.8\"  ], \"ReleaseDate\": \"2022-11-01\" },\n          { \"ModuleVersion\": \"1.0.1\", \"ServerVersionRange\": [ \"2.0.9\", \"2.4.0\"  ], \"ReleaseDate\": \"2023-12-06\", \"ReleaseNotes\": \"Updated modulesettings schema\", \"Importance\": \"Minor\" },\n          { \"ModuleVersion\": \"1.1.0\", \"ServerVersionRange\": [ \"2.4.0\", \"\"       ], \"ReleaseDate\": \"2023-12-09\", \"ReleaseNotes\": \"Installer updates\" },\n        ]\n      },\n\n      \"RouteMaps\": [\n        {\n           ...\n        }\n      ]\n    }\n  }\n}\n</code></pre> <p>The EnvironmentVariables section defines key/value pairs that will be used to set environment variables that may be required by the module. In this case, the path to the AI model files. This is a value specific to, and defined by, the rembg module.</p> <p><code>CURRENT_MODULE_PATH</code> is a macro that will expand to the location of the directory containing the current module. In this case /src/modules/BackgroundRemover.</p> <p>The settings under \"Launch instructions\" section should be self-explanatory. The FilePath is the path to the file to be executed, relative to the <code>MODULES_PATH</code> directory. AutoStart sets whether or not this module will be launched at startup.</p> <p>Runtime defines what runtime will launch the file. We currently support dotnet (.NET), python37 (python 3.7) and python39 (Python 3.9). If omitted, the CodeProject.AI Server will attempt to guess based on the FilePath. RuntimeLocation refers to whether this module should have its runtime (in this case the Python3.9 environment) setup locally to the module, or in a shared location where libraries will be shared among multiple modules. Sharing saves space at the expense of compatibility issues.</p> <p>The Platforms array contains an entry for each platform on which the service can run. Currently Windows, Linux, Linux-Arm64, macOS, and macOS-Arm64 are supported.</p>","tags":["module"]},{"location":"devguide/module_examples/add_python_module.html#routemaps","title":"Routemaps","text":"<p>The file also defines the API routes for the module under the RouteMaps section</p> JSON<pre><code>{\n  \"Modules\": {\n    \"ModulesConfig\": {\n      \"BackgroundRemoval\": {\n         \"Name\": \"Background Removal\",\n         \"Version\": \"1.0\",\n         \"Description\": \"Removes backgrounds from images.\",\n\n         ...\n\n      \"RouteMaps\": [\n        {\n          \"Name\": \"Background Remover\",\n          \"Route\": \"image/removebackground\",\n          \"Method\": \"POST\",\n          \"Command\": \"removebackground\",\n          \"Description\": \"Removes the background from behind the main subjects in images.\",\n          \"Inputs\": [\n            {\n              \"Name\": \"image\",\n              \"Type\": \"File\",\n              \"Description\": \"The image to have its background removed.\"\n            },\n            {\n              \"Name\": \"use_alphamatting\",\n              \"Type\": \"Boolean\",\n              \"Description\": \"Whether or not to use alpha matting.\",\n              \"DefaultValue\": \"false\"\n            }\n          ],\n          \"Outputs\": [\n            {\n              \"Name\": \"success\",\n              \"Type\": \"Boolean\",\n              \"Description\": \"True if successful.\"\n            },\n            {\n              \"Name\": \"imageBase64\",\n              \"Type\": \"Base64ImageData\",\n              \"Description\": \"The base64 encoded image that has had its background removed.\"\n            },\n            {\n              \"Name\": \"inferenceMs\",\n              \"Type\": \"Integer\",\n              \"Description\": \"The time (ms) to perform the AI inference.\"\n            },\n            {\n              \"Name\": \"processMs\",\n              \"Type\": \"Integer\",\n              \"Description\": \"The time (ms) to process the image (includes inference and image manipulation operations).\"\n            }\n          ]\n         ]\n       }\n     }\n   }\n}\n</code></pre> <p>Path is the API path, in this case localhost:32168/v1/image/removebackground. Remember that this was what we chose (arbitrarily) as our API. It can be anything as long as it isn't currently in use.</p> <p>Method is the HTTP method to use. In our case POST.</p> <p>Command is the method in the API controller that will be called, in this case  <code>removebackground</code>. Command is passed to the <code>process</code> method and is only needed if your process method will handle more than one command.</p> <p>Description, Inputs and Outputs are purely documentation at this stage.</p>","tags":["module"]},{"location":"devguide/module_examples/add_python_module.html#creating-an-installer-script-or-two","title":"Creating an installer script. Or two.","text":"<p>the <code>rembg</code> module comprises the following</p> <ol> <li>The python code</li> <li>The python 3.9 interpreter</li> <li>Some Python packages</li> <li>The AI models</li> </ol> <p>To ensure these are all in place within the development environment we need to create an <code>install.bat</code> install script in the folder containing our project. In our case we don't need to do anything other than download models. The python runtime is setup automatically, as are the packages. There are no libraries or drivers to install, so our install script is simple.</p>","tags":["module"]},{"location":"devguide/module_examples/add_python_module.html#for-windows-installbat","title":"For Windows (install.bat)","text":"<p>This script will be called automatically from the global <code>setup.bat</code> script</p> Batchfile<pre><code>:: Download the models and store in /models\ncall \"%sdkScriptsPath%\\utils.bat\" GetFromServer \"rembg-models.zip\" \"models\" \"Downloading Background Remover models...\"\n</code></pre> <p>Obviously there's not a lot this script needs to do other than download the models for the module.</p> <p>When writing install scripts you have the following variables and methods at your disposal:</p>","tags":["module"]},{"location":"devguide/module_examples/add_python_module.html#variables-available","title":"Variables available:","text":"Variable Description absoluteAppRootDir the root path of the app (default: C:\\Program Files\\CodeProject\\AI) sdkScriptsPath the path to the installation utility scripts (%rootPath%\\SDK\\Scripts) runtimesPath the path to the installed runtimes (%rootPath%/src/runtimes) downloadPath the path to where downloads will be stored (%sdkScriptsPath%\\downloads) modulesPath the path to all the AI modules (%rootPath%\\src\\modules) moduleDir the name of the directory containing this module (def: name of current dir) os \"windows\" for this script architecture \"x86_64\" or \"arm64\" platform \"windows\" for this script verbosity quiet, info or loud. Use this to determines the noise level of output. forceOverwrite if true then ensure you force a re-download and re-copy of downloads. GetFromServer will honour this value. Do it yourself for DownloadAndExtract","tags":["module"]},{"location":"devguide/module_examples/add_python_module.html#methods-available-call-by-call-sdkscriptspathutilsbat-method","title":"Methods available (call by <code>call %sdkScriptsPath%\\utils.bat &lt;method&gt;</code>)","text":"Method Description Write     text [foreground [background]] Writes text without a linefeed using the given colors (eg call %sdkScriptsPath%\\utils.bat WriteLine \"Hi\" \"green\") WriteLine text [foreground [background]] Writes text with a linefeed using the given colors GetFromServer filename moduleAssetDir message Downloads a file from the server and places it in the directory specified within the module's folder <ul><li>filename       - Name of the compressed archive to be downloaded<li>moduleAssetDir - Name of folder in module's directory where archive will be extracted<li>message        - Message to display during download   DownloadAndExtract  storageUrl filename downloadPath dirNameToSave message Downloads a file from a remote server, unpacks it and stores it in the given folder <ul><li>storageUrl    - Url that holds the compressed archive to Download <li>filename      - Name of the compressed archive to be downloaded <li>downloadPath  - Path to where the downloaded compressed archive should be downloaded <li>dirNameToSave - name of directory, relative to downloadPath, where contents of archive will be extracted and saved <li>message       - Message to display during download","tags":["module"]},{"location":"devguide/module_examples/add_python_module.html#for-linux-and-macos-installsh","title":"For Linux and macOS (install.sh)","text":"<p>The script is essentially the same as the Windows version:</p> Bash<pre><code>getFromServer \"rembg-models.zip\" \"models\" \"Downloading Background Remover models...\"\n</code></pre> <p>The variables and methods available in Linux / macOS are the same as in Windows, with the exception that in Linux/macOS the methods are camelCased.</p>","tags":["module"]},{"location":"devguide/module_examples/add_python_module.html#create-a-simple-explorehtml-file","title":"Create a simple <code>explore.html</code> file","text":"<p>To test our module that's running under CodeProject.AI Server we'll write a simple webpage that will call the module via its API and display the results. We do this using a special file <code>explore.html</code> that will be written in a way that allows the CodeProject.AI Server's Explorer to extract the UI  from this explore.html file and incorporate it into the server's main explorer.html file</p> <p>The entire HTML file is shown below. Note the 3 sections marked by</p> <ol> <li>START/END EXPLORER STYLE</li> <li>START/END EXPLORER MARKUP</li> <li>START/END EXPLORER SCRIPT</li> </ol> <p>These three sections will be pulled out from this file and inserted into the main server's explorer.html file. For this to work seamlessly it's best if your explore.html test file for this module uses the same scripts and follows the same naming conventions as the explorer.html</p> <p>Specifically</p> <ol> <li>We include the explorer.js class to get access to the methods used to call the server API.</li> <li>We include <code>imgPreview</code> and <code>imgMask</code> elements for displaying image results</li> <li>We include a <code>results</code> element for displaying text results.</li> <li>We prefix any IDs that may have naming conflicts with the <code>_MID_</code> macro. This marker will be    replaced by the module's name in the main explorer.html file to avoid name collisions.</li> </ol> HTML<pre><code>&lt;html&gt;\n&lt;head&gt;\n    &lt;meta charset=\"utf-8\" /&gt;\n    &lt;title&gt;Background Remover Module Test&lt;/title&gt;\n\n    &lt;link id=\"bootstrapCss\" rel=\"stylesheet\" type=\"text/css\" href=\"http://localhost:32168/assets/bootstrap-dark.min.css\"&gt;\n    &lt;link rel=\"stylesheet\" type=\"text/css\" href=\"http://localhost:32168/assets/dashboard.css?v=2.5.0.0\"&gt;\n    &lt;script type=\"text/javascript\" src=\"http://localhost:32168/assets/explorer.js\"&gt;&lt;/script&gt;\n\n    &lt;style&gt;\n/* START EXPLORER STYLE */\n/* END EXPLORER STYLE */\n    &lt;/style&gt;\n\n&lt;/head&gt;\n&lt;body class=\"dark-mode\"&gt;\n&lt;div class=\"mx-auto\" style=\"max-width: 800px;\"&gt;\n\n    &lt;h2 class=\"mb-3\"&gt;Background Remover Module Test&lt;/h2&gt;\n    &lt;form method=\"post\" action=\"\" enctype=\"multipart/form-data\" id=\"myform\"&gt;\n\n&lt;!-- START EXPLORER MARKUP --&gt;\n        &lt;div class=\"form-group row\"&gt;\n            &lt;label class=\"col-form-label col-2\"&gt;Image&lt;/label&gt;\n            &lt;input class=\"col form-control btn-light\" id=\"_MID_removeBgInput\" type=\"file\" style=\"width:17rem\"\n                    onchange=\"return previewImage(this)\" /&gt;\n            &lt;input class=\"form-control btn-success\" type=\"button\" value=\"Remove Background\"\n                    style=\"width:11rem\" id=\"_MID_removeBackground\"\n                    onclick=\"_MID_onRemoveBackground(_MID_removeBgInput, _MID_use_alphamatting.checked)\" /&gt;\n        &lt;/div&gt;\n        &lt;div class=\"form-group row mt-1 justify-content-end\"&gt;\n            &lt;div class=\"checkbox checkbox-primary col-4\"&gt;\n                &lt;input id=\"_MID_use_alphamatting\" class=\"styled\" type=\"checkbox\" checked&gt;\n                &lt;label for=\"_MID_use_alphamatting\"&gt;Use Alpha matting&lt;/label&gt;\n            &lt;/div&gt;\n        &lt;/div&gt;\n&lt;!-- END EXPLORER MARKUP --&gt;\n\n        &lt;div class=\"w-100 position-relative form-control my-4 p-0\"&gt;\n            &lt;div id=\"imgMask\" class=\"position-absolute\"\n                    style=\"left:0;top:0;pointer-events:none;z-index:10\"&gt;&lt;/div&gt;\n            &lt;img src=\"\" id=\"imgPreview\" class=\"w-100\" style=\"height:250px;visibility:hidden\"&gt;\n        &lt;/div&gt;\n        &lt;div&gt;\n            &lt;h2&gt;Results&lt;/h2&gt;\n            &lt;div id=\"results\" name=\"results\" class=\"bg-light p-3\" style=\"min-height: 100px;\"&gt;&lt;/div&gt;\n        &lt;/div&gt;\n\n    &lt;/form&gt;\n\n    &lt;script type=\"text/javascript\"&gt;\n// START EXPLORER SCRIPT\n\n        function _MID_onRemoveBackground(fileChooser, use_alphamatting) {\n\n            clearImagePreview();\n\n            if (fileChooser.files.length == 0) {\n                alert(\"No file was selected for background removal\");\n                return;\n            }\n\n            previewImage(fileChooser);\n            let images = [fileChooser.files[0]];\n            let params = [[\"use_alphamatting\", use_alphamatting]]\n\n            setResultsHtml(\"Removing background...\");\n            submitRequest('image', 'removebackground', images, params, function (data) {\n                setResultsHtml(\"Background Removal complete\" + getProcessingMetadataHtml(data));\n                showResultsImageData(data);\n            });\n        }\n// END EXPLORER SCRIPT\n    &lt;/script&gt;\n&lt;/div&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre> <p>Once the server is running, and this module is started, then launching this HTML page locally will allow you to test the module live.</p>","tags":["module"]},{"location":"devguide/module_examples/add_python_module.html#create-a-packager","title":"Create a packager","text":"<p>The final (optional) step is to create a packager. The package creates a zip file that the installer can install. In our case the installer simply unzips the module's package and runs a command to download models, so our package only needs to create the zip file to be unzipped.</p> WindowsLinux / macOS Batchfile<pre><code>@Echo off\nREM Module Packaging script. To be called from create_packages.bat\n\nset moduleId=%~1\nset version=%~2\n\ntar -caf %moduleId%-%version%.zip --exclude=__pycache__  --exclude=*.development.* --exclude=*.log ^\n    rembg\\* *.py modulesettings.* requirements.* install.sh install.bat explore.html test\\*\n</code></pre> Bash<pre><code>#!/bin/bash\n# Module Packaging script. To be called from create_packages.sh\n\nmoduleId=$1\nversion=$2\n\ntar -caf ${moduleId}-${version}.zip --exclude=__pycache__  --exclude=*.development.* --exclude=*.log \\\n    rembg/* *.py modulesettings.* requirements.* install.sh install.bat explore.html test/*\n</code></pre>","tags":["module"]},{"location":"devguide/module_examples/add_python_module.html#install-and-test","title":"Install and test","text":"<p>At this point we have a module, an install script and a test client. Let's give  it a run</p> <ol> <li> <p>Ensure you have the latest CodeProject.AI repo  downloaded. That has all the code we've talked about above already in place</p> </li> <li> <p>Run the dev install script. This will see your new script and then run it to ensure Python 3.9 is installed and setup, and that the required Python modules are installed.</p> </li> <li> <p>Launch the server by starting a new debug session in Visual Studio or VS Code.</p> </li> <li> <p>In Debug, the CodeProject.AI Dashboard is automatically launched when run. After the server starts all  the backend Modules, including the Background Removal module, the Dashboard will display the  modules it knows about. </p> </li> <li> <p>Launch the <code>explore.html</code> file in a browser, choose a file and click \"Submit\" button. The results  should be shown.  Also test in the server's explorer.html.  </p> </li> </ol>","tags":["module"]},{"location":"devguide/module_examples/add_python_module.html#what-next","title":"What next?","text":"<p>That's up to you. We've demonstrated a very simple AI module that removes the background from an image. The main work was</p> <ol> <li>Ensuring you have the assets (eg models) available on a server so they can be downloaded</li> <li>Updating the install script so your assets can be downloaded and moved into place, as well as     ensuring you have the necessary runtime and libraries installed</li> <li>Dropping in your module's code and writing an adapter so it can talk to the CodeProject.AI Server</li> <li>Writing a modulesettings file that describes the API for your module</li> <li>Testing! Always the fun part.</li> </ol> <p>The possibilities on what you can add are almost limitless. Our goal is to enable you, as a   developer, to add your own AI modules easily, and in turn get the benefit of modules that others  have added. Mix and match, play with different sets of trained modules, experiment with settings  and see where you can take this.</p> <p>It's about learning and it's about having some fun. Go for it.</p>","tags":["module"]},{"location":"devguide/module_examples/adding_new_modules.html","title":"Adding new modules to CodeProject.AI","text":"<p>CodeProject.AI allows developers to easily add new AI functionality to an existing system  without having to fight the tools and libraries</p> <p>Adding AI capabilities to an app is reasonably straight forward if you're happy to follow the  twisty turny maze that is the endless list of libraries, tools, interpreters, package managers and  all the other fun stuff that sometimes makes coding about as fun as doing the dishes. </p> <p>CodeProject.AI makes this easier by providing a framework to manage this pain so you can focus on your code, not the tools.</p>","tags":["module"]},{"location":"devguide/module_examples/adding_new_modules.html#aggregating-not-adding","title":"Aggregating, not adding","text":"<p>We say \"add\", but \"aggregating\" is more accurate. There is a ton of amazing AI projects out there being actively developed and improved and we want to allow developers to take these existing, evolving AI modules or applications and drop them into the CodeProject.AI ecosystem with as little fuss as possible. This could mean dropping in a console application, a Python module, or a .NET project. </p> <p>For development, all you need to do is </p> <ol> <li>Find or write the code you want to include. This could be a project you find online, a project you've written yourself you wish to include, or you might just start from scratch on a new project.</li> <li>Write an adapter that handles communication between the AI code you've written or are including in the module, and the CodeProject.AI server itself.</li> <li>Provide a <code>modulesettings.json</code> file that describes the module and provides instruction    to CodeProject.AI on how to launch the module.</li> <li>Create an install script (usually short) to setup the pre-requisites (download models, install necessary runtimes)</li> <li>(Optional but recommended) Create a simple <code>explore.html</code> file for testing your module, and to provide integration with the CodeProject.AI Explorer</li> <li>(Optional but recommended) Create a packager so your module can be packaged up and included in the main CodeProject.AI registry.</li> </ol> <p>I just want to code!</p> <p>If you want to cut to the chase and just get on with writing a module, then read Adding your own Python module or Adding your own .NET module.</p>","tags":["module"]},{"location":"devguide/module_examples/adding_new_modules.html#the-codeprojectai-architecture-in-under-30-seconds","title":"The CodeProject.AI architecture in under 30 seconds","text":"<p>CodeProject.AI is an HTTP based REST API server. It's basically just a webserver to which your application  sends requests. Those requests are placed on a queue, and the analysis services (aka The Modules)  pick requests off the queues they know how to service. Each request is then processed (an AI operation is performed based on the request) and the results are sent back to the API server,  which in turn sends it back to the application that made the initial call.</p> <p>Suppose we had 3 analysis modules, Face recognition using Python 3.7, Object Detection using .NET,  and Text Analysis using Python 3.10:</p> <p> </p> <ol> <li>An application sends a request to the API server</li> <li>The API server places the request on the appropriate queue</li> <li>The backend modules poll the queue they are interested in, grab a request and process it</li> <li>The backend module then sends the result back to the API server</li> <li>The API Server then sends the result back to the calling application</li> </ol> <p>The CodeProject.AI API Server runs independently of the calling application. </p> <p>Think of CodeProject.AI like a database server or any other service you have running in the background:  it runs as a service or daemon, you send it commands and it responds with results. You don't  sweat the details of how it goes about its business, you just focus on your application's core  business.</p>","tags":["module"]},{"location":"devguide/module_examples/adding_new_modules.html#how-modules-work","title":"How Modules Work","text":"<p>Here's a complicated diagram explaining the modules</p> <p></p> <p>From left to right:</p> <ol> <li>We have the CodeProject.AI Server that starts up and monitors the analysis modules</li> <li> <p>We have a set of analysis modules that take requests from the CodeProject.AI Server's queues and    process them. Each module consists of</p> <ol> <li>An adapter which provides the communication between the module and the CodeProject.AI server,</li> <li>The module itself.</li> </ol> </li> <li> <p>We have a number of runtimes (eg Python or .NET) that the modules run under. Multiple modules    can share a given runtime: we don't (yet) sandbox.</p> </li> </ol> <p>The adapter for each module typically runs in the same runtime (and often within the same process) as the module, but this isn't required. You could easily write a simple Python script to act as an adapter that sends and receives data from a .NET module.</p>","tags":["module"]},{"location":"devguide/module_examples/adding_new_modules.html#setting-up-models-and-runtimes","title":"Setting up: models and runtimes","text":"<p>The setup script should take care of downloading an AI models that are needed as well as installing any necessary runtimes. By default we currently ensure Python 3.7 and 3.9, as well as .NET 7 are  installed and available to all. </p> <p>For Python modules the setup script would typically also ensure any Python packages are installed.</p> <p>A Windows BAT file and a Linux/macOS bash file should be provided for setup, depending on which platforms you're supporting.</p>","tags":["module"]},{"location":"devguide/module_examples/adding_new_modules.html#choosing-the-code-to-add","title":"Choosing the code to add","text":"<p>When thinking about what modules are suitable to include in CodeProject.AI, consider the following:</p> <ul> <li>Is the code self contained? The less baggage a module needs to drag along the better</li> <li>Does the module provide a simple, easily callable API? For instance a Python module may provide    a function you call, or a Go application may provide a simple console app experience that allows    it to be called from the command line.</li> <li>Can the module run offline? A module that requires an internet connection may simply not work    in all envuironments, or may not be acceptable to some users who need to ensure their data stays    within their environment</li> <li>If the module is updated, how hard will it be to drop in the updated code? The less modifications    you make to the original code, the easier it is to update the module later. Confine, if possible,    your code changes to your adapter</li> </ul>","tags":["module"]},{"location":"devguide/module_examples/adding_new_modules.html#writing-an-adapter","title":"Writing an Adapter","text":"<p>The adapter for a module has one task: to shuttle communications between CodeProject.AI and the module.</p> <p>An example could be a module written in Python. You have your my_module.py file that contains your AI inference code, and within that module might be a method <code>predict</code>. The adapter would</p> <ul> <li>Setup the module by querying the environment variables or checking the values modulesettings.json    files</li> <li>Process requests (in our case, call the <code>predict</code> method in our my_module.py file) and return the result</li> </ul> <p>If at all possible one should avoid modifying the module's code. The adapter abstracts the module from CodeProject.AI, so if the module is updated, the updates can be dropped in and the adapter will  (hopefully) still work. If not, adjusting the adapter to cater for a changed API, data format or method signature should be a quick and easy fix.</p>","tags":["module"]},{"location":"devguide/module_examples/adding_new_modules.html#the-modulesettingsjson-file","title":"The modulesettings.json file","text":"<p>The CodeProject.AI Server, on startup, will load the <code>modulesettings.json</code> file and its variants in the module's directory. The files are read by the NET Configuration system  in the following order:</p> <ul> <li>modulesettings.json - common and default configuration values</li> <li>modulesettings.&lt;production|development&gt;.json - production or development values</li> <li>modulesettings.&lt;platform&gt;.json - values specific to the runtime OS (platform).      Currently supported are: <code>windows</code>, <code>linux</code>, <code>docker</code>, <code>macos</code> and <code>macos-arm</code>.</li> <li>modulesettings.&lt;platform&gt;.&lt;production|development&gt;.json - values specific to a       platform and environment.</li> </ul> <p>The settings in each file will override any previously loaded settings, allowing you to specify, in each variant, only the settings you need to adjust for the given scenario.</p>","tags":["module"]},{"location":"devguide/module_examples/adding_new_modules.html#the-modulesettingsjson-schema","title":"The modulesettings.json schema","text":"<p>The <code>modulesettings.json</code> file defines the common metadata for the module in a <code>Modules</code> section. This metadata include information about </p> <ul> <li>A name and description for the module,</li> <li>Whether it should be run at startup,</li> <li>How the modules should be started.        a. Specify a <code>runtime</code> and a <code>filePath</code>. This will then launch the module specified in the       filepath using the given runtime. Currently supported runtimes include <code>dotnet</code>, <code>python37</code>, <code>python38</code>, and <code>python39</code>. Adding more Python runtimes is trivial.       b. Alternatively, specify a full command to run in order to start the module.</li> <li>A list of the Platorms that the module can be run under. Currently supported include      <code>windows</code>, <code>linux</code>, <code>docker</code>, <code>macos</code> and <code>macos-arm</code>. </li> <li>The name of the queue the module will process. This can be any name the module desires to use.</li> <li>The endpoints, (<code>RouteMaps</code>),  that the CodeProject.AI Server will expose for this       module. For example the endpoint could be a GET call to <code>image/detect_animals</code>, which would       map to GET: localhost:32168/v1/image/detect_animal. The inputs and outputs      for this endopoint are also included here, but are used solely for documentation.</li> </ul> <p>An example would be</p> JSON<pre><code>{\n  \"Modules\": {\n    \"PortraitFilter\": {\n      \"Name\": \"Portrait Filter\",\n      \"Version\": \"1.0.0\",\n\n      \"PublishingInfo\" : {\n        \"Description\": \"Provides a depth-of-field (bokeh) effect on images. Great for selfies.\", \n        \"Category\": \"Image Processing\"\n      },\n\n      \"LaunchSettings\": {\n        \"AutoStart\": true,\n        \"FilePath\": \"PortraitFilter.exe\",\n        \"Runtime\": \"dotnet\"\n      },\n\n      \"EnvironmentVariables\": {\n      },\n\n      \"GpuOptions\" : {\n        \"InstallGPU\": true,\n        \"EnableGPU\": true\n      },\n\n      \"InstallOptions\" : {\n        \"Platforms\": [ \"windows\" ],     // errors with Microsoft.ML.OnnxRuntime.NativeMethods in macOS, and System.Drawing issues in Linux\n        \"ModuleReleases\": [             // Which server version is compatible with each version of this module.\n          { \"ModuleVersion\": \"1.0\",   \"ServerVersionRange\": [ \"2.5.0\", \"\" ], \"ReleaseDate\": \"2022-06-01\" }\n        ]\n      },\n\n      \"RouteMaps\": [\n        {\n          \"Name\": \"Portrait Filter\",\n          \"Route\": \"image/portraitfilter\",\n          \"Method\": \"POST\",\n          \"Command\": \"filter\",\n          \"Description\": \"Blurs the background behind the main subjects in an image.\",\n          \"Inputs\": [\n            {\n              \"Name\": \"image\",\n              \"Type\": \"File\",\n              \"Description\": \"The image to be filtered.\"\n            },\n            {\n              \"Name\": \"strength\",\n              \"Type\": \"Float\",\n              \"Description\": \"How much to blur the background (0.0 - 1.0).\",\n              \"MinValue\": 0.0,\n              \"MaxValue\": 1.0,\n              \"DefaultValue\": 0.5\n            }\n          ],\n          \"Outputs\": [\n            {\n              \"Name\": \"success\",\n              \"Type\": \"Boolean\",\n              \"Description\": \"True if successful.\"\n            },\n            {\n              \"Name\": \"filtered_image\",\n              \"Type\": \"Base64ImageData\",\n              \"Description\": \"The base64 encoded image that has had its background blurred.\"\n            }\n          ]\n        }\n      ]\n    }\n  }\n}\n</code></pre>","tags":["module"]},{"location":"faq/index.html","title":"CodeProject.AI Server FAQ","text":"","tags":["FAQ"]},{"location":"faq/index.html#current-faqs","title":"Current FAQs","text":"<ul> <li>Blue Iris</li> <li>Coral.AI</li> <li>Development Environment</li> <li>Docker</li> <li>GPU</li> <li>Home Assistant</li> <li>Jetson Nano</li> <li>Server Mesh processing</li> <li>Raspberry Pi (and other hobby boards)</li> <li>Ubuntu installer</li> <li>Virtual machines</li> <li>Windows Installer</li> </ul>","tags":["FAQ"]},{"location":"faq/blue-iris.html","title":"Blue Iris Webcam Software","text":"","tags":["Blue-Iris"]},{"location":"faq/blue-iris.html#unable-to-load-model-at-cprogram-filescodeprojectaimodulesobjectdetectionyoloassetsyolov5mpt","title":"Unable to load model at C:\\Program Files\\CodeProject\\AI\\modules\\ObjectDetectionYolo\\assets\\yolov5m.pt","text":"<p>A user encountered the following error on their CodeProject.AI Server console.</p> Text Only<pre><code>1:08:29 PM: Object Detection (YOLO): Unable to load model at C:\\Program Files\\CodeProject\\AI\\modules\\ObjectDetectionYolo\\assets\\yolov5m.pt (CUDA error: no kernel image is available for execution on the device\n1:08:29 PM: Object Detection (YOLO): Unable to create YOLO detector for model yolov5m\n</code></pre> <p> In this case the user was using an NVIDIA GeForce GTX 660, with driver version 30.0.14.73481. This card has compute capability 3.0 which, at the time of the message's posting was below what CodeProject.AI Server supported. However, this user was also trying to use YOLO detection, which only works for custom models. The fix for this is to go to the CodeProject.AI Server dashboard, stop Object Detection (YOLO) by clicking the three dots next to it on the dashboard, selecting Stop, then going to Object Detection (.NET), clicking on the the three dots and choosing Start.</p>","tags":["Blue-Iris"]},{"location":"faq/blue-iris.html#codeprojectai-server-log-shows-requests-every-minute-or-less-when-there-is-no-motion-detection","title":"CodeProject.AI Server log shows requests every minute or less when there is no motion detection","text":"<p>In this instance a user was seeing requests in the CodeProject.AI Server log every minute or less when there was no motion detection.</p> <p></p> <p>Within Blue Iris there is an option within the AI tab in the camera settings panel called \"Detect/Ignore static objects\" where Blue Iris checks for static objects. If this box is checked, there will be a steady stream of requests in the CodeProject.AI Server log, because Blue Iris is constantly checking for static objects. Unchecking the \"Detect/Ignore static objects\" box and hitting OK resolves the issue.</p>","tags":["Blue-Iris"]},{"location":"faq/blue-iris.html#browser-cannot-open-port-321678","title":"Browser cannot open port 321678","text":"<p>In this issue on the CodeProject.AI Server forum, a user could not access http://localhost:32168 and was unable to launch CodeProject.AI Server in their browser.</p> <p></p> <p>To resolve this issue, a user recommended using netstat to see if that port was listening. The user was using Windows. To do this, hit the Win + R keys on the keyboard to open the Run window, then type <code>cmd</code> to open the CMD prompt. From there, type <code>netstat -ano -p tcp</code>. This opens a list of all ports and indicates which ports are listening.</p> <p></p> <p>For this user, port 32168 did not show up on the list at all. The fix was to go to Windows services and start or restart CodeProject.AI Server. To do this, hit the Win + R keys on the keyboard to open the Run window, then type <code>services.msc</code>. This opens Windows services. Scroll down to CodeProject.AI Server and hit the \"Start Service\" button. CodeProject.AI Server then launches successfully.</p>","tags":["Blue-Iris"]},{"location":"faq/blue-iris.html#codeprojectai-server-and-python-using-a-lot-of-system-resources","title":"CodeProject.AI Server and Python using a lot of system resources","text":"<p>This issue comes fom the Blue Iris User Group on Facebook (note: it is a private group). This user reported that their CPU system resources were hitting 20%.</p> <p></p> <p>They provided an image of their CodeProject.AI Server dashboard and they were running both Object Detection (.NET) and Object Detection (YOLO). These two modules were not meant to be used in unison. The solution is simply to turn Object Detection (YOLO) off. Go to the CodeProject.AI Server dashboard, click the three dots next to it on the dashboard, and select Stop.</p>","tags":["Blue-Iris"]},{"location":"faq/blue-iris.html#confirmed-but-nothing-detected","title":"Confirmed but nothing detected","text":"<p>This issue comes from the Blue Iris forums. In this instance Blue Iris detects motion but does not recognize anything. Blue Iris logs it as \"Confirmed\", but according to CodeProject.AI Server, nothing is found. The user wants to know why CodeProject.AI Server does not cancel this if nothing is found.</p> <p></p> <p>The solution comes from the ipcamtalk forums. In the global AI tab on the camera settings, there is a field \"To cancel.\" Using \"Nothing found:0\" in the \"To cancel\" box eliminates (green) \"Nothing found\" from the Confirmed alerts list. It forces the AI to search through all the images in an alert to select the best one.</p>","tags":["Blue-Iris"]},{"location":"faq/blue-iris.html#codeprojectai-server-in-docker-container-doesnt-respond-to-requests","title":"CodeProject.AI Server in Docker Container Doesn't Respond to Requests","text":"<p>In this setup, a user has CodeProject.AI running in a Docker container. CodeProject.AI loads, the web interface can be accessed, it can ping the Blue Iris server, but CodeProject.AI in both the Explorer and Blue Iris, just time out for detection requests and generate no logs.</p> <p>Here's a few items to try:</p> <p>Are there any firewall rules stopping POST requests to the CodeProject.AI server? You can obviously make GET calls in order to view the UI, and get updates.</p> <ul> <li>If you see a decent set of logs then it means the backend modules can contact the front end server</li> <li>If you can see any logs then it means the machine you're running the Explorer on can access the CodeProject.AI machine</li> <li>Move the log detail slider all the way to the right to show Trace. If you see any \"request from queue processing\" type messages then it means requests are getting to the modules.</li> <li>If you're seeing timeouts then either the modules are having issues returning values, or the modules are simply timing out does the dashboard show a object detection module running (green band)?</li> <li>If you're at this point, maybe try stopping one object detection module and switching to another (eg try the .NET instead of the Python or vice versa)</li> </ul>","tags":["Blue-Iris"]},{"location":"faq/blue-iris.html#how-do-i-resolve-error-500","title":"How Do I Resolve Error 500","text":"<p>With CodeProject.AI Server 2.1.6, a number of Blue Iris users report getting Error 500 in their Blue Iris logs.</p> <p>Short version: try the latest Blue Iris release, version 5.7.5.6. Should resolve it.</p> <p>Long version: The error 500 issue revolves around how Blue Iris processes Face Processing requests and responses. There's no \"error,\" rather Blue Iris is passing images to face recognition when there's no detectable face, and CodeProject.AI returns success = false (and code = 500) as a result. The call was made, the image processed, but the recognition processing failed because there's no face to recognise.</p> <p>Purely an issue around what constitutes success and failure.</p>","tags":["Blue-Iris"]},{"location":"faq/blue-iris.html#running-codeprojectai-on-a-different-system-from-blue-iris","title":"Running CodeProject.AI on a Different System from Blue Iris","text":"<p>Here is an example of how to get CodeProject.AI Server running on a different system than Blue Iris and accessing its GPU. In this example, CodeProject.AI is in a Docker container on a Linux system.</p> <ol> <li> <p>Install all the CodeProject.AI Server pre-requisites on the Linux system. </p> </li> <li> <p>Downloaded the Docker image </p> </li> <li> <p>Running that Docker image, run the CPAI dashboard and explorer (localhost:32168) and make sure all was OK and the GPU is being used. </p> </li> <li> <p>Reboot the Blue Iris system. On the AI tab point to the IP/port address on the Linux system.</p> </li> </ol> <p>If the CodeProject.AI system is slower, start it first. Do not start from the Blue Iris.</p>","tags":["Blue-Iris"]},{"location":"faq/blue-iris.html#slower-detection-times-with-coral-and-blue-iris","title":"Slower detection times with Coral and Blue Iris","text":"<p>If you're experiencing slower than expected detection times and you have a decent system with a Coral USB to boot, you might be expecting to see sub-250 ms detections. If you're seeing higher detection times, there are a couple of settings in Blue Iris you can try.</p> <p>In the main AI settings, in the Default object detection field, change the model size to Small. The reason for this, is that the Small is the fastest model.</p> <p>The other thing to do, is within the camera settings, in the Artificial Intelligence tab, make sure use main stream if available is unchecked. Having it checked slows down detection speed and does not improve accuracy.</p>","tags":["Blue-Iris"]},{"location":"faq/coral.html","title":"Coral USB Accelerator","text":"","tags":["Coral"]},{"location":"faq/coral.html#the-coral-usb-dongle-isnt-working","title":"The Coral USB dongle isn't working","text":"<p>If the Coral module installed OK but you're seeing errors (or no action) when you make calls to the module, try switching the USB port.</p> <p>If you have the Coral plugged in via a dock or dongle, try and connect the Coral directly to your computer, or at the very least ensure the dock or dongle passed through enough power to the dongle.</p> <p>If you are running on a Pi, ensure the Pi's power supply is sufficient to power the Pi and the Coral.</p>","tags":["Coral"]},{"location":"faq/coral.html#how-do-you-run-google-coral-at-max-performance-and-force-the-install-of-libedgetpu1-max","title":"How do you run Google Coral at max performance, and force the install of libedgetpu1-max?","text":"<p>If you're running CodeProject.AI Server in Docker or natively in Ubuntu and want to force the installation of libedgetpu1-max, first stop the Coral module from CodeProject.AI Server.</p> <p>If in docker, open a Docker terminal and launch bash:</p> bash<pre><code>docker exec codeprojectai /bin/bash\n</code></pre> <p>Then for Docker or native Ubuntu, run:</p> bash<pre><code>sudo apt-get remove libedgetpu1-std\nDEBIAN_FRONTEND=dialog sudo apt-get install libedgetpu1-max\n</code></pre> <p>Finally, start the Coral module from the CodeProject.AI Server web interface.</p>","tags":["Coral"]},{"location":"faq/coral.html#coral-and-blue-iris-slow-detection-times","title":"Coral and Blue Iris - Slow detection times","text":"<p>If you're using Blue Iris and a Coral USB, you may see detection times increasing past 1000 ms. You may also experience an error: \"The interpreter is in use. Please try again later\"</p> <p>In order to avoid this, in the Blue Iris main AI settings, change the model size to Small.</p> <p>Additionally, ensure you aren't sending too many images to CodeProject.AI Server. You can adjust the amount of images you're sending to CodeProject.AI Server in the Trigger tab for your camera, under Artificial Intelligence. In the field + real-time images make sure this number isn't too big. Somewhere around 10 should be a good value here, but you can adjust to taste.</p>","tags":["Coral"]},{"location":"faq/coral.html#using-coral-m2-accelerator-with-frigate","title":"Using Coral M.2 Accelerator with Frigate","text":"<p>If you have a Coral M.2 Accelerator that you're sharing between CodeProject.AI Server and Frigate, it may interfere with detections. </p> <p>If this is the case, try shutting Frigate down and seeing if detections in CodeProject.AI Server resume.</p>","tags":["Coral"]},{"location":"faq/coral.html#problems-getting-codeprojectai-server-to-identify-coral-m2-accelerator-with-unraid","title":"Problems getting CodeProject.AI Server to identify Coral M.2 Accelerator with Unraid","text":"<p>Some users have reported difficulty or errors like:</p> Text Only<pre><code>docker: Error response from daemon: could not select device driver \"\" with capabilities: [[gpu]].\n</code></pre> <p>when trying to get the CodeProject.AI Server Coral module working with their M.2. In these cases, stopping all Object Detection modules, uninstalling the Coral module, then re-installing it helps. </p> <p>A user made a helpful guide for this, that includes images and instructions on how to do a Docker install of CodeProject.AI Server on Unraid. The following is a re-working of that original article, with permission unRAID-CoralTPU-Guide w/ CodeProject.AI.</p>","tags":["Coral"]},{"location":"faq/coral.html#hardware-installation","title":"Hardware Installation","text":"<ul> <li>USB Accelerator - https://coral.ai/products/accelerator/</li> <li>M.2 Accelerator B+M key - https://coral.ai/products/m2-accelerator-bm/</li> <li>M.2 Accelerator with Dual Edge TPU - https://coral.ai/products/m2-accelerator-dual-edgetpu/</li> </ul> <p>If using the M.2 Accelerator with Dual Edge TPU you might have to use adapters to make it work with your system. These are located here:</p> <ul> <li>https://www.makerfabs.com/dual-edge-tpu-adapter.html</li> <li>https://www.makerfabs.com/dual-edge-tpu-adapter-m2-2280-b-m-key.html</li> </ul> <p>Simply install the TPU in your system and boot up your Unraid server.</p>","tags":["Coral"]},{"location":"faq/coral.html#unraid-coral-drivers-installation","title":"Unraid Coral Drivers Installation","text":"<p>First you must make sure you have the community applications installed. This can be done by going here: https://forums.unraid.net/topic/38582-plug-in-community-applications/</p> <p>Go to the APPS tab:</p> <p></p> <p>In the search bar type: coral accelerator module drivers</p> <p></p> <p>Once installed, go to SETTINGS &gt; CORAL DRIVER. If Unraid can see your TPU then you will see something like this:</p> <p></p>","tags":["Coral"]},{"location":"faq/coral.html#codeprojectai-docker-install","title":"CodeProject.AI Docker Install","text":"<p>Go back to the APPS tab in Unraid and search for codeproject.ai_server</p> <p></p> <p>Click on it and press install.</p> <p>Next, pass through the Coral TPU by clicking Add another Path, Port Variable, Label or Device</p> <p></p> <p>Change Config Type to \"Device\"</p> <p></p> <p>In the Value field put:</p> <ul> <li>USB - /dev/bus/usb</li> <li>M.2 - /dev/apex_0</li> <li>Dual Edge TPU - /dev/apex_0</li> </ul> <p>Press ADD then press APPLY. It will pull the image and run the docker run command and you should have an output similiar to this:</p> <p></p>","tags":["Coral"]},{"location":"faq/coral.html#codeprojectai-coral-module-installation","title":"CodeProject.AI Coral Module Installation","text":"<p>Here, you want to stop all ObjectDetection modules, uninstall the Coral module, and re-install Coral module until it works, which you confirm by watching in the System Log tab. If there are any \"pip\" error messages then try and reinstall until sucessful. </p> <p>The PIP errors will look something like this:</p> <p></p> <p>Turn off all Object Detection Modules</p> <p></p> <p>Uninstall the Coral module</p> <p></p> <p>Go back to Install Modules and re-install the Coral module. Make sure to watch the server log to see if it worked without any PIP errors.</p> <p>This is what a bad install looks like</p> <p></p> <p>And this is what a good install looks like</p> <p></p> <p>Once you acheive a good install, head back to the Status tab and enable ObjectDetection(Coral)</p> <p>Now you should see this \"Edge TPU detected\" in the log and at the bottom \"CPU\" should have changed to \"GPU(TPU)\"</p> <p></p>","tags":["Coral"]},{"location":"faq/dev-environment.html","title":"Build and Debug the Code","text":"","tags":["dev"]},{"location":"faq/dev-environment.html#command-not-found-during-development-setup-on-linuxmacos","title":"'command not found' during development setup on Linux/macOS","text":"<p>When running the setup script (or any script, for that matter) on Linux or macOS, you see an error of the form:</p> Text Only<pre><code>: No such file or directory 1: #1/bin/bash\n: No such file or directory 10: ./utils.sh\nsetup.sh: line 11: $'\\r': command not found\n</code></pre> <p>This indicates that the .sh script file has been saved with Windows-style CRLF line endings instead of Linux/Unix style LF line endings.</p> <p>Open the script in Visual Studio or Visual Studio Code and at the bottom right of the editor window you will see a line ending hint</p> <p></p> <p>Click that and choose 'LF' to correct the line endings and re-run the script.</p>","tags":["dev"]},{"location":"faq/dev-environment.html#models-not-found","title":"Models not found","text":"<p>When building you see:</p> Text Only<pre><code>error MSB3030: Could not copy the file \"&lt;path&gt;\\\\ObjectDetectionNet\\\\assets\\\\yolov5m.onnx\"\n  because it was not found.\n</code></pre> <p>Ensure you've run the development setup scripts before attempting to build</p>","tags":["dev"]},{"location":"faq/dev-environment.html#port-already-in-use","title":"Port already in use","text":"<p>If you see: <pre><code>Unable to start Kestrel.\nSystem.IO.IOException: Failed to bind to address http://127.0.0.1:5000: address \n   already in use.\n</code></pre> Either you have CodeProject.AI already running, or another application is using  port 5000.</p> <p>Our first suggestion is to no longer use port 5000. It's a reserved port, though not all operating systems are actively using it. We prefer port 32168 since it's easy to remember and well out of harm's way of other used ports.</p> <p>You can change the external port that CodeProject.AI uses by editing the  appsettings.json file and changing the value of the <code>CPAI_PORT</code>  variable. In the demo app there is a Port setting you will need to edit to match  the new port.</p> <p>Failing that, shut down any application using port 5000 (including any installed version of CodeProject.AI Server if you're trying to run in Development mode in Visual Studio)..</p>","tags":["dev"]},{"location":"faq/dev-environment.html#cant-find-custom-models","title":"Can't find custom models","text":"<p>When CodeProject.AI Server is installed it will comes with multiple object detection modules. All generally work the same, with the differences being in the languages and platforms supported as well as the models that are used.</p> <ol> <li>Ensure Object Object detection is enabled (it is by default)</li> <li> <p>Use the provided custom models, or </p> <p>a) add your own models to the module's custom model folder  (eg <code>C:\\Program Files\\CodeProject\\AI\\modules\\ObjectDetectionYolo\\custom-models</code> for the Python YOLO detector, or <code>C:\\Program Files\\CodeProject\\AI\\modules\\ObjectDetectionNet\\custom-models</code> for the .NET Object detector, just to name two modules) </p> <p>b) specify a directory that will contain the models (See the Docker guide)</p> </li> </ol> <p>To specify a different folder to use for custom models, you can</p> <ol> <li>Set the <code>--Modules:ObjectDetectionYolo:EnvironmentVariables:CUSTOM_MODELS_DIR</code> parameter when launching, or</li> <li>Set the <code>Modules:ObjectDetectionYolo:EnvironmentVariables:CUSTOM_MODELS_DIR</code> environment variable, or</li> <li>Set the <code>CUSTOM_MODELS_DIR</code> value in the modulesettings.json file in the ObjectDetectionYolo folder, or</li> <li>Set the global override (to be deprecated!) variable <code>MODELSTORE-DETECTION</code> to point to your custom object folder, or</li> <li> <p>(For Docker) Map the folder containing your custom models (eg. <code>C:\\MyCustomModels</code>) to the Object     Detection's custom assets folder (<code>/app/modules/ObjectDetectionYolo/custom-models</code>). An   example would be:</p> Text Only<pre><code>docker run -p 32168:32168 --name CodeProject.AI-Server -d ^\n  --mount type=bind,source=C:\\ProgramData\\CodeProject\\AI\\docker\\data,target=/etc/codeproject/ai ^\n  --mount type=bind,source=C:\\MyCustomModels,target=/app/modules/ObjectDetectionYolo/custom-models,readonly \n    codeproject/ai-server\n</code></pre> <p>This mounts the <code>C:\\MyCustomModels</code> directory on my local system and maps it to the /app/modules/ObjectDetectionYolo/custom-models folder in the Docker container. Now, when CodeProject.AI Server is looking for the list of custom models, it will look in <code>C:\\MyCustomModels</code> rather than <code>/app/modules/ObjectDetectionYolo/custom-models</code></p> <p>See the API Reference - CodeProject.AI Server</p> </li> </ol>","tags":["dev"]},{"location":"faq/dev-environment.html#server-startup-failed","title":"Server startup failed","text":"Text Only<pre><code>System.ComponentModel.Win32Exception: The system cannot find the file specified.\n   at System.Diagnostics.Process.StartWithCreateProcess(ProcessStartInfo startInfo)\n</code></pre> <p>Ensure you've run the development setup scripts before attempting to start the server.</p>","tags":["dev"]},{"location":"faq/dev-environment.html#background-remover-doesnt-run-vs-debug-gpu","title":"Background Remover doesn't run VS Debug GPU","text":"<p>Having a path to your development solution directory that is too long will prevent some modules from working due to the 256 char limit on file names in Windows. If you see something like the following error:</p> Text Only<pre><code>rembg_adapter.py:   File \"C:\\Users\\matth\\source\\repos\\codeproject\\CodeProject.AI-Server-Private\\src\\modules\\BackgroundRemover\\bin\\windows\\python39\\venv\\lib\\site-packages\\numba\\core\\caching.py\", line 662, in _save_overload\nrembg_adapter.py: FileNotFoundError: [Errno 2] No such file or directory\n</code></pre> <p>Simply move the directory with the repo in it to a shorter directory, like C:\\Dev, and run the Clean and Setup again. This is required as the Python virtual environment has a hard link to the old directory structure.</p>","tags":["dev"]},{"location":"faq/docker.html","title":"Using Docker","text":"","tags":["Docker"]},{"location":"faq/docker.html#inference-may-randomly-crash-if-running-docker-in-windows-under-wsl2","title":"Inference may randomly crash if running Docker in Windows under WSL2.","text":"<p>When Docker is installed on Windows it will, by default, use WSL2 if available. WSL2 will only use a max of 50% of available memory which isn't always enough. To solve this you can create a .wslconfig file to change this:</p> .wslconfig<pre><code># Place this file into /users/&lt;username&gt; \n\n# Settings apply across all Linux distros running on WSL 2\n[wsl2]\n\n# Limits VM memory to use no more than 12 GB, this can be set as whole numbers \n# using GB or MB. The default is 50% of available RAM and 8GB isn't (currently) \n# enough for CodeProject AI Server GPU\nmemory=12GB \n\n# Sets amount of swap storage space to 8GB, default is 25% of available RAM\nswap=8GB\n</code></pre>","tags":["Docker"]},{"location":"faq/docker.html#tf-lite-install-hangs-on-docker","title":"TF-Lite install hangs on Docker","text":"<p>If you're running Docker on a Linux system, and if you see the following error:</p> Text Only<pre><code>objectdetection_tflite_adapter.py: ModuleNotFoundError: No module named 'cv2'\nModule ObjectDetectionTFLite has shutdown\n</code></pre> <p>This is a timeout. As of CodeProject.AI Server 2.1.9, we've added the ability to adjust the <code>ModuleInstallTimeout</code> value in appsettings.json. In order to edit appsettings.json you need to update the file from the command line.</p> Bash<pre><code>docker ps -a\n</code></pre> <p>First, you need the Container ID for your CodeProject.AI Server Docker container. From the command line, type, <code>docker ps -a</code>.</p> <p></p> <p>The Container ID is under the CONTAINER ID column. For me this is <code>fbcdef25436d</code>. Now that we have the Container ID, we need to access the container from the command line. To do this we input <code>docker exec -u 0 -it containerID bash</code>.</p> <p>Next we need to install a text editor to edit the appsettings.json file. Before we do that, we have to type <code>apt-get update</code> to make sure we have the most recent packages. I'm installing nano, but you could install vim if you want, To install nano, type <code>apt-get install nano</code>.</p> <p>Once nano finishes installing, you can use it to edit the appsettings.json file. Type <code>nano appsettings.json</code>. It's hard to see where <code>ModuleInstallTimeout</code> is, so hit Ctrl and \"W\" to search the files, and type \"where-is-moduleinstalltimeout\".</p> <p></p> <p>Once <code>ModuleInstallTimeout</code> is located, you can edit the length of time. The default is 20 minutes, which should be plenty, but if it's timing out due to internet issues, or just being slower on a Raspberry Pi, you might want to double it. Once you've edited the number, hit Ctrl + X, and then \"Y\" to indicate you would like to Save the modified buffer, then hit Enter.</p> <p>You'll have to restart the Pi (if you're using a Pi) or restart your container for the change to take effect.</p>","tags":["Docker"]},{"location":"faq/docker.html#error-response-from-daemon-invalid-mode-etccodeprojectai","title":"Error response from daemon: invalid mode: /etc/codeproject/ai.","text":"<p>If you're in Windows, ensure you're running Docker from Windows Powershell or terminal, and not from a WSL terminal.</p>","tags":["Docker"]},{"location":"faq/docker.html#modules-fail-to-run-when-installed-in-docker-with-mappings","title":"Modules fail to run when installed in Docker with mappings","text":"<p>When installing CodeProject.AI Server in a Docker container and mapping your modules to a specific folder, you may get an error like:</p> Text Only<pre><code>ModuleNotFoundError: No module named 'aiohttp'\n</code></pre> <p>This is a timeout. As of CodeProject.AI Server 2.1.9, we've added the ability to adjust the <code>ModuleInstallTimeout</code> value in appsettings.json. In order to edit appsettings.json, go to Visual Studio Code. In the Extensions tab, search for \"Docker\" and install the Docker extension to Visual Studio Code if you haven't alraedy.</p> <p>Then, from the Docker icon on the left-hand side of Visual Studio Code, look under Containers -&gt; app -&gt; server, then find appsettings.json. Select the file, then click the Open icon. Then navigate to <code>ModuleInstallTimeout</code>.</p> JSON<pre><code>    // The time allowed for a module to be installed. 20 mins should be plenty, but for a Raspberry\n    // Pi, or slow internet, it will need longer.\n    \"ModuleInstallTimeout\": \"00:20:00\",\n</code></pre> <p>The default is 20 minutes, which should be plenty, but if it's timing out due to internet issues, you might want to double it. Once you've edited the number, hit Ctrl + S to Save.</p>","tags":["Docker"]},{"location":"faq/docker.html#you-have-an-nvidia-card-but-gpucuda-utilization-isnt-being-reported-in-the-codeprojectai-server-dashboard-when-running-under-docker","title":"You have an NVidia card but GPU/CUDA utilization isn't being reported in the CodeProject.AI Server dashboard when running under Docker","text":"<p>Please ensure you start the Docker image with the <code>--gpus all</code> parameter:</p> Text Only<pre><code>docker run -d -p 32168:32168 --gpus all codeproject/ai-server:cuda11_7\n</code></pre>","tags":["Docker"]},{"location":"faq/docker.html#docker-with-volumes-re-installing-a-pre-installed-module-and-starting-a-new-container-result-in-module-in-both-module-dirs","title":"Docker with volumes: Re-installing a pre-installed module and starting a new container result in module in both module dirs","text":"<p>If you create a Docker container with a volume mapped to the /app/moduels directory  the changes to this directory are persisted in the volume across container restarts and updates. So, when you uninstall a pre-installed module, it is removed from the pre_installed directory. Re-Installing it adds it to the modules directory.</p> <p>For example, if you use Docker Compose with the following docker-compose.yml</p> <p>YAML<pre><code>version: '3'\nservices:\n  CodeProjectAI:\n    image: codeproject/ai-server\n    container_name: \"CodeProjectAI\"\n    ports:\n      - \"32168:32168\"\n    environment:\n      - TZ=America/Toronto\n    volumes:\n      - cpaidata:/etc/codeproject/ai\n      - cpaimodules:/app/modules\n    restart: unless-stopped\n\nvolumes:\n  cpaidata:\n  cpaimodules:\n</code></pre> </p> <p>Then un-install one of the pre-installed modules, like Face Processing, or Object Detection (YOLOv5.6.2), then re-install it, then delete that container and restart it from the same image, or upgrade CodeProject.AI Server the pre-installed module will move to the modules directory.</p> <p>Ultimately, while messy, this is not a problem if the newly installed module is compatible with the docker container's service.</p>","tags":["Docker"]},{"location":"faq/gpu.html","title":"GPUs, TPUs, NPUs","text":"","tags":["GPU"]},{"location":"faq/gpu.html#gpu-is-not-being-used","title":"GPU is not being used","text":"<p>Please ensure you have the NVIDIA CUDA drivers installed:</p> <ol> <li>Install the\u00a0CUDA 11.7 Drivers</li> <li>Install the CUDA Toolkit 11.7.</li> <li>Download and run our cuDNN install script.</li> </ol>","tags":["GPU"]},{"location":"faq/gpu.html#inference-randomly-fails","title":"Inference randomly fails","text":"<p>Loading AI models can use a lot of memory, so if you have a modest amount of RAM on your GPU, or on your system as a whole, you have a few options</p> <ol> <li>Disable the modules you don't need. The dashboard (http://localhost:32168) allows you to disable modules individually</li> <li>If you are using a GPU, disable GPU for those modules that don't necessarily need the power of the GPU.</li> <li>If you are using a module that offers smaller models (eg Object Detector (YOLO)) then try selecting a smaller model size via the dashboard</li> </ol> <p>Some modules, especially Face comparison, may fail if there is not enough memory. We're working on meaking the system leaner and meaner.</p>","tags":["GPU"]},{"location":"faq/gpu.html#you-have-an-nvidia-card-but-gpucuda-utilization-isnt-being-reported-in-the-codeprojectai-server-dashboard-when-running-under-docker","title":"You have an NVIDIA card but GPU/CUDA utilization isn't being reported in the CodeProject.AI Server dashboard when running under Docker","text":"<p>Please ensure you start the Docker image with the <code>--gpus all</code> parameter:</p> Terminal<pre><code>docker run -d -p 32168:32168 --gpus all codeproject/ai-server:cuda11_7\n</code></pre>","tags":["GPU"]},{"location":"faq/gpu.html#how-to-downgrade-cuda-to-118","title":"How to downgrade CUDA to 11.8","text":"<p>If you're on Windows and having issues with your GPU not starting, but your GPU supports CUDA and you have CUDA installed, make sure you are running the correct CUDA version. </p> <p>Open the command prompt and type</p> Terminal<pre><code>run nvcc --version\n</code></pre> <p>On Windows, we recommend running CUDA 11.8. If you are not running CUDA 11.8, un-install your version of CUDA then download and install CUDA 11.8: https://developer.download.nvidia.com/compute/cuda/11.8.0/local_installers/cuda_11.8.0_522.06_windows.exe</p>","tags":["GPU"]},{"location":"faq/gpu.html#cuda-not-available","title":"CUDA not available","text":"<p>If have an NVIDIA card and you're looking in the CodeProject.AI Server logs and see:</p> Text Only<pre><code>CUDA Present...False\n</code></pre> <p>You can check to see what version of CUDA you're running by opening a terminal and running:</p> Terminal<pre><code>nvidia-smi\n</code></pre> <p>then:</p> Terminal<pre><code>run nvcc --version\n</code></pre> <p>If you see <code>nvcc --version</code> is unknown, you may not have CUDA installed. You can confirm this by going Windows Settings, then Apps &amp; features and searching for \"CUDA\" and see what comes up. If CUDA is installed, you will see the following:</p> <p></p>","tags":["GPU"]},{"location":"faq/home-assistant.html","title":"Home Assistant Integration","text":"","tags":["Home-Assistant"]},{"location":"faq/home-assistant.html#where-is-my-home-assistant-container-config-folder","title":"Where is my Home Assistant Container config folder?","text":"<p>If you're setting up Home Assistant Container, the Docker command you need to run in order to establish a config folder for Home Assistant is specific, in addition to being specific to your operating system. You need the config folder in order to be able to edit the Home Assistant configuration.yaml file, which is required for setting up a lot of integrations in Home Assistant.</p> <p>The first thing we need to do is create a configuration folder for Home Assistant.</p> <p></p> <p>The configuration folder will be on whatever drive you run the Docker pull on, which in my case, is the _C:_ drive. So open Windows Explorer, go to the _C:_ drive, right-click in the folder area, and go to New -&gt; Folder. Input the folder name you want. This examples calls the folder  \"haconfig\".</p> <p>Now go to the Start button for Windows and type \"CMD\" and open Command Prompt. Then type</p> Terminal<pre><code>docker run -d --name homeassistant --privileged --restart=unless-stopped -e TZ=America/Edmonton -v c:\\haconfig:/config -p 8123:8123 homeassistant/home-assistant:latest\n</code></pre> <p>where <code>--name homeassistant</code> is the name of your container, <code>TZ=America/Edmonton</code> is the time zone for Home Assistant (just enter your own time zone here), and <code>-v c:\\haconfig:/config</code> is the path of the config folder (if you created a folder in C:\\Users\\Joseph\\haconfig this would be <code>-v C:\\Users\\Joseph\\haconfig:/config)</code>.</p>","tags":["Home-Assistant"]},{"location":"faq/home-assistant.html#balena-etcher-error-attention-something-went-wrong","title":"balena Etcher Error - Attention Something Went Wrong","text":"<p>If you're using balena Etcher to set up Home Assistant OS, you might be trying to use Flash from URL. For most people, this should work. But some might get an error:</p> Text Only<pre><code>Attention. Something went wrong. if it is a compressed image, please check that the archive is no corrupted. Command failed: cmd /c \"C:\\Users\\AppData\\Local\\Temp\\etcher\\balena-etcher-electron-78afb6aee66.cmd\"\n</code></pre> <p>Try downloading the first image instead of pointing to the Git URL. </p> <p></p> <p>Take the URL https://github.com/home-assistant/operating-system/releases/download/10.0/haos_rpi4-64-10.0.img.xz and put it into a browser window. It automatically downloads. Go back to balena Etcher. Only this time, select Flash from a file instead of from a URL. Select the haos_rpi4-64-10.0.img.xz that downloads automatically.</p>","tags":["Home-Assistant"]},{"location":"faq/home-assistant.html#socketgaierror-errno-2-name-does-not-resolve","title":"socket.gaierror: [Errno -2] Name does not resolve","text":"<p>If you're setting up CodeProject.AI Server on a different machine than Home Assistant (for example, on Windows), when you call CodeProject.AI Server, it might not be able to be reached. Here is the error in the Home Assistant logs, home-assistant.log, which can be found from File Editor in the left navigation pane by clicking the Browse Filesystem button.</p> <p>Text Only<pre><code> File \"/usr/local/lib/python3.10/site-packages/requests/adapters.py\", line 565, in send\n    raise ConnectionError(e, request=request)\nrequests.exceptions.ConnectionError: HTTPConnectionPool(host='http', port=80): Max retries exceeded with url: //10.0.0:32168/v1/vision/detection (Caused by NewConnectionError('&lt;urllib3.connection.HTTPConnection object at 0x7f9fd21990&gt;: Failed to establish a new connection: [Errno -2] Name does not resolve'))\n</code></pre> Despite being on the same network, you need to open port 32168 so it can communicate with the Pi. Here's how to you do that on Windows 10.</p> <p>From the Windows Start button, select Settings. Then select Update and Security. Then select Windows Security from the left navigation pane and click the Open Windows Security button. Then select Firewall and nework protection from the left navigation pane, then Advanced Settings. Click Yes to allow the app to make changes. Now click Inbound Rules.</p> <p></p> <p>From here, click New Rule....</p> <p></p> <p>We want to create a rule for port 32168, so select Port then hit Next.</p> <p></p> <p>Make sure TCP is selected, then in Specific local ports type \"32168\" and hit Next.</p> <p></p> <p>The default selection is Allow the connection which is correct. Hit Next.</p> <p></p> <p>Check all these boxes and hit Next.</p> <p></p> <p>You can name the rule whatever you want. Probably best to name it something you'll recognize later. Hit Finish. Now CodeProject.AI Server can be called from Home Assistant OS on my Raspberry Pi 4.</p>","tags":["Home-Assistant"]},{"location":"faq/jetson.html","title":"Jetson Nano Dev Kit","text":"","tags":["Jetson"]},{"location":"faq/jetson.html#dpkg-error-processing-package-nvidia-l4t-bootloader-configure","title":"dpkg: error processing package nvidia-l4t-bootloader (--configure)","text":"<p>You may see this error, or something similar, while trying to install VS Code (or anything else) on the Jetson</p> Text Only<pre><code>Setting up nvidia-l4t-bootloader (32.7.1-20220219090432) ...\n3448-300---1--jetson-nano-qspi-sd-mmcblk0p1\nStarting bootloader post-install procedure.\nERROR. Procedure for bootloader update FAILED.\nCannot install package. Exiting...\ndpkg: error processing package nvidia-l4t-bootloader (--configure):\n installed nvidia-l4t-bootloader package post-installation script subprocess returned error exit status 1\nProcessing triggers for libc-bin (2.27-3ubuntu1.3) ...\nErrors were encountered while processing:\n nvidia-l4t-bootloader\nE: Sub-process /usr/bin/dpkg returned an error code (1)\n</code></pre> <p>To resolve, try the following (with thanks to Carlos on the NIVIDIA forums).The issue is related to the information that dpkg saves. To do a hacky fix we'll move the dpkg info to a backup, update and reinstall dpkg packages, and then copy back the info we backed up. This will correct corrupted the information that had been stored</p> <ol> <li>Backup <code>/var/lib/dpkg/info</code></li> </ol> bash<pre><code>sudo mv /var/lib/dpkg/info/ /var/lib/dpkg/backup/\nsudo mkdir /var/lib/dpkg/info/\n</code></pre> <ol> <li>Update the repos and force install</li> </ol> bash<pre><code>sudo apt-get update\nsudo apt-get -f install\n</code></pre> <ol> <li>Copy the corrected dpkg/info files over our old backup</li> </ol> bash<pre><code>sudo mv /var/lib/dpkg/info/* /var/lib/dpkg/backup/\n</code></pre> <ol> <li>Remove the new info and copy our backup (with corrected info) back into place</li> </ol> bash<pre><code>sudo rm -rf /var/lib/dpkg/info\nsudo mv /var/lib/dpkg/backup/ /var/lib/dpkg/info/\n</code></pre> <p>Things should now work properly.</p>","tags":["Jetson"]},{"location":"faq/mesh-faq.html","title":"Mesh","text":"","tags":["Mesh"]},{"location":"faq/mesh-faq.html#windows-doesnt-see-both-pcs","title":"Windows doesn't see both PCs","text":"<p>If you're trying to use CodeProject.AI Server Mesh with a Windows 10 machine, you may encounter networking issues, where CodeProject.AI Server Mesh doesn't pick up the Window 10 machine. There is a networking fix for this, whereby you change the network priority of the Windows connection. </p> <p>To do this, open up Control Panel, select Network and Sharing Center &gt; Change Adapter Settings, then right-click on whatever adapter you want to set as your priority conncetion, and select Properties. Select Internet Protocol Version 4 (TCP/IPv4), then hit Properties &gt; Advanced, uncheck Automatic metric, then within the Interface metric field put in \"10\", hit OK, OK, then restart your computuer.</p> <p>This comes from the video How to Change Network Priority of Connection on Windows 11 if you prefer a visual aid.</p>","tags":["Mesh"]},{"location":"faq/raspberry-pi.html","title":"Raspberry Pi","text":"","tags":["Raspberry-Pi"]},{"location":"faq/raspberry-pi.html#you-must-install-net-to-run-this-application","title":"You must install .NET to run this application","text":"<p>You may have already installed the dev environment on your Pi, but the latest version of the .NET runtime or SDK may have been updated and you will need to manually update .NET.</p> <p>Go to /src/Scripts in a terminal on your Pi and run</p> bash<pre><code>sudo bash dotnet-install-rpi.sh\n</code></pre>","tags":["Raspberry-Pi"]},{"location":"faq/ubuntu-installer.html","title":"Ubuntu / WSL","text":"","tags":["Ubuntu"]},{"location":"faq/ubuntu-installer.html#error-saying-you-need-to-install-net","title":"Error saying you need to install .NET","text":"<p>In Ubuntu or WSL, if you have .NET 7 installed and you try and build, there may be an error saying you need to install .NET.</p> <p>The easiest fix is to simply reinstall .NET by removing .NET, delete the PMC repository from APT by deleting the repo .list file, then reinstalling the .NET SDK</p> bash<pre><code>sudo apt remove 'dotnet*'\nsudo apt remove 'aspnetcore*'\nsudo rm /etc/apt/sources.list.d/microsoft-prod.list \nsudo apt update\nsudo apt install dotnet-sdk-7.0\n</code></pre>","tags":["Ubuntu"]},{"location":"faq/virtual-machine.html","title":"Proxmox","text":"","tags":["Docker"]},{"location":"faq/virtual-machine.html#alpr-installation-issue-on-proxmox","title":"ALPR installation issue on Proxmox","text":"<p>If you're installing CodeProject.AI Server on Windows running in a VM on a Proxmox host, and you see the following when installing the ALPR module you get the error</p> Text Only<pre><code>ALPR_adapter.py: Error: Your machine doesn't support AVX, but the\ninstalled PaddlePaddle is avx core, you should reinstall paddlepaddle with no-avx core.\n</code></pre> <p>You may need to change the CPU type in Proxmox to \"Host\" which to expose the AVX instruction set. To do so, click on the VM and shut it down. Then click Hardware - &gt; Processors -&gt; Edit - &gt; and set type to Host.</p> <p>The other option is to create specific CPU models and then add the appropriate flags you desire to those models as documented in the Proxmox manual here: Manual: cpu-models.conf - Proxmox VE.</p>","tags":["Docker"]},{"location":"faq/windows-installer.html","title":"Windows Installer","text":"","tags":["Windows"]},{"location":"faq/windows-installer.html#cant-find-custom-models","title":"Can't find custom models","text":"<p>When CodeProject.AI Server is installed it will comes with two different object detection modules. Both modules work the same, with the difference that one is a Python implementation that supports CUDA GPUs, and the other is a .NET implementation that supports embedded Intel GPUs. Each come with the same set of custom models that can be used. For custom object detection you need to:</p> <ol> <li>Ensure Object Object detection is enabled (it is by default)</li> <li>Use the provided custom models, or a. add your own models to the standard custom model folder (C:\\Program Files\\CodeProject\\AI\\modules\\ObjectDetectionYolo\\custom-models or C:\\Program Files\\CodeProject\\AI\\modules\\ObjectDetectionNet\\custom-models) if using a Windows install, or b. specify a directory that will contain the models (handy for Docker)</li> </ol> <p>To specify a different folder to use for custom models, you can</p> <ol> <li>Set the <code>--Modules:ObjectDetectionYolo:EnvironmentVariables:CUSTOM_MODELS_DIR</code> parameter when launching, or</li> <li>Set the <code>Modules:ObjectDetectionYolo:EnvironmentVariables:CUSTOM_MODELS_DIR</code> environment variable, or</li> <li>Set the <code>CUSTOM_MODELS_DIR</code> value in the modulesettings.json file in the ObjectDetectionYolo folder, or</li> <li>Set the global override (to be deprecated!) variable <code>MODELSTORE-DETECTION</code> to point to your custom object folder, or</li> <li>(For Docker) Map the folder containing your custom models (eg. C:\\MyCustomModels) to the Object Detection's custom assets folder (/app/modules/ObjectDetectionYolo/custom-models). An example would be:</li> </ol> Terminal<pre><code>docker run -p 32168:32168 --name CodeProject.AI-Server -d ^\n  --mount type=bind,source=C:\\ProgramData\\CodeProject\\AI\\docker\\data,target=/etc/codeproject/ai ^\n  --mount type=bind,source=C:\\MyCustomModels,target=/app/modules/ObjectDetectionYolo/custom-models,readonly \n    codeproject/ai-server\n</code></pre> Text Only<pre><code>This mounts the *C:\\MyCustomModels* directory on my local system and maps it to the */app/modules/ObjectDetectionYolo/custom-models* folder in the Docker container. Now, when CodeProject.AI Server is looking for the list of custom models, it will look in *C:\\MyCustomModels* rather than */app/modules/ObjectDetectionYolo/custom-models*\n\nSee the [API Reference - CodeProject.AI Server](https://codeproject.github.io/codeproject.ai/api/api_reference.html#custom-object-detector)\n</code></pre>","tags":["Windows"]},{"location":"faq/windows-installer.html#port-already-in-use","title":"Port already in use","text":"<p>If you see:</p> Text Only<pre><code>Unable to start Kestrel.\nSystem.IO.IOException: Failed to bind to address http://127.0.0.1:5000: address \n   already in use.\n</code></pre> <p>Either you have CodeProject.AI already running, or another application is using port 5000.</p> <p>Our first suggestion is to no longer use port 5000. It's a reserved port, though not all operating systems are actively using it. We prefer port 32168 since it's easy to remember and well out of harm's way of other used ports.</p> <p>You can change the external port that CodeProject.AI uses by editing the appsettings.json file and changing the value of the <code>CPAI_PORT</code> variable. In the demo app there is a Port setting you will need to edit to match the new port.</p> <p>Failing that, shut down any application using port 5000 (including any installed version of CodeProject.AI Server if you're trying to run in Development mode in Visual Studio).</p>","tags":["Windows"]},{"location":"faq/windows-installer.html#gpu-is-not-being-used","title":"GPU is not being used","text":"<p>Please ensure you have the NVidia CUDA drivers installed:</p> <ol> <li>Install the\u00a0CUDA 11.7 Drivers</li> <li>Install the CUDA Toolkit 11.7.</li> <li>Download and run our cuDNN install script.</li> </ol>","tags":["Windows"]},{"location":"faq/windows-installer.html#codeprojectai-server-stops-randomly-often-at-the-same-time-of-the-day","title":"CodeProject.AI Server stops randomly (often at the same time of the day)","text":"<p>If you're finding CodeProject.AI Server is randomly going offline once (or sometimes twice) a day, or if you find certain modules like the License Plate Reader are shutting down once a day, there may be conflict with backup software. In order to determine if this is the case, manually run your backup and see if CodeProject.AI Server goes offline. Interestingly, the service is still running, but shows as \"Offline.\" It may be related to specific backup software, like Acronis.</p> <p>For the Licence Plate Reader shutting down, and you're using the GPU for CodeProject.AI Server and running an AMD GPU, try enabling Object Detection (YOLOv5.NET) and disable the Object Detection (YOLOv5.6.2). The latter only supports newer NVidia GPUs, while the former uses DirectX on Windows or WSL to access the GPU so it will support your AMD GPU if you have the DirectX driver installed.</p>","tags":["Windows"]},{"location":"faq/windows-installer.html#oserror-winerror-1455-the-paging-file-is-too-small-for-this-operation-to-complete","title":"OSError: [WinError 1455] The paging file is too small for this operation to complete","text":"<p>In a nutshell, your paging file is too small. If you've changed (reduced) your paging file settings to save disk space you may want to increase the size to provide more headroom for the system. Failing that it could simply be the case you have too little RAM installed. We'd recommend at least 8Gb for the current versions.</p>","tags":["Windows"]},{"location":"faq/windows-installer.html#server-fails-to-start-with-systemmanagementmanagementexception-invalid-class-error","title":"Server fails to start with 'System.Management.ManagementException: Invalid class' error","text":"<p>The error you will see is</p> Text Only<pre><code>Description: The process was terminated due to an unhandled exception.\nException Info: System.TypeInitializationException: The type initializer for\n'CodeProject.AI.SDK.Common.SystemInfo' threw an exception.\n---&gt; System.Management.ManagementException: Invalid class\n</code></pre> <p>This is due to the WMI repository being corrupt. To repair the corrupted WMI do the following:</p> <ul> <li>Open an Administrator Command or PowerShell window</li> <li>run <code>winmgmt /verifyrepository</code>. If the repository has an issue, it will respond \"repository is not consistent\".</li> <li>if the respository is inconsistent it needs to be repaired. run <code>winmmgmt /salvagerepository</code></li> <li>run <code>winmgmt /verifyrepository</code> again.</li> <li>if it still reports the \"respository in not consistent\" then you might have to take drastic action and reset the repository. See the links below for details on how to do this, and the risks involved.</li> </ul>","tags":["Windows"]},{"location":"faq/windows-installer.html#see","title":"See","text":"<ul> <li>WMI: Repository Corruption, or Not?.</li> <li>Winmgmt</li> </ul>","tags":["Windows"]},{"location":"faq/windows-installer.html#module-fails-to-start","title":"Module fails to start","text":"<p>If you have a modest hardware setup then each module may require a little more time to start up before being able to proceed to loading the next module.</p> <p>In the modulesettings.json file in each module's older is the setting <code>PostStartPauseSecs</code>. This specifies a pause between loading the given module and loading the next. Set it to 3 to 5 seconds if you modules are failing to load properly.</p>","tags":["Windows"]},{"location":"faq/windows-installer.html#installation-of-faceprocessing-reports-a-urllib3-error-which-can-be-ignored","title":"Installation of FaceProcessing reports a urllib3 Error which can be ignored","text":"<p>While installing FaceProcessing, an error related to urllib3 is reported. This can be safetly ignored. It's something we're pondering how to address, but FaceProcessing should work just fine without it.</p> Text Only<pre><code>22:58:56:ObjectDetectionYolo: ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n22:58:56:ObjectDetectionYolo: botocore 1.29.142 requires urllib3&lt;1.27,&gt;=1.25.4, but you have urllib3 2.0.3 which is incompatible.\n22:58:56:ObjectDetectionYolo: google-auth 2.19.0 requires urllib3&lt;2.0, but you have urllib3 2.0.3 which is incompatible.\n</code></pre>","tags":["Windows"]},{"location":"faq/windows-installer.html#usbdk-installation-failing-for-objectdetectioncoral","title":"UsbDk installation failing for ObjectDetection(Coral)","text":"<p>If you're installing the UsbDk that's required for Coral USB, there is a known issue on Windows. The error will look something like: There is a problem with this Windows Installer package. A program run as part of the setup did not finish as expected. Contact your support personnel or package vendor.</p> <p>To solve this, unpack the MSI to some subdirectory, like this:</p> Terminal<pre><code>cd c:\\users\\user\\downloads\nmkdir 122\nmsiexec /a UsbDk_1.0.22_x64.msi /qb TARGETDIR=%cd%\\122\n</code></pre> <p>Open administrator command line. Go to the directory where the usbdkcontroller.exe is placed:</p> Terminal<pre><code>cd c:\\users\\user\\downloads\\122\\USB Runtime Library etc\nusbdkcontroller.exe -u\n</code></pre> <p>Reboot</p> Terminal<pre><code>usbdkcontroller.exe -i\n</code></pre>","tags":["Windows"]},{"location":"faq/windows-installer.html#windows-10-update-for-net-framework-kb5034275-kb5033918-kb5034466-kb5034582-affecting-powershell-cudnn","title":"Windows 10 Update for .NET Framework (KB5034275) (KB5033918) (KB5034466) (KB5034582) affecting PowerShell, cuDNN","text":"<p>If you recently made a Windows 10 update and are finding that any external program that needs to call anything PowerShell is not working, and the script for cuDNN is not running, it could be a recent Windows 10 update issue. Particularly these updates:</p> <p>January 9, 2024-KB5034275 Cumulative Update for .NET Framework 3.5, 4.8 and 4.8.1 for Windows 10 Version 22H2, (KB5033918) or (KB5034466), and January 23, 2024-KB5034582 Cumulative Update Preview for .NET Framework 3.5, 4.8 and 4.8.1 for Windows 10 Version 22H2.</p> <p>Upgrading beyond these, or possibly uninstalling the update should resolve the issue.</p>","tags":["Windows"]},{"location":"faq/windows-installer.html#object-detection-fails-unable-to-create-yolo-detector-for-model","title":"Object Detection fails: Unable to create YOLO detector for model","text":"<p>If you are attempting to use an Object Detection module and get</p> Text Only<pre><code>Object Detection (YOLO): Unable to create YOLO detector for model\n</code></pre> <p>Please go to C:\\Program Files\\CodeProject\\AI\\modules\\ObjectDetectionYolo\\assets. You should see a set of .pt files. If you do not, this means your installer failed.</p> <p>To fix this, you need to re-install the module. Go to the Install Modules tab on your CodeProject.AI Server dashboard. Find the Object Detection module you are using, then click Uninstall. Then, once availabile, click Install.</p>","tags":["Windows"]},{"location":"faq/windows-installer.html#errors-regarding-a-module-not-starting-a-package-not-found-something-failing-or-shutting-down","title":"Errors regarding a module not starting, a package not found, something failing or shutting down","text":"<p>If you get any of the following errors while using a particular module:</p> Text Only<pre><code>Unable to load the model\nError trying to start\nAn error occurred trying to start process\nModuleNotFoundError: No module named\n</code></pre> <p>Please go to C:\\Program Files\\CodeProject\\AI\\modules[Module in question] and look at install.log. This may point to the installation failing or something failing to be downloaded. If this is the case, you need to re-install the module. Go to the Install Modules tab on your CodeProject.AI Server dashboard. Find the module, then click Uninstall. Then, once availabile, click Install.</p>","tags":["Windows"]},{"location":"faq/windows-installer.html#modulenotfounderror-no-module-named-pil","title":"ModuleNotFoundError: No module named 'PIL'","text":"<p>If you see this error:</p> Text Only<pre><code>ModuleNotFoundError: No module named 'PIL'\n</code></pre> <p>This means the module that threw the error wasn\u2019t installed fully, most likely due to a timeout.</p> <p>If this is the case, you need to re-install the module. Go to the Install Modules tab on your CodeProject.AI Server dashboard. Find the module, then click Uninstall. Then, once availabile, click Install.</p>","tags":["Windows"]},{"location":"faq/windows-installer.html#paddlepaddle-install-issues-blocking-domains","title":"PaddlePaddle Install Issues / Blocking Domains","text":"<p>If you see this error:</p> Text Only<pre><code>ModuleNotFoundError: No module named 'paddle'\n</code></pre> <p>While not exclusive to PaddlePaddle, or the Optical Character Recognition module, and License Plate Reader module, an error like this could be indicative of domain blocking. PaddlePaddle, for instance, may come from Chinese domains and if you have Chinese IPs blocked, these modules won't be able to install.</p> <p>To resolve this, temporarily allow the IPs you're restricting, and re-install the module.</p>","tags":["Windows"]},{"location":"features/mesh.html","title":"The CodeProject.AI Server Mesh","text":"<p>One of the core issues we face when handling AI related tasks is computing power. Generally the bigger the better, but you can get away with a lower calibre machine if you aren't stressing it too much. But what happens when an unexpected load hits and your tiny machine starts to creak and groan? What you really want is a system to allow you to offload AI tasks to other servers if you have other servers sitting around with capacity to take on work.</p> <p>This is where Mesh comes into play.</p> <p></p> <p>Here we see two machines, each with CodeProject.AI Server installed, each with the new Mesh functionality enabled.</p> <p>Each server will handle its own requests, but is also aware of other servers in the mesh and will send requests to other servers if it thinks another server can do better. To determine whether another server can do better, each server will simply try out the other servers and see how they go. From then on the server receiving a request will either process the request itself, or pass it to another server if another server can do it faster.</p> <p>In the above image, server .145 processed two ALPR requests. The first it handled itself with a 77ms response. The second request it passed to the remote .90 server, which recorded a response time of 1621ms. The remote server will not get any further requests while it's response time is that high.</p> <p>After a (configurable) period of time, the recorded response time of remote servers will \"decay\". Older request timings are discarded and the \"effective\" response time heads back to 0</p> <p></p> <p>Notice on .90, the response time for ALPR is now at 0.</p> <p>Now imagine it's Black Friday and the carpark security goes nuts at 6AM. The current server gets a deluge of requests and can't keep up. The effective response time of the current server increases 10X to be nearly 800ms, and so it starts offloading requests to the other server.</p> <p>The other server has warmed up, and that 16 second inference is nowhere in sight. The remote server runs a decent GPU and its local response time, even under load, is around 80ms</p> <p></p> <p>What we now see in the Mesh summary tab for the current server is the remote server processing the majority of the AI operations with an effective response time of 154ms. On the remote server, we see that it reports an effective response of 84ms, but remember that the remote server's timing doesn't include the network roundtrip or work involved in repackaging the initial AI request.</p> <p>Mesh processing is purely optional and the dashboard makes it trivial to turn it on and off. There is zero configuration needed. All servers self-discover the mesh, register and unregister themselves from the mesh, and load balancing is built in. </p>","tags":["CodeProject.AI","Mesh"]},{"location":"features/mesh.html#setup","title":"Setup","text":"<p>There is no setup. It just works.</p>","tags":["CodeProject.AI","Mesh"]},{"location":"features/mesh.html#ok-what-about-settings","title":"OK, what about Settings.","text":"<p>The mesh settings are in the <code>appsettings.json</code> file in the server's root folder under the <code>MeshOptions</code> branch. The most basic is whether or not mesh is enabled, and this setting is exposed in the Mesh tab on the dashboard.</p>","tags":["CodeProject.AI","Mesh"]},{"location":"features/mesh.html#enablebroadcasting-and-monitornetwork","title":"EnableBroadcasting and MonitorNetwork","text":"<p>Other settings include <code>EnableBroadcasting</code> which sets whether or not the local server will broadcast its availability to the local network, and <code>MonitorNetwork</code> which sets whether the server will spend CPU cycles watching for other servers. Mesh functionality does not rely on monitoring or broadcasting, so these flags allow a server to see what's happening on the network without broadcasting  potentially sensitive information.</p>","tags":["CodeProject.AI","Mesh"]},{"location":"features/mesh.html#acceptforwardedrequests-and-allowrequestforwarding","title":"AcceptForwardedRequests and AllowRequestForwarding","text":"<p>These two settings determine whether or not a server will accept requests from another server in the mesh, and whether it will send requests to other servers in the mesh. These settings allow a server to provide support for other servers without burdening other servers, or conversely, to use the mesh for support  without allowing other servers to add load to itself.</p> <p>An example would be a Raspberry Pi and a Macbook Pro. The Pi might have  <code>AcceptForwardedRequests</code> false and <code>AllowRequestForwarding</code> true, and the mac could have <code>AcceptForwardedRequests</code> true and <code>AllowRequestForwarding</code> as false. This provides the Pi with help if needed, without worrying about the mac sending  unwanted load.</p>","tags":["CodeProject.AI","Mesh"]},{"location":"features/mesh.html#routes-and-modules","title":"Routes and Modules","text":"<p>Each server in a mesh may have different modules installed, or indeed have different models installed in a given module. Servers may also be running different hardware, operating systems, or be attached to different peripherals such as external AI accelerators such as the Coral.</p> <p>Some modules also persist data locally as part of their process, and so calls to different servers may result in different responses. Face recognition, for instance, stores registered faces in a local database, and so a call to one server to register a face, followed by a call to a different server to recognise a face may fail if the other server does not also have that face registered.</p> <p>With this is mind every route in every module can be marked as <code>MeshEnabled</code>. If true, the server will expose this route (eg <code>/v1/vision/face/detect</code>) to the mesh. If <code>MeshEnabled</code> is false (eg <code>/v1/vision/face/register</code>) then that route will  not be available to other servers.</p> <p><code>MeshEnabled</code> is set in the <code>modulesettings.json</code> files for each module, under the <code>Routes</code> branch in that file.</p>","tags":["CodeProject.AI","Mesh"]},{"location":"features/mesh.html#docker","title":"Docker","text":"","tags":["CodeProject.AI","Mesh"]},{"location":"features/mesh.html#known-servers-working-around-network-issues","title":"Known Servers: working around network issues","text":"<p>CodeProject.AI Server works within a Docker container, with the caveat that the broadcast that the server makes to announce its participation in a mesh may not make it through the network layers and subnet to other servers. This means that your Docker instance of CodeProject.AI server may be invisible to other servers looking for mesh participants.</p> <p>To work around this, edit the appsettings.json file in the root directory of CodeProject.AI server for each server that wishes to use the Docker instance, and edit the <code>MeshOptions.KnownMeshHostnames</code> collection.</p> <p>An example is we have a PC sitting in our dusty garage named PC-GARAGE. It's running CodeProject.AI Server in Docker. We have another server that would like to use PC-GARAGE, so on that server we edit the <code>appsettings.json</code> file to include this</p> <p>JSON<pre><code>  \"MeshOptions\": {\n    \"Enable\": true,\n    \"KnownMeshHostnames\": [ \"PC-GARAGE\" ]\n  },\n</code></pre> Our server will now regularly ping PC-GARAGE directly, instead of waiting for a UDP broadcast packet that will never arrive. Pinging PC-GARAGE will return the same information as PC-GARAGE is (fruitlessly) broadcasting and will enable our machine to start offloading requests to PC-GARAGE.</p>","tags":["CodeProject.AI","Mesh"]},{"location":"features/mesh.html#ensuring-udp-traffic-is-enabled","title":"Ensuring UDP traffic is enabled","text":"<p>To enable the broadcasting and the monitoring of the broadcasting, you will need to ensure UDP port 32168 is exposed by the Docker container.</p> <p>If you are launching via command line then you will need to add <code>-p 32168:32168/udp</code> to the command line to open map the UDP port that enables the servers to broadcast their mesh status.</p> WindowsLinuxRaspberry Pi (Arm64)macOS (Intel)macOS (Apple Silicon) Command line<pre><code>docker run --name CodeProject.AI -d -p 32168:32168 -p 32168:32168/udp codeproject/ai-server\n</code></pre> Terminal<pre><code>docker run --name CodeProject.AI -d -p 32168:32168 -p 32168:32168/udp codeproject/ai-server\n</code></pre> <p>If you are running Linux on Arm then use <code>codeproject/ai-server:arm64</code> instead of <code>codeproject/ai-server</code></p> Terminal<pre><code>docker run --name CodeProject.AI -d -p 32168:32168 -p 32168:32168/udp codeproject/ai-server:rpi64\n</code></pre> Terminal<pre><code>docker run --name CodeProject.AI -d -p 32168:32168 -p 32168:32168/udp codeproject/ai-server\n</code></pre> Terminal<pre><code>docker run --name CodeProject.AI -d -p 32168:32168 -p 32168:32168/udp codeproject/ai-server:arm64\n</code></pre> <p>If launching using Docker Desktop then the only extra thing needed for mesh support is to fill in the 32168/udp port text box:</p> <p></p>","tags":["CodeProject.AI","Mesh"]},{"location":"features/modules.html","title":"CodeProject.AI Modules","text":"<p>Supporting CodeProject.AI Server 2.9.4.</p>","tags":["CodeProject.AI","Modules"]},{"location":"features/modules.html#computer-audition","title":"Computer Audition","text":"<ul> <li>Sound Classifier (TensorFlow)    The sound classifier uses Tensorflow with Python to classify sound files based on the UrbanSound8K dataset.    v1.4.0 \u00a0 All Platforms \u00a0 Python, TensorFlow    Project by Chris Maunder, based on Tensorflow-Audio-Classification. </li> </ul>","tags":["CodeProject.AI","Modules"]},{"location":"features/modules.html#computer-vision","title":"Computer Vision","text":"<ul> <li>License Plate Reader    Detects and readers single-line and multi-line license plates using YOLO object detection and the PaddleOCR toolkit    v3.3.3 \u00a0 All Platforms except Windows-arm64 \u00a0 Python, PaddlePaddle    By Mike Lud </li> <li>License Plate Reader (RKNN)    Detects and readers single-line and multi-line licence plates. This module only works with Rockchip RK3588/RK3588S NPUs like the Orange Pi 5/5B/5 Plus or Radxa ROCK.    v1.5.0 \u00a0 Orange Pi,  Radxa ROCK \u00a0 Python, FastDeploy    By Mike Lud </li> <li>Object Detection (Coral)    The object detection module uses the Coral TPU to locate and classify the objects the models have been trained on.    v2.4.0 \u00a0 All Platforms \u00a0 Python, TensorFlow-Lite    Project by Chris Maunder, Seth Price, based on Coral.ai examples. </li> <li>Object Detection (YOLOv5 .NET)    Provides Object Detection using YOLOv5 ONNX models with DirectML. This module is best for those on Windows and Linux without CUDA enabled GPUs    v1.13.0 \u00a0 All Platforms except Windows-arm64 \u00a0 C#, ONNX, DirectML, YOLO    Project by Matthew Dennis, based on yolov5-net. </li> <li>Object Detection (YOLOv5 3.1)    Provides Object Detection using YOLOv5 3.1 targeting CUDA 10 or 11 for older GPUs.    v1.12.1 \u00a0 All Platforms except macOS \u00a0 Python, PyTorch, YOLO    Project by Chris Maunder, Matthew Dennis, based on Deepstack. </li> <li>Object Detection (YOLOv5 6.2)    Provides Object Detection using YOLOv5 6.2 targeting CUDA 11.5+, PyTorch &lt; 2.0 for newer GPUs.    v1.10.0 \u00a0 All Platforms except Raspberry Pi,  Jetson \u00a0     Project by Matthew Dennis, based on Ultralytics YOLOv5. </li> <li>Object Detection (YOLOv5 RKNN)    Provides Object Detection using YOLOv5 RKNN models. This module only works with Rockchip RK3588/RK3588S NPUs like the Orange Pi 5/5B/5 Plus    v1.8.1 \u00a0 Orange Pi,  Radxa ROCK \u00a0 Python, FastDeploy, YOLO    By Mike Lud </li> <li>Object Detection (YOLOv8)    Provides Object Detection in Python&gt;=3.8 using YOLOv8. Great for newer NVIDIA GPUs    v1.6.2 \u00a0 All Platforms \u00a0 Python, PyTorch, YOLO    Project by Chris Maunder, based on ultralytics. </li> <li>Optical Character Recognition    Provides OCR support using the PaddleOCR toolkit    v2.2.3 \u00a0 All Platforms except Windows-arm64 \u00a0 Python, PaddlePaddle    By Mike Lud </li> <li>Scene Classification    Classifies an image according to one of 365 pre-trained scenes    v1.8.0 \u00a0 All Platforms except Jetson \u00a0 Python, PyTorch    Project by Chris Maunder, Matthew Dennis, based on Deepstack. </li> </ul>","tags":["CodeProject.AI","Modules"]},{"location":"features/modules.html#face-recognition","title":"Face Recognition","text":"<ul> <li>Face Processing    A number of Face image APIs including detect, recognize, and compare.    v1.12.1 \u00a0 All Platforms except Jetson \u00a0 Python, PyTorch    Project by Chris Maunder, Matthew Dennis, based on Deepstack. </li> </ul>","tags":["CodeProject.AI","Modules"]},{"location":"features/modules.html#generative-ai","title":"Generative AI","text":"<ul> <li>LlamaChat    A Large Language Model based on the Machine Learning Compilation for LLMs    v1.7.1 \u00a0 All Platforms except Windows-arm64,  Raspberry Pi,  Orange Pi,  Radxa ROCK,  Jetson \u00a0 Python, Llama    Project by Chris Maunder, based on llama-cpp-python. </li> <li>MultiModeLLM    A multi-modal Large Language Model    v1.1.0 \u00a0 Windows,  Linux,  macOS,  macOS \u00a0 Python, Phi-3    Project by Chris Maunder, based on chat-with-phi-3-vision. </li> <li>Text to Image    Generates an image from a text prompt.    v1.3.1 \u00a0 Windows,  macOS,  Linux \u00a0 Python, PyTorch, Stable Diffusion    Project by Matthew Dennis, based on Diffusers. </li> </ul>","tags":["CodeProject.AI","Modules"]},{"location":"features/modules.html#image-processing","title":"Image Processing","text":"<ul> <li>Background Remover    Automatically removes the background from a picture    v1.11.0 \u00a0 All Platforms except Linux,  Raspberry Pi,  Orange Pi,  Radxa ROCK,  Jetson \u00a0 Python, ONNX    Project by Chris Maunder, based on rembg. </li> <li>Cartoonizer    Convert a photo into an anime style cartoon    v1.7.0 \u00a0 All Platforms except Raspberry Pi,  Orange Pi,  Radxa ROCK,  Jetson \u00a0 Python, PyTorch    Project by Chris Maunder, based on animegan2-pytorch. </li> <li>Portrait Filter    Provides a depth-of-field (bokeh) effect on images. Great for selfies.    v2.3.0 \u00a0 Windows \u00a0 C#, ONNX, DirectML    Project by Matthew Dennis, based on C# PortraitModeFilter. </li> <li>Super Resolution    Increases the resolution of an image using AI    v2.2.1 \u00a0 All Platforms \u00a0 Python, ONNX    Project by Chris Maunder, based on PyTorch.org example. </li> </ul>","tags":["CodeProject.AI","Modules"]},{"location":"features/modules.html#natural-language","title":"Natural Language","text":"<ul> <li>Sentiment Analysis    Provides an analysis of the sentiment of a piece of text. Positive or negative?    v2.3.0 \u00a0 Windows,  macOS \u00a0 C#, TensorFlow    Project by Matthew Dennis, based on .NET ML Samples. </li> <li>Text Summary    Summarizes text content by selecting a number of sentences that are most representative of the content.    v1.9.0 \u00a0 All Platforms \u00a0 Python, NLTK    Project by Chris Maunder, based on Github gist. </li> </ul>","tags":["CodeProject.AI","Modules"]},{"location":"features/modules.html#training","title":"Training","text":"<ul> <li>Training for YoloV5 6.2    Train custom models for YOLOv5 v6.2 with support for CPUs, CUDA enabled GPUs, and Apple Silicon.    v1.7.0 \u00a0 All Platforms except Raspberry Pi,  Orange Pi,  Radxa ROCK,  Jetson \u00a0 Python, PyTorch, YOLO    Project by Matthew Dennis, based on Ultralytics YOLOv5. </li> </ul>","tags":["CodeProject.AI","Modules"]},{"location":"install/install_on_jetson.html","title":"Installing CodeProject.AI Server on a Jetson Nano as a Development environment","text":"<p>The Jetson Nano is a popular dev board that comes in 2GB and 4GB sizes. This makes it a challenge to work with, but the onboard GPU does make the effort worthwhile.</p> <p>While NVIDIA provides images for the Nano, these images are based on Ubuntu 18.04 which can cause issues due to its age. We support the QEngineering Jetson Nano images that come with Ubuntu 20.04, as well as PyTorch, Tensorflow, TensorRT and OpenCV pre-installed.</p> <p>While the QEngineering image will cause more stress on a tiny system, the ease of use outweighs this.</p>","tags":["CodeProject.AI","Jetson Nano"]},{"location":"install/install_on_jetson.html#to-setup-your-jetson-nano-for-codeprojectai-server","title":"To setup your Jetson Nano for CodeProject.AI Server","text":"<ol> <li> <p>Create a boot SD card by downloading balena Etcher and the QEngineering Ubuntu 20.04 Image. We recommend this in preference to the Official Jetson Nano image due to the QEngineering image being a more updated distro, as well as it being chock-full of all the AI pieces you need such as Tensorflow, PyTorch, TensorRT and OpenCV.</p> </li> <li> <p>Setup your Nano by inserting the card into the device, plugging in keyboard, mouse, monitor and power, and following the instructions. After a restart you'll find yourself on the Jetson Nano desktop</p> </li> <li> <p>Increase your swap size by downloading the <code>setSwapMemorySize.sh</code> script. The script will arrive in <code>~/Downloads</code>. Open a terminal and run the following.     Bash<pre><code>cd ~/Downloads\nbash setSwapMemorySize.sh -g 4\n</code></pre></p> <p>Reboot your Jetson to allow the changes to take effect.</p> </li> <li> <p>Download VSCode using the Mozilla browser (don't download Chrome on the QEngineering image because it will mess with snapd) and head to code.visualstudio.com/download. Download the arm64 debian package. The Visual Studio Code installer will be in saved to <code>~/Downloads</code></p> </li> <li> <p>To install Visual Studio Code open a terminal window and call    Bash<pre><code>cd ~/Downloads\nsudo apt install code_1.85.1-102461056_arm64.deb\n</code></pre> <code>code_1.85.1-102461056_arm64.deb</code> is the latest filename, but this will change. Use the name of the .deb file you downloaded from Microsoft.</p> </li> <li> <p>Open VSCode, sign in, and sync your VSCode settings (the 'head' icon at the bottom left of the VSCode window). A browser window will pop up to allow you to authenticate: always close this after authentication to reduce memory pressure. The syncing of settings may take some time depending on how many extensions you have. The Jetson itself may become unresponsive during this due to lack of memory. Let it run - it could be half an hour to an hour.</p> <p>Your Nano may lockup and VSCode may crash. Multiple times. Just keep trying. Eventually you will have VS Code setup and synced.</p> </li> <li> <p>Clone the CodeProject.AI project from GitHub (using the Git tools in VSCode is easiest) and then open the project. Again: the GitHub auth will launch Chrome and you may run out of memory and see lockups and crashes. Persist.</p> </li> <li> <p>Setup the dev environment on the Nano by heading to <code>src</code> in the project and running      ```bash     bash setup.sh     ````     You will need to provide an admin password at various points in the process. The entire setup will take over an hour.</p> </li> <li> <p>Build and run in VSCode using the debug tab. Choose 'Build all and Launch server (arm64)'. Note that this will build and run CodeProject.AI on the Jetson using the full VSCode editor, the .NET runtime, and Python, while also opening a Chrome browser. Your Jetson Nano will be stressed to the limit.</p> </li> <li> <p>Alternative option: SSH to the Jetson Using SSH to connect to the Jetson Nano allows you to have a smooth and seamless editing, build and debug experience on the Jetson without placing undue memory and processing pressure on the device from VSCode. If the device locks up you can still edit, and sometimes even save changes, while the device works through its issues. </p> <p>You can SSH from another machine into the Jetson Nano using the VSCode Remote SSH extension. Install this on your main desktop, go to the Dev Containers tab on your desktop, enter the IP address of your Jetson (use <code>ifconfig</code> on the Jetson to find your address) and then follow the prompts to login. </p> <p>The 'Open Folder' menu option allows you to then open the folder containing the cloned CodeProject.AI solution, and from then on you are editing, building and debugging on the Jetson, but using the power of your desktop to take the editing and GUI load off the Nano.</p> <p>As in step 9, simply choose 'Build all and Launch server (arm64)' to build on the Jetson, and then launch the server and dashboard for the Jetson.</p> </li> </ol>","tags":["CodeProject.AI","Jetson Nano"]},{"location":"install/install_on_linux.html","title":"Installing CodeProject.AI Server on Linux","text":"<p>To install CodeProject.AI as a standalone service ready for integration with applications such as HomeAssist or BlueIris on Linux, download the  latest installation package being careful to ensure you choose the x64 (Intel chips) or arm64 (Arm chips such as Raspberry Pi or Orange Pi).</p>","tags":["CodeProject.AI","Linux"]},{"location":"install/install_on_linux.html#prerequisites","title":"Prerequisites","text":"","tags":["CodeProject.AI","Linux"]},{"location":"install/install_on_linux.html#nvidia-gpu-support","title":"NVIDIA GPU support","text":"<p>For NVIDIA GPU support, ensure you have the latest NVIDIA CUDA drivers installed.</p>","tags":["CodeProject.AI","Linux"]},{"location":"install/install_on_linux.html#net-runtime","title":".NET runtime","text":"<p>Ensure you have the .NET runtime installed by calling</p> Bash<pre><code>sudo apt update &amp;&amp; sudo apt install dotnet-sdk-9.0\n</code></pre> <p>Note</p> <p>Please review Microsoft's guide to installing .NET on Ubuntu. .NET 9 is still in its early days so installing still has some rough edges.</p> <p>Once you have .NET installed run the post-install step:</p> Bash<pre><code>pushd \"/usr/bin/codeproject.ai-server-2.9.1/server\" &amp;&amp; bash ../setup.sh &amp;&amp; popd\n</code></pre>","tags":["CodeProject.AI","Linux"]},{"location":"install/install_on_linux.html#running-the-installer","title":"Running the installer","text":"<p>Unzip the .deb package, right-click and choose \"install\", or run Bash<pre><code>sudo dpkg -i codeproject.ai-server_2.9.1_Ubuntu_x64.deb\n</code></pre> (Assuming you are downloading version 2.9.1)</p> <p>This will install the server running under <code>systemd</code>. CodeProject.AI Server and the backend analysis services will now be running. The front-end server and the  analysis services will automatically restart each time your machine is restarted.</p> <p>To manually start the service, run the command Bash<pre><code>bash /usr/bin/codeproject.ai-server/start.sh\n</code></pre></p> <p>To explore CodeProject.AI Click on the CodeProject.AI Explorer link at the top of the server dashboard. </p>","tags":["CodeProject.AI","Linux"]},{"location":"install/install_on_linux.html#accessing-the-codeprojectai-server-dashboard","title":"Accessing the CodeProject.AI Server Dashboard.","text":"<p>Open a browser and navigate to http://localhost:32168 to open the CodeProject.AI Dashboard.  This will provide you with details of the server operation.</p>","tags":["CodeProject.AI","Linux"]},{"location":"install/install_on_linux.html#play-with-the-server","title":"Play with the Server","text":"<p>We provide a sample application written in HTML and JavaScript that performs various AI operations. Open http://localhost:32168/explorer.html in a browser.  There is also a link to this at the top of the Dashboard.</p>","tags":["CodeProject.AI","Linux"]},{"location":"install/install_on_linux.html#to-uninstall-codeprojectai","title":"To uninstall CodeProject.AI.","text":"<p>To uninstall CodeProject.AI Server run</p> Bash<pre><code>sudo dpkg -r codeproject.ai-server\n</code></pre>","tags":["CodeProject.AI","Linux"]},{"location":"install/install_on_macos.html","title":"Installing CodeProject.AI Server on macOS","text":"<p>To install CodeProject.AI as a standalone service ready for integration with applications such as HomeAssist or BlueIris on macOS, download the  latest installation package for macOS.</p> <p>Unzip and double click the .pkg package. This will install the server as a Login Item. CodeProject.AI Server and the backend analysis services will run each time you log in. </p> <p>To manually start the service, double click on <code>CodeProject.AI Server.command</code> in the <code>/Library/CodeProject.AI Server/2.9.1/</code> folder (replace 2.9.1 with the version you currently have installed).</p> <p>To explore CodeProject.AI Click on the CodeProject.AI Explorer link at the top of the server dashboard. </p>","tags":["CodeProject.AI","macOS"]},{"location":"install/install_on_macos.html#accessing-the-codeprojectai-server-dashboard","title":"Accessing the CodeProject.AI Server Dashboard.","text":"<p>Open a browser and navigate to http://localhost:32168 to open the CodeProject.AI Dashboard.  This will provide you with details of the server operation.</p>","tags":["CodeProject.AI","macOS"]},{"location":"install/install_on_macos.html#play-with-the-server","title":"Play with the Server","text":"<p>We provide a sample application written in HTML and JavaScript that performs various AI operations. Open http://localhost:32168/explorer.html in a browser.  There is also a link to this at the top of the Dashboard.</p>","tags":["CodeProject.AI","macOS"]},{"location":"install/install_on_macos.html#to-uninstall-codeprojectai","title":"To uninstall CodeProject.AI.","text":"<p>To uninstall CodeProject.AI Server run</p> <p>Bash<pre><code>sudo bash '/Library/CodeProject.AI Server/2.9.1/uninstall.sh'\n</code></pre> Replace the version (2.9.1) with the version of the server you wish to uninstall.</p>","tags":["CodeProject.AI","macOS"]},{"location":"install/install_on_windows.html","title":"Installing CodeProject.AI Server on Windows","text":"<p>To install CodeProject.AI as a standalone service ready for integration with applications such as HomeAssist or BlueIris, download the  latest installation package.</p> <p>For NVIDIA GPU support, ensure you have the latest NVidia CUDA drivers installed.</p> <p>Double click the installer. This will install the server as a Windows Service. CodeProject.AI Server and the backend analysis services will now be running. The front-end server and the  analysis services will automatically restart each time your machine is restarted.</p> <p>To explore CodeProject.AI Click on the CodeProject.AI Explorer link at the top of the server dashboard. </p> <p>The server will, of course, need to be running for this test application to function. Sample images can be found in the TestData folder under the C:\\Program Files\\CodeProject\\AI folder</p>","tags":["CodeProject.AI","Windows"]},{"location":"install/install_on_windows.html#accessing-the-codeprojectai-server-dashboard","title":"Accessing the CodeProject.AI Server Dashboard.","text":"<p>Open a browser and navigate to http://localhost:32168 to open the CodeProject.AI Dashboard.  This will provide you with details of the server operation.</p>","tags":["CodeProject.AI","Windows"]},{"location":"install/install_on_windows.html#play-with-the-server","title":"Play with the Server","text":"<p>We provide a sample application written in HTML and JavaScript that performs various AI operations. Open http://localhost:32168/explorer.html in a browser.  There is also a link to this at the top of the Dashboard.</p>","tags":["CodeProject.AI","Windows"]},{"location":"install/running_in_docker.html","title":"Running CodeProject.AI Server in Docker","text":"<p>Docker is a technology that allows you to package up applications, libraries,  drivers, runtimes, files, and anything else your application may need, all in a single deployable image that can run wherever Docker can run. Which is almost everywhere. The beauty is you don't have to worry about installing drivers or packages or having version conflicts: everything you need to run is in the container, ready to go.</p> <p>To run CodeProject.AI Server in Docker you will need to download and install Docker Desktop. This will install the runtime and desktop manager.</p>","tags":["CodeProject.AI","Docker"]},{"location":"install/running_in_docker.html#codeprojectai-server-images","title":"CodeProject.AI Server images","text":"<ul> <li>codeproject/ai-server The basic CPU-only server for x64 systems</li> <li>codeproject/ai-server:cuda11_7 A GPU (NVIDIA CUDA 11.7) enabled version x64 systems.   This image will also run on non-GPU systems.</li> <li>codeproject/ai-server:cuda12_2 A GPU (NVIDIA CUDA 12.2) enabled version x64 systems.   This image will also run on non-GPU systems.</li> <li>codeproject/ai-server:arm64 An image built for Arm64 chipsets such as the   Raspberry Pi or Apple Silicon devices.</li> <li>codeproject/ai-server:rpi64 A version specifically for Raspberry Pi Arm64  devices. This contains an object detection module suited for low resource systems.</li> </ul>","tags":["CodeProject.AI","Docker"]},{"location":"install/running_in_docker.html#running-a-docker-container","title":"Running a Docker container","text":"<p>There are two main ways to launch a Docker container: command line or using Docker Desktop. </p> <p>When running a Docker image in a container you have two further choices</p> <ol> <li>Save settings and downloadable modules directly in the Docker container. Each    time you restart the container these settings will remain. If you update your    Docker image and container you will lose your changes.</li> <li>Save settings and downloadable modules in a folder outside the Docker container.    This allows you to update your Docker image and container and keep all the modules and settings you've saved.</li> </ol>","tags":["CodeProject.AI","Docker"]},{"location":"install/running_in_docker.html#simple-docker-launch-settings-saved-in-the-container","title":"Simple Docker launch (settings saved in the container)","text":"<p>For this case we simply need to map the ports used and choose the correct image.</p> WindowsLinuxRaspberry Pi (Arm64)macOS (Intel)macOS (Apple Silicon) Command line<pre><code>docker run --name CodeProject.AI -d -p 32168:32168 codeproject/ai-server\n</code></pre> <p>If you are running Windows on Arm then use <code>codeproject/ai-server:arm64</code> instead of <code>codeproject/ai-server</code></p> Terminal<pre><code>docker run --name CodeProject.AI -d -p 32168:32168 codeproject/ai-server\n</code></pre> <p>If you are running Linux on Arm then use <code>codeproject/ai-server:arm64</code> instead of <code>codeproject/ai-server</code></p> Terminal<pre><code>docker run --name CodeProject.AI -d -p 32168:32168 \\\n --privileged -v /dev/bus/usb:/dev/bus/usb codeproject/ai-server:rpi64\n</code></pre> <p>For those who have a USB device such as a Coral.AI Edge TPU, including the --privileged -v /dev/bus/usb:/dev/bus/usb flags will provide access to the Coral hardware.</p> Terminal<pre><code>docker run --name CodeProject.AI -d -p 32168:32168 codeproject/ai-server\n</code></pre> Terminal<pre><code>docker run --name CodeProject.AI -d -p 32168:32168 codeproject/ai-server:arm64\n</code></pre>","tags":["CodeProject.AI","Docker"]},{"location":"install/running_in_docker.html#advanced-docker-launch-settings-saved-outside-of-the-container","title":"Advanced Docker launch (settings saved outside of the container)","text":"<p>We will need to map two folders from the Docker image to the host file system in order to allow settings to be persisted outside the container, and to allow modules to be downloaded and installed. This ensures that when you upgrade your docker images, the settings and modules you've already downloaded remain.</p> <p>The two folders are:</p> <ol> <li>The settings folder (/etc/codeproject/ai in the Docker image)</li> <li>The module downloads folder (/app/modules in the Docker image)</li> </ol> <p>Note that we use different folders in each Operating System based on best practices. <code>/etc</code> and <code>/opt</code> folders for Linux, <code>ProgramData</code> for Windows, and <code>Library\\Application Support</code> for macOS.</p> WindowsLinuxRaspberry Pi (Arm64)macOS (Intel)macOS (Apple Silicon) Command line<pre><code>docker run --name CodeProject.AI -d -p 32168:32168 ^\n --mount type=bind,source=C:\\ProgramData\\CodeProject\\AI\\docker\\data,target=/etc/codeproject/ai ^\n --mount type=bind,source=C:\\ProgramData\\CodeProject\\AI\\docker\\modules,target=/app/modules ^\n   codeproject/ai-server\n</code></pre> <p>If you are running Windows on Arm then use <code>codeproject/ai-server:arm64</code> instead of <code>codeproject/ai-server</code></p> Terminal<pre><code>docker run --name CodeProject.AI -d -p 32168:32168 \\\n --mount type=bind,source=/etc/codeproject/ai,target=/etc/codeproject/ai \\\n --mount type=bind,source=/opt/codeproject/ai,target=/app/modules \\\n   codeproject/ai-server\n</code></pre> <p>If you are running Linux on Arm then use <code>codeproject/ai-server:arm64</code> instead of <code>codeproject/ai-server</code></p> Terminal<pre><code>docker run --name CodeProject.AI -d -p 32168:32168 \\\n --mount type=bind,source=/etc/codeproject/ai,target=/etc/codeproject/ai \\\n --mount type=bind,source=/opt/codeproject/ai,target=/app/modules \\\n --privileged -v /dev/bus/usb:/dev/bus/usb\n   codeproject/ai-server:rpi64\n</code></pre> <p>For those who have a USB device such as a Coral.AI Edge TPU, including the --privileged -v /dev/bus/usb:/dev/bus/usb flags will provide access to the Coral hardware.</p> Terminal<pre><code>docker run --name CodeProject.AI -d -p 32168:32168 \\\n --mount type=bind,source='/Library/Application Support/CodeProject/AI/docker/data',target=/etc/codeproject/ai \\\n --mount type=bind,source='/Library/Application Support/CodeProject/AI/docker/modules',target=/app/modules \\\n   codeproject/ai-server\n</code></pre> <p>File Sharing in macOS</p> <p>By default, macOS only allows mounting to /tmp, /Users/Volumes/private and /var/folders. In order to bind mount to the /Library/Application Support folder you will need to go into Docker settings and add this folder</p> <p></p> Terminal<pre><code>docker run --name CodeProject.AI -d -p 32168:32168 \\\n --mount type=bind,source='/Library/Application Support/CodeProject/AI/docker/data',target=/etc/codeproject/ai \\\n --mount type=bind,source='/Library/Application Support/CodeProject/AI/docker/modules',target=/app/modules \\\n   codeproject/ai-server:arm64\n</code></pre> <p>File Sharing in macOS</p> <p>By default, macOS only allows mounting to /tmp, /Users/Volumes/private and /var/folders. In order to bind mount to the /Library/Application Support folder you will need to go into Docker settings and add this folder</p> <p></p>","tags":["CodeProject.AI","Docker"]},{"location":"install/running_in_docker.html#to-use-the-gpu-enabled-images","title":"To use the GPU enabled images","text":"<p>The Docker GPU version is specific to NVidia's CUDA enabled cards with compute capability &gt;= 6.0. In order to ensure the Docker image has access to the GPU hardware, you need to use the  <code>--gpus all</code> flag, and pull down the codeproject/ai-server:cuda11_7 image.</p> <p>Note there is no Arm64 GPU-enabled build at this time.</p> WindowsLinux Command line<pre><code>docker run --name CodeProject.AI -d -p 32168:32168 --gpus all ^\n --mount type=bind,source=C:\\ProgramData\\CodeProject\\AI\\docker\\data,target=/etc/codeproject/ai ^\n --mount type=bind,source=C:\\ProgramData\\CodeProject\\AI\\docker\\modules,target=/app/modules ^\n   codeproject/ai-server:cuda11_7\n</code></pre> Terminal<pre><code>docker run --name CodeProject.AI -d -p 32168:32168 --gpus all \\\n --mount type=bind,source=/etc/codeproject/ai,target=/etc/codeproject/ai \\\n --mount type=bind,source=/opt/codeproject/ai,target=/app/modules \\\n   codeproject/ai-server:cuda11_7\n</code></pre> <p>The Docker GPU version will only run under Windows and Ubuntu. macOS no longer supports NVidia hardware.</p>","tags":["CodeProject.AI","Docker"]},{"location":"install/running_in_docker.html#starting-using-docker-desktop","title":"Starting using Docker Desktop","text":"<p>To launch an image using Docker Desktop you will first need to pull an image from Docker hub. In a command terminal run <code>docker pull codeproject/ai-server</code> to get  the latest version of the CodeProject.AI Docker image, or use one of the images names listed above. You can launch a container  using the image via Docker Desktop. </p> <p></p> <p>In the Images tab, select the image you wish to run, click the 'Run' button,  and then click Optional Settings</p> <p></p> <p>Set the following properties:</p> WindowsLinuxmacOS <ul> <li>Ports: 32168 (or 5000 if you've been using legacy versions that expect port 5000)</li> <li>Volumes (if you wish to persist settings outside the container):<ul> <li>Host Volume C:\\ProgramData\\CodeProject\\AI\\docker\\data maps to Container path /etc/codeproject/ai</li> <li>Host Volume C:\\ProgramData\\CodeProject\\AI\\docker\\modules maps to Container path /app/modules</li> </ul> </li> </ul> <ul> <li>Ports: 32168 (or 5000 if you've been using legacy versions that expect port 5000)</li> <li>Volumes (if you wish to persist settings outside the container):<ul> <li>Host Volume /etc/codeproject/ai maps to Container path /etc/codeproject/ai</li> <li>Host Volume /opt/codeproject/ai maps to Container path /app/modules</li> </ul> </li> </ul> <ul> <li>Ports: 32168 (or 5500 if you've been using legacy versions that expect port 5500)</li> <li>Volumes (if you wish to persist settings outside the container):<ul> <li>Host Volume /Library/Application Support/CodeProject/AI/docker/data maps to Container path /etc/codeproject/ai</li> <li>Host Volume /Library/Application Support/CodeProject/AI/docker/modules maps to Container path /app/modules</li> </ul> </li> </ul> <p>You can also provide an optional name. Just be sure not to include spaces.  Docker Desktop is fussy.</p> <p>Click 'Run' and you're done.</p>","tags":["CodeProject.AI","Docker"]},{"location":"install/running_in_docker.html#docker-compose","title":"Docker Compose","text":"<p>Here are some basic docker compose files to get you started. </p> CPUGPU EnabledCUDA 11.7Raspberry Pi / Orange PiApple Silicon YAML<pre><code>version: \"3.9\"\n\nservices:\n  CodeProjectAI:\n    image: codeproject/ai-server\n    container_name: codeproject-ai-server-cpu\n    hostname: codeproject-ai-server\n    restart: unless-stopped\n    ports:\n      - \"32168:32168\"\n    environment:\n      - TZ=America/Toronto \n</code></pre> YAML<pre><code>version: \"3.9\"\n\nservices:\n  CodeProjectAI:\n    image: codeproject/ai-server:cuda11_7\n    container_name: codeproject-ai-server-cuda\n    hostname: codeproject-ai-server\n    restart: unless-stopped\n    ports:\n      - \"32168:32168\"\n    environment:\n      - TZ=America/Toronto\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              count: 1\n              capabilities: [gpu]\n</code></pre> YAML<pre><code>version: '3.9'\n\nservices:\n  CodeProjectAI:\n    image: codeproject/ai-server:cuda11_7\n    container_name: \"codeproject-ai-server-cuda\"\n    restart: unless-stopped\n    ports:\n      - \"32168:32168\"\n    environment:\n      - TZ=America/Toronto\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              count: 1\n              capabilities: [gpu]\n</code></pre> YAML<pre><code>version: \"3.9\"\n\nservices:\n  CodeProjectAI:\n    image: codeproject/ai-server:rpi64\n    container_name: codeproject-ai-server-rpi\n    hostname: codeproject-ai-server\n    restart: unless-stopped\n    privileged: true\n    ports:\n      - \"32168:32168\"\n    environment:\n      - TZ=America/Toronto\n    volumes:\n      - '/dev/bus/usb:/dev/bus/usb'\n</code></pre> YAML<pre><code>version: \"3.9\"\n\nservices:\n  CodeProjectAI:\n    image: codeproject/ai-server:arm64\n    container_name: codeproject-ai-server-arm64\n    hostname: codeproject-ai-server\n    restart: unless-stopped\n    ports:\n      - \"32168:32168\"\n    environment:\n      - TZ=America/Toronto\n</code></pre> <p>Here's an example of how to use a docker compose file on Windows. Open up Notepad. Copy and paste the above command line into Notepad. Then save the file as a .yml, not .txt, and remember where you saved it.</p> <p>Open a command line. Navigate to the folder where you saved the .yml file, then type :</p> Text Only<pre><code>``` batch title='Command line'\ndocker compose docker-compose.yml up\n```\n</code></pre> <p>where docker-compose.yml is the name of your .yml file.</p> <p>If you want to use a docker compose file that will allow you to add custom models, make the following additions to your docker compose file:</p> Text Only<pre><code>```yaml\nversion: \"3.9\"\n\nservices:\n  CodeProjectAI:\n    image: codeproject/ai-server\n    container_name: codeproject-ai-server-cpu\n    hostname: codeproject-ai-server\n    restart: unless-stopped\n    ports:\n      - \"32168:32168\"\n    environment:\n      - TZ=America/Toronto \n    volumes:\n      - codeproject_ai_data:/etc/codeproject/ai\n      - codeproject_ai_modules:/app/modules\nvolumes:\n  codeproject_ai_data:\n  codeproject_ai_modules:\n```\n</code></pre> <p>Then open a command line and type:</p> Text Only<pre><code>``` batch title='Command line'\ndocker compose docker-compose.yml up\n```\n</code></pre> <p>Open Docker Desktop and select the Containers tab and click on the container name</p> <p></p> <p>This will open up the container. There are a number of tabs that let you get information about the container. Click on the Files tab. This is where you would add/delete file from Volumes as they are mapped to directories in the container file system.</p> <p></p> <p>You can navigate to the custom-models directory, then drag and drop your custom model onto the folder name. </p>","tags":["CodeProject.AI","Docker"]},{"location":"install/running_in_docker.html#changing-server-settings","title":"Changing Server settings","text":"<p>To change the settings of an instance of CodeProject.AI Server running in a Docker container, you have two main options</p> <ol> <li> <p>Pass command line variables to the docker command.  Read more here</p> </li> <li> <p>Use the CodeProject.AI Server settings API. You can read more here</p> </li> </ol>","tags":["CodeProject.AI","Docker"]},{"location":"install/running_in_docker.html#common-example-specifying-a-folder-for-custom-object-detection-files-for-the-objectdetectionyolo-module","title":"Common example: specifying a folder for custom object detection files for the ObjectDetectionYolo module","text":"<p>The Object Detection (YOLO) module provides the ability to specify a folder that will contain custom models. To access this functionality in Docker you simply map the folder inside the docker instance to a folder on your host operating system.</p> <p>This module comes pre-installed in most docker images (the exception being the Raspberry Pi and Jetson docker images). Pre-installed modules live in /app/preinstalled-modules, rather than /app/modules. This allows you to download, install and persist modules by mapping to the /modules folder while still having pre-installed modules available.</p> <p>Suppose we have our custom models in the <code>C:\\my-custom-models</code> folder. To map this to the  custom-models folder in the docker instance you would launch the image like:</p> Terminal<pre><code>docker run --name CodeProject.AI -d -p 32168:32168 ^\n --mount type=bind,source=C:\\ProgramData\\CodeProject\\AI\\docker\\data,target=/etc/codeproject/ai ^\n --mount type=bind,source=C:\\ProgramData\\CodeProject\\AI\\docker\\modules,target=/app/modules ^\n --mount type=bind,source=C:\\my-custom-models,target=/app/preinstalled-modules/ObjectDetectionYolo/custom-models,readonly ^\n   codeproject/ai-server\n</code></pre>","tags":["CodeProject.AI","Docker"]},{"location":"install/running_in_docker.html#accessing-the-codeprojectai-server-dashboard","title":"Accessing the CodeProject.AI Server Dashboard.","text":"<p>Open a browser and navigate to http://localhost:32168 to open the CodeProject.AI Dashboard.  This will provide you with details of the server operation.</p>","tags":["CodeProject.AI","Docker"]},{"location":"install/running_in_docker.html#explore-the-modules-running-in-the-server","title":"Explore the modules running in the server","text":"<p>We provide a sample application written in HTML and JavaScript that performs various AI operations.  Open http://localhost:32168/explorer.html in a browser.  There is also a link to this at the top of the Dashboard.</p>","tags":["CodeProject.AI","Docker"]},{"location":"install/running_in_docker.html#adding-new-modules-to-a-docker-container","title":"Adding new modules to a Docker container","text":"<p>As of version 1.6.8 there is now the ability to add modules to the host server and have them be run  inside the Docker container. Version 2.0 of CodeProject.AI Server extends this too allow modules to be downloaded and installed at runtime via the dashboard.</p> <p>In our discussions on launching a Docker image above, we included a mapping of the  /app/modules folder (in the Docker image) to a folder in the host file system  (eg C:\\ProgramData\\CodeProject\\AI\\docker\\modules). Doing this allows CodeProject.AI Server  the chance to download, install and store modules that will persist over Docker container  restarts.</p> <p>It also provides the means for you to add a new module to an existing Docker container.</p> <ol> <li> <p>Launch your Docker container as per the instructions above. The downloaded modules    in the Docker container are stored in /app/modules. In Windows you would have mapped    this folder to C:\\ProgramData\\CodeProject\\AI\\docker\\modules. in Linux you would have     mapped it to /opt/codeproject/ai, and in macOS it would be mapped to     /Library/Application Support/CodeProject/AI/docker/modules.</p> </li> <li> <p>Copy your new module's folder into the mapped /app/modules folder on your host    (eg. C:\\ProgramData\\CodeProject\\AI\\docker\\modules in Windows).</p> </li> <li> <p>Ensure the module's modulesettings.json settings file has <code>RuntimeLocation</code> set as \"Local\"\".</p> JSON<pre><code>\"Modules\": {\n    \"MyModuleId\": {\n      \"Name\": \"My Module\",\n      \"Version\": \"1.0\",\n\n      \"LaunchSettings\": {\n          \"FilePath\": \"my_module.py\",\n          \"Runtime\": \"python38\", \n          \"RuntimeLocation\": \"Local\", // Can be Local or Shared\n</code></pre> </li> <li> <p>Ensure you have a install.sh script for your module, in the module's folder.    Any calls to 'setupPython' or 'installPythonPackages' should specify the install location as \"Local\".</p> </li> <li> <p>Open a terminal to your Docker Container. Docker Desktop provides a neat GUI    for doing this. Go to /app/modules/[your module folder] and execute </p> Bash<pre><code>bash ../../setup.sh\n</code></pre> <p>This will run the setup script on your new module from within the Docker container, ensuring the module is setup correctly for the container's  operating system.</p> </li> </ol> <p>The next time you start the Docker container this module will be discovered and started along with the other modules.</p>","tags":["CodeProject.AI","Docker"]},{"location":"why/how_to_add_AI_to_an_app.html","title":"How to add Artificial Intelligence to an existing Application","text":"<p>If you haven't already been asked to add Artificial Intelligence (AI) capabilities to an app then it's probably only a matter of time before the topic is raised.</p> <p>Adding AI capabilities isn't hard. However, that's like saying adding database support to an app  isn't hard. It's not, but choosing the correct database, setting up the schema and stored  procedures can be hard work. Then you need to decide whether the database should be on the same  server, different server. You also need to decide which database you'll use: relational, document  based, Key/Value... It can get complicated.</p> <p>AI is just like that. Add a library, use a local service, use a hosted service, which service do I use? How do I set it up. And then the tricky questions: how much will it cost? How will my data be handled? How secure is it?</p> <p>So let's do a quick walk through of your options so you at least know the questions to ask.</p>","tags":["CodeProject.AI","Artificial-Intelligence"]},{"location":"why/how_to_add_AI_to_an_app.html#writing-it-yourself","title":"Writing it yourself","text":"<p>I'll start by saying this is how we started our foray into AI a decade ago and I really  wouldn't recommend it. There are so, so many brilliant researchers who have spent a zillion man-hours building incredibly powerful and efficient AI libraries and models based on a fast evolving corpus of research into AI that it's simply easier, faster and safer to use one of the many AI solutions already available.</p> <p>Having said that, diving into something like a simple neural network to build up the ability to classify data based on your specific scenarios can be fulfilling, provide great results, and will result in little overhead. CodeProject's SPAM filter is just such a beast and anything bigger would, in our view, be overkill. The right tool for the job in this case.</p> <p>Pros:   - It's fun writing code.  - You get exactly what you need and nothing more  - You may end up with a far smaller codebase since you're not importing libraries and all their    dependencies</p> <p>Cons - You're reinventing the wheel - You'll (probably) not do it as well, as accurately, or have a solution as fast as what is   already out there. - It may be a distraction from your core business case. It could end up costing you more in   time, missed opportunities and developer time than simply using an existing solution.</p>","tags":["CodeProject.AI","Artificial-Intelligence"]},{"location":"why/how_to_add_AI_to_an_app.html#using-an-ai-library-or-toolkit-directly-in-your-code","title":"Using an AI library or toolkit directly in your code","text":"<p>If you wish to include AI processing directly in your code base then you can't go wrong using libraries such as such as Tensorflow or PyTorch. There are lots of mature, supported, easy to use libraries available for multiple languages and platforms. They take care of the hardwork for you, and together with the many pre-trained models out there, all you need to do is include the toolkit, load the model, input your data, run the inference and output the results.</p> <p>Here's how to use the latest YOLO5 model in python:</p> Python<pre><code>import torch                                            # import the Torch library\n\nmodel = torch.hub.load('ultralytics/yolov5', 'yolov5s') # Load the YOLO model\nimg = '~/username/images/street.jpg'                    # Specify the image\nresults = model(img)                                    # Perform the inference\nresults.print()                                         # Output the results\n</code></pre> <p>How easy is that!</p> <p>Issues start to arise when you need to cater for different model versions, and the versions  of the libraries that the models were trained for. And the versions of the compiler or  interpreter needed for the libraries that are needed for the models. And then what happens if  all of this conflicts with the libraries, interpreter versions and even hardware requirements  in other parts of your app?</p> <p>It can be a real challenge, especially for, say, Python, where you may need to setup multiple virtual environments, each with their own copies of the library packages, and each using a different version of Python. Keeping these in sync and uncorrupted can take a lot of patience.</p> <p>You may have a wonderful solution for, say, Python 3.7, but when it's run on another machine that has Python 3.11 installed, it may simply fail.</p> <p>Adding AI directly into your application can mean you will need to be extremely careful to ensure you always deploy all the parts needed as one unit. Docker will save you here, but to many that  kind of overhead may not be acceptable.</p> <p>Finally, in adding an AI toolkit to your app you need to remember that you'll also be adding the  model itself. AI models can be big, Gigabyte big.</p> <p>Pros</p> <ul> <li>You build on the work of brilliant developers and researchers before you</li> <li>Many of the libraries are Open Source so you can view and audit the code</li> <li>AI libraries and tools are being developed and refined at breakneck speed. There is a constant    stream of new features and improved performance being released</li> <li>The libraries are generally very easy to use</li> <li>There are tools to allow conversion of models between libraries. </li> <li>There's a model for almost any language and platform</li> </ul> <p>Cons</p> <ul> <li>There's definitely a learning curve to using a library</li> <li>You may be restricted to using a particular model format for the given library</li> <li>There's so, so many libraries. The paradox of choice.</li> <li>Including a library rarely means just one library: it usually brings along all its friends and   relatives and distant cousins. Things can get bloated</li> <li>Inluding a library means including the models. Things can get really, really big, fast.</li> <li>You have to ensure you keep your compiler/interpreter, libraries and models in sync with regards   to versioning. Never assume the defualt installation of python, for instance, will work with   your code.</li> </ul>","tags":["CodeProject.AI","Artificial-Intelligence"]},{"location":"why/how_to_add_AI_to_an_app.html#using-an-abstracting-library-net-ml-openvino","title":"Using an abstracting library (.NET ML, OpenVINO)","text":"<p>Many libraries require you use a specific form of pre-trained model, or they require a different library for different hardware. This issue is solved by libraries such as ML.NET and openVino that work to aggregate and abstract libraries and hardware in order to provide a single API for your AI operations.</p> <p>Pros</p> <ul> <li>All the pros of using a dedicated library</li> <li>No need to have specific version for specific hardware. The libraries will adapt dynamically</li> <li>You're able to easily consume a wider range of models</li> <li>You're somewhat future proofed against new hardware and model formats </li> </ul> <p>Cons</p> <ul> <li>An aggregation of libraries and capabilities will result in a larger footprint.</li> <li>Abstraction may result in the \"least common denominator\" issue whereby a library only exposes   common functionality, meaning you lose access to some features or fine tuning available in a    dedicated library</li> <li>Your choice of language or platform may be limited. </li> </ul>","tags":["CodeProject.AI","Artificial-Intelligence"]},{"location":"why/how_to_add_AI_to_an_app.html#hosted-ai-service","title":"Hosted AI Service","text":"<p>Using a hosted AI service means you do away with all the issues involved with libraries and hardware and compatible toolkits and dragging around GB of models. You make a call to a hosted AI service and the result comes back milliseconds later over your low latency, high bandwidth internet connection. Assuming you have one of those, of course.</p> <p>The range of services offered by hosted providers is truly amazing. Pre-built models, fast hardware, great APIs. Just be aware of the cost.</p> <p>When thinking about the cost you need to understand the charges. Will it cost to upload data to the provider? What about downloading results? What's the cost pre request and how is it calculated? Some services will charge per request, some per processing unit, some per time. You also need to factor in the cost of data storage and any licensing costs that may be applicable. Note also that the cost will be affected to a high degree by the tasks: passing in data that is applied to a pre- trained model is one thing, but passing in terrabytes of data for training new models is order of magnitudes more expensive. GPT-3, for instance, is rumoured to have cost around $5 million to  train.</p> <p>There are options to reduce your cost. One method is to mix and match service providers: Upload  and store your data with a provider such as DELL that has cheap storage. Send this data to Azure,  which may not have storage ingesting charges, train the model, and send the results back to your DELL storage. Your data is safe and stored relatively cheaply on one provider, while another  provider has done the heavy lifting of training your model. Sending data between large hosting  providers is often extremely fast due to the massive pipes they sit on.</p> <p>If you are simply using the hosting provider for AI inferencing (ie sending data to an AI model to have a prediction or analysis made by the model) then you should also be of constraints such as limits to the absolute number of calls, as well as any throttling that would limit the number of calls in a given period of time. How will your users react if a piece of functionality disappears because other users have exhausted your quota for the day?</p> <p>You also need to understand where the data goes and how the laws in that jurisdiction may affect you. Will a copy of your data be stored in a foreign jurisdiction? Will your data feed be monitored or made available to third parties? A webcam feed from inside a person's home may not be something  a user or your app wants to know is being sent to a foreign country for processing. There may even be legal or insurance restrrictions you need to be aware of if sending personally identifiable data outside of your country.</p> <p>Pros</p> <ul> <li>Fast, powerful, and you get access to the latest and greatest</li> <li>No need to worry about AI libraries or versions</li> <li>No need to worry about hardware speed or capacity. Your credit card is your only limiter.</li> <li>You're able to easily use a wide range of models</li> <li>You're future proofed. You'll have access to the latest and greatest</li> <li>Your apps will be smaller. No carrying around library code or models</li> <li>They will work with any language that can make an API call over HTTP</li> </ul> <p>Cons</p> <ul> <li>You will probably need a decent internet connection to make use of these services</li> <li>They can be expensive, or more often than not, ambiguous or opaque in what it will actually cost</li> <li>The system is closed. You can't really see what's happening behind the curtains</li> <li>You don't control where your data goes. This can be an issue for privacy and security.</li> <li>You may face quota or usage issues</li> </ul>","tags":["CodeProject.AI","Artificial-Intelligence"]},{"location":"why/how_to_add_AI_to_an_app.html#local-ai-service","title":"Local AI Service","text":"<p>So what if you don't want to write your own code for AI, you want to use any language or platform you choose to for your AI analysis, you don't want your data to leave your local network (or even your machine) and you don't want to pay an unknown amount for a service that you know is available for free.</p> <p>A locally hosted AI service such as CodeProject.AI is a great example of an AI service that is the best of both worlds. You do not need to worry about dealing with libraries and versions, and any language that can make a HTTP call can interact with the service.</p> <p>Pros</p> <ul> <li>Local Open Source AI servers can be found online and used for free</li> <li>There are no usage limits</li> <li>Your data stays where you can see it. Nothing gets sent outside the network unless you choose.</li> <li>Like hosted AI services, you don't need to worry about libraries or versioning. It's all within   the bounds of the service</li> <li>You get the benefit of accessing multiple AI features easily without needing to install a library   or toolkit for each AI operation you want to perform</li> </ul> <p>Cons</p> <ul> <li>The installer for the service may be large depending on what models and features are included.</li> <li>Like using a library directly, you are limited by the power of the machine the service is    installed on</li> <li>Again, like using a library directly, you won't get updates automatically. </li> <li>The AI features offered will not be as large as a hosted service. However, a single service may   provide multiple AI operations from multiple providers, all presented through a unified API</li> </ul>","tags":["CodeProject.AI","Artificial-Intelligence"]},{"location":"why/how_to_add_AI_to_an_app.html#next-steps","title":"Next Steps","text":"<p>Adding AI to an application can fundamentally expand the capabilities of an application while reducing complexity. Heuristics and hard coded if-then statements get replaced by training sets based on (hopefully) a wide range of real world data that traditional binary logic can't easily encompass.</p> <p>The manner in which you add, AI, has equally fundamental consequences and the choice of how you do this depends on your requirements, your budget and ultimately your circumstances. </p> <p>At CodeProject we have dabbled with all of the methods outlined above in adding AI to our systems. Our experience has ranged from being pleasantly surprised at how easy some methods  are, to outright enraged at having to fight the tools every step of the way. </p> <p>In the end we wanted to share our experience (and the fun) it working with AI to as many developers as  possible without asking them to go through the frustration. Hunting down compatible libraries, dealing with models, system tools, paths, oddities between operating systems and oddities between the same operating system on different CPUs was and still is incredibly time consuming. We built CodeProject.AI Server as a means to wrap up this complexity into a self contained package that did all the grunt work for you. Once installed you were immediately at the point were you could start  playing with, and using, AI in your apps.</p> <p>Download the Latest version and give it a try.</p>","tags":["CodeProject.AI","Artificial-Intelligence"]},{"location":"why/integrations.html","title":"Who is using CodeProject.AI?","text":"<p>Blue --- title: Who is using CodeProject.AI Server tags:   - CodeProject.AI   - Docker</p>"},{"location":"why/integrations.html#who-is-using-codeprojectai-server","title":"Who is using CodeProject.AI Server?","text":""},{"location":"why/integrations.html#blue-iris","title":"Blue Iris","text":"<p>5.5.8 - June 13, 2022: More formal support for Code Project\u2019s SenseAI, now our preferred no-extra-cost AI provider over DeepStack. SenseAI is better supported by its developers and has been found to be more stable overall. At this point, if you are relying heavily upon custom AI models, you may need to stick with DeepStack until guidance can be provided on how to add these to SenseAI.</p> <p>Blue Iris Software</p>"},{"location":"why/integrations.html#agent-dvr","title":"Agent DVR","text":"<p>Agent integrates with DeepStack AI and senseAI to provide intelligent object detection and Facial Recognition services. Agent DVR uses AI object detection to apply an intelligent filter to alerts to reduce the number of false alert events to near zero. You configure Agent DVR as usual with a motion detector/ alert configuration and then add AI alert filtering to ignore alerts that don't have specific objects in the frame - like car, person, dog etc.</p> <p>Agent DVR</p>"}]}